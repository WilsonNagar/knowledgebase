---
number: 8
title: "Sharding & Partitioning"
slug: "sharding-partitioning"
level: "intermediate"
tags: ["databases", "sharding", "partitioning", "scalability", "distributed"]
prerequisites: ["database-replication"]
estimated_minutes: 120
contributors: []
diagrams: []
examples: []
canonical_id: "cs-db-08"
---

# Sharding & Partitioning

## Overview

Sharding and partitioning are techniques for horizontally scaling databases by splitting data across multiple servers. Understanding partitioning strategies, shard key selection, rebalancing, and cross-shard queries is essential for building scalable database systems.

## Table of Contents

1. [What is Sharding?](#what-is-sharding)
2. [Partitioning Strategies](#strategies)
3. [Hash Partitioning](#hash-partitioning)
4. [Range Partitioning](#range-partitioning)
5. [Directory-Based Partitioning](#directory-based)
6. [Shard Key Selection](#shard-key)
7. [Rebalancing](#rebalancing)
8. [Cross-Shard Queries](#cross-shard)

## What is Sharding?

### Definition

**Sharding**: Split database into smaller pieces (shards)

**Purpose**: Horizontal scaling

**Benefit**: Distribute data and load across servers

### Sharding vs Replication

**Replication**: 
- **Same data**: Copies on multiple servers
- **Purpose**: Availability, read scaling

**Sharding**:
- **Different data**: Each shard has different data
- **Purpose**: Write scaling, storage scaling

**Combined**: Sharded + Replicated (best of both)

## Partitioning Strategies

### Horizontal Partitioning

**Definition**: Split rows across shards

**Example**:
```
Shard 1: Users 1-1000
Shard 2: Users 1001-2000
Shard 3: Users 2001-3000
```

### Vertical Partitioning

**Definition**: Split columns across shards

**Example**:
```
Shard 1: User profiles (id, name, email)
Shard 2: User activity (id, actions, timestamps)
```

**Use**: When columns accessed independently

## Hash Partitioning

### How It Works

**Process**:
```
1. Hash shard key
2. Modulo number of shards
3. Route to shard
```

**Formula**:
```
shard = hash(shard_key) % num_shards
```

### Hash Partitioning Example

**Shard Key**: User ID

**Shards**: 4 shards

**Example**:
```
hash(123) % 4 = 3 → Shard 3
hash(456) % 4 = 0 → Shard 0
hash(789) % 4 = 1 → Shard 1
```

### Hash Partitioning Benefits

**1. Uniform Distribution**:
```
Good hash function → Even distribution
```

**2. Simple Routing**:
```
Direct computation → Fast routing
```

**3. Load Balancing**:
```
Even load across shards
```

### Hash Partitioning Limitations

**1. Range Queries**:
```
Cannot efficiently query ranges
Must query all shards
```

**2. Rebalancing**:
```
Adding shards requires rehashing all data
```

## Range Partitioning

### How It Works

**Process**:
```
1. Define ranges
2. Route based on value range
```

**Example**:
```
Shard 1: Users 1-1000
Shard 2: Users 1001-2000
Shard 3: Users 2001-3000
```

### Range Partitioning Benefits

**1. Range Queries**:
```
Efficient range queries
Only query relevant shards
```

**2. Time-Based**:
```
Natural for time-series data
Recent data in one shard
```

### Range Partitioning Limitations

**1. Uneven Distribution**:
```
May have hot spots
Some shards overloaded
```

**2. Skew**:
```
Data may not distribute evenly
```

## Directory-Based Partitioning

### How It Works

**Process**:
```
1. Maintain directory (mapping)
2. Lookup shard for key
3. Route to shard
```

**Example**:
```
Directory:
  User 123 → Shard 2
  User 456 → Shard 0
  User 789 → Shard 1
```

### Directory-Based Benefits

**1. Flexibility**:
```
Can move data easily
Update directory
```

**2. Rebalancing**:
```
Easy to rebalance
Just update directory
```

### Directory-Based Limitations

**1. Single Point of Failure**:
```
Directory must be highly available
```

**2. Lookup Overhead**:
```
Extra lookup step
May cache directory
```

## Shard Key Selection

### Criteria

**1. Distribution**:
```
Even distribution across shards
Avoid hot spots
```

**2. Query Patterns**:
```
Most queries include shard key
Enables single-shard queries
```

**3. Growth**:
```
Shard key should distribute new data evenly
```

### Good Shard Keys

**User ID**: 
- **Even distribution**: Usually good
- **Query pattern**: Most queries by user

**Geographic Region**:
- **Distribution**: May be uneven
- **Query pattern**: Queries often regional

### Bad Shard Keys

**Sequential ID**:
- **Problem**: Creates hot spots
- **New data**: Always goes to last shard

**Low Cardinality**:
- **Problem**: Few distinct values
- **Distribution**: Uneven

## Rebalancing

### When to Rebalance

**Triggers**:
- **Add shards**: Need to redistribute
- **Remove shards**: Need to consolidate
- **Imbalance**: Some shards too large

### Rebalancing Strategies

**1. Offline Rebalancing**:
```
Stop writes
Move data
Resume writes
```

**2. Online Rebalancing**:
```
Continue serving requests
Move data gradually
Update routing
```

### Rebalancing Process

**Steps**:
```
1. Identify data to move
2. Copy data to new shard
3. Update routing
4. Verify consistency
5. Remove old data
```

## Cross-Shard Queries

### The Challenge

**Single-Shard Query**:
```
Query includes shard key
Route to one shard
Fast
```

**Cross-Shard Query**:
```
Query doesn't include shard key
Must query all shards
Slow, expensive
```

### Cross-Shard Query Strategies

**1. Scatter-Gather**:
```
Query all shards
Gather results
Merge results
```

**2. Avoid Cross-Shard**:
```
Design queries to include shard key
Prefer single-shard queries
```

**3. Materialized Views**:
```
Pre-compute aggregations
Store in single shard
```

## Real-World Examples

### Example 1: MongoDB Sharding

**Shard Key**: User ID

**Method**: Hash or range partitioning

**Config Servers**: Maintain metadata

**Mongos**: Route queries

### Example 2: Cassandra Partitioning

**Partition Key**: Determines node

**Method**: Consistent hashing

**Replication**: Multiple replicas per partition

### Example 3: MySQL Sharding

**Shard Key**: User ID

**Method**: Application-level routing

**Challenges**: Cross-shard joins difficult

## Common Pitfalls

### Problem: Poor Shard Key

```sql
-- BAD: Sequential ID as shard key
-- Creates hot spots

-- GOOD: Hash of user ID or UUID
-- Even distribution
```

### Problem: Cross-Shard Joins

```sql
-- BAD: Join across shards
SELECT * FROM users u
JOIN orders o ON u.id = o.user_id
-- Must query all shards!

-- GOOD: Denormalize or use materialized views
-- Or ensure join key is shard key
```

### Problem: Not Planning for Growth

```c
// BAD: Fixed number of shards
// Hard to scale later

// GOOD: Plan for adding shards
// Use consistent hashing or directory
```

## Quiz

1. What is the main purpose of sharding?
   - **A)** Replicate data
   - **B)** Horizontally scale database by splitting data across servers
   - **C)** Backup data
   - **D)** Encrypt data

2. What is the difference between hash and range partitioning?
   - **A)** No difference
   - **B)** Hash: uniform distribution, simple routing; Range: efficient range queries, may have hot spots
   - **C)** Hash is slower
   - **D)** Range is simpler

3. What is a good shard key?
   - **A)** Sequential ID
   - **B)** Key that distributes evenly and matches query patterns
   - **C)** Random number
   - **D)** Any key

**Answers:**
1. **B** - Sharding horizontally scales databases by splitting data across multiple servers (shards), enabling write scaling and storage scaling
2. **B** - Hash partitioning provides uniform distribution and simple routing but requires querying all shards for ranges; range partitioning enables efficient range queries but may create hot spots
3. **B** - A good shard key distributes data evenly across shards and matches query patterns (most queries include the shard key) to enable efficient single-shard queries

## Next Steps

- [Database Backup & Recovery](./09.%20Database%20Backup%20%26%20Recovery.md) - Data protection
- [Storage Engines - InnoDB & RocksDB](./10.%20Storage%20Engines%20-%20InnoDB%20%26%20RocksDB.md) - Engine internals

