---
number: 10
title: "Parallel Algorithms - Sort & Search"
slug: "parallel-algorithms-sort-search"
level: "intermediate"
tags: ["concurrency", "parallel-algorithms", "parallel-sort", "parallel-search", "algorithms"]
prerequisites: ["multithreaded-debugging-techniques"]
estimated_minutes: 120
contributors: []
diagrams: []
examples: []
canonical_id: "cs-conc-10"
---

# Parallel Algorithms - Sort & Search

## Overview

Parallel algorithms exploit multiple processors to solve problems faster. Understanding parallel sorting algorithms like parallel merge sort and parallel quicksort, parallel search, and performance analysis is essential for leveraging modern multi-core systems effectively.

## Table of Contents

1. [Parallel Computing Basics](#basics)
2. [Speedup & Efficiency](#speedup)
3. [Parallel Merge Sort](#parallel-merge-sort)
4. [Parallel Quick Sort](#parallel-quicksort)
5. [Parallel Search](#parallel-search)
6. [Parallel Reduction](#parallel-reduction)
7. [Load Balancing](#load-balancing)
8. [Applications](#applications)

## Parallel Computing Basics

### Parallelism Types

**1. Data Parallelism**:
```
Same operation on different data
```

**2. Task Parallelism**:
```
Different operations in parallel
```

**3. Pipeline Parallelism**:
```
Stages execute in parallel
```

### Parallel Algorithm Goals

**1. Speedup**:
```
Faster than sequential
```

**2. Scalability**:
```
Scales with processors
```

**3. Efficiency**:
```
Good processor utilization
```

## Speedup & Efficiency

### Speedup

**Speedup**: S(p) = T(1) / T(p)

**Where**:
- **T(1)**: Sequential time
- **T(p)**: Parallel time with p processors

**Ideal**: S(p) = p (linear speedup)

**Reality**: S(p) < p (due to overhead)

### Efficiency

**Efficiency**: E(p) = S(p) / p

**Range**: 0 < E(p) ≤ 1

**Ideal**: E(p) = 1 (perfect efficiency)

**Goal**: High efficiency

## Parallel Merge Sort

### Algorithm

**Process**:
```
1. Divide array into p parts
2. Sort each part in parallel
3. Merge sorted parts
```

**Merging**: 
- **Sequential**: Merge pairs sequentially
- **Parallel**: Merge multiple pairs in parallel

### Implementation

**Divide**:
```
Split array into p chunks
Each processor sorts one chunk
```

**Merge**:
```
Merge pairs of sorted chunks
Repeat until one sorted array
```

**Complexity**: 
- **Time**: O(n log n / p + n log p)
- **Speedup**: Near linear for large n

### Parallel Merge Sort Example

```
Array: [5, 2, 8, 1, 9, 3, 7, 4]
Processors: 4

Step 1: Divide
  P1: [5, 2] → [2, 5]
  P2: [8, 1] → [1, 8]
  P3: [9, 3] → [3, 9]
  P4: [7, 4] → [4, 7]

Step 2: Merge pairs (parallel)
  P1: [2, 5] + [1, 8] → [1, 2, 5, 8]
  P2: [3, 9] + [4, 7] → [3, 4, 7, 9]

Step 3: Final merge
  [1, 2, 5, 8] + [3, 4, 7, 9] → [1, 2, 3, 4, 5, 7, 8, 9]
```

## Parallel Quick Sort

### Algorithm

**Process**:
```
1. Choose pivot
2. Partition in parallel
3. Recursively sort partitions
```

**Partitioning**: Can be parallelized

**Recursion**: Independent partitions

### Implementation

**Parallel Partition**:
```
Divide array into chunks
Each processor partitions its chunk
Combine results
```

**Recursive Sort**:
```
Sort left and right partitions in parallel
```

**Complexity**: 
- **Time**: O(n log n / p) average
- **Speedup**: Good for large arrays

## Parallel Search

### Parallel Linear Search

**Method**: 
```
Divide array into p parts
Search each part in parallel
Return first match
```

**Speedup**: Near linear

**Use**: Unsorted arrays

### Parallel Binary Search

**Challenge**: Binary search is sequential

**Method**: 
```
Use parallel search trees
Or parallelize different searches
```

**Benefit**: Multiple searches in parallel

## Parallel Reduction

### What is Reduction?

**Reduction**: Combine elements into one value

**Operations**: Sum, max, min, product

**Example**: Sum of array

### Parallel Reduction

**Tree Reduction**:
```
1. Pair up elements
2. Compute partial results in parallel
3. Repeat until one result
```

**Complexity**: O(log n) with n processors

**Example**:
```
Array: [1, 2, 3, 4, 5, 6, 7, 8]

Level 1: [3, 7, 11, 15]  (parallel sums)
Level 2: [10, 26]         (parallel sums)
Level 3: [36]             (final sum)
```

## Load Balancing

### The Problem

**Load Imbalance**: Some processors do more work

**Result**: Poor efficiency

**Solution**: Load balancing

### Load Balancing Strategies

**1. Static**:
```
Divide work evenly upfront
```

**2. Dynamic**:
```
Distribute work as needed
Work-stealing
```

**3. Work-Stealing**:
```
Idle processors steal work
```

## Applications

### Application 1: Big Data Processing

**Use**: Process large datasets

**Algorithms**: Parallel sort, parallel reduction

**Benefit**: Faster processing

### Application 2: Scientific Computing

**Use**: Numerical computations

**Algorithms**: Parallel matrix operations

**Benefit**: High performance

### Application 3: Machine Learning

**Use**: Train models

**Algorithms**: Parallel gradient computation

**Benefit**: Faster training

## Real-World Examples

### Example 1: MapReduce

**Use**: Large-scale data processing

**Method**: Map (parallel) + Reduce (parallel reduction)

**Benefit**: Scalable processing

### Example 2: GPU Computing

**Use**: Parallel algorithms on GPU

**Method**: Many threads in parallel

**Benefit**: Massive parallelism

## Common Pitfalls

### Problem: Overhead Dominates

```c
// BAD: Parallelize small problem
// Overhead > benefit

// GOOD: Parallelize large problems
// Overhead < benefit
```

### Problem: Load Imbalance

```c
// BAD: Uneven work distribution
// Some processors idle

// GOOD: Balance load
// Use work-stealing
```

## Quiz

1. What is parallel speedup?
   - **A)** Number of processors
   - **B)** Ratio of sequential time to parallel time
   - **C)** Parallel time
   - **D)** Efficiency

2. What is parallel reduction?
   - **A)** Reducing processors
   - **B)** Combining elements into one value using tree structure
   - **C)** Reducing array size
   - **D)** Reducing memory

3. What is load balancing?
   - **A)** Balancing memory
   - **B)** Distributing work evenly across processors
   - **C)** Balancing speed
   - **D)** Balancing time

**Answers:**
1. **B** - Parallel speedup is the ratio of sequential execution time to parallel execution time, measuring how much faster the parallel version is
2. **B** - Parallel reduction combines array elements into a single value (like sum or max) using a tree structure where operations happen in parallel at each level
3. **B** - Load balancing distributes work evenly across processors to maximize efficiency and prevent some processors from being idle while others are overloaded

## Next Steps

- [GPU Parallel Programming](../03_advanced/01.%20GPU%20Parallel%20Programming.md) - GPU computing
- [Distributed Concurrency](../03_advanced/02.%20Distributed%20Concurrency.md) - Distributed systems

