---
number: 3
title: "Eventual Consistency"
slug: "eventual-consistency"
level: "intermediate"
tags: ["distributed-systems", "consistency", "eventual", "replication", "cassandra"]
prerequisites: ["cap-theorem-pacelc"]
estimated_minutes: 110
contributors: []
diagrams: []
examples: []
canonical_id: "cs-dist-03"
---

# Eventual Consistency

## Overview

Eventual consistency is a consistency model where distributed systems guarantee that if no new updates are made, all replicas will eventually converge to the same value. Understanding eventual consistency, conflict resolution, vector clocks, and when to use it is essential for building scalable distributed systems.

## Table of Contents

1. [Consistency Models](#consistency-models)
2. [What is Eventual Consistency?](#what-is-eventual)
3. [Eventual Consistency Properties](#properties)
4. [Conflict Resolution](#conflict-resolution)
5. [Vector Clocks](#vector-clocks)
6. [Read Repair](#read-repair)
7. [Hinted Handoff](#hinted-handoff)
8. [Tunable Consistency](#tunable)
9. [Real-World Examples](#examples)

## Consistency Models

### Strong Consistency

**Definition**: All nodes see same data simultaneously

**Guarantee**: Immediate consistency

**Trade-off**: Lower availability, higher latency

**Use**: When consistency critical (financial systems)

### Eventual Consistency

**Definition**: All nodes eventually converge to same value

**Guarantee**: Consistency after some time

**Trade-off**: Higher availability, temporary inconsistencies

**Use**: When availability critical (social media, content delivery)

### Consistency Spectrum

```
Strong Consistency ←→ Eventual Consistency
     (CP)              (AP)
```

**CAP Theorem**: Choose consistency or availability

## What is Eventual Consistency?

### Definition

**Eventual Consistency**: 
- **If**: No new updates
- **Then**: All replicas eventually have same value

**Key Points**:
- **Eventually**: Not immediately
- **Convergence**: All nodes converge
- **Temporary**: Inconsistencies temporary

### Example

**Scenario**: 3 replicas

**Update**:
```
Time T1: Node A updates X = 10
Time T2: Node B reads X → 5 (old value, not yet updated)
Time T3: Node C reads X → 5 (old value)
Time T4: Replication propagates to B and C
Time T5: Node B reads X → 10 (updated)
Time T6: Node C reads X → 10 (updated)
```

**Result**: Eventually all nodes see X = 10

## Eventual Consistency Properties

### BASE Properties

**BASE**: Basically Available, Soft state, Eventual consistency

**Basically Available**:
- **System**: Available most of the time
- **Degraded**: May have reduced functionality
- **Not**: Completely unavailable

**Soft State**:
- **State**: May change without input
- **Replication**: Replication updates state
- **Not**: Hard state (only changes with input)

**Eventual Consistency**:
- **Convergence**: Eventually consistent
- **Time**: After some time period

### Consistency Guarantees

**1. Read Your Writes**:
```
After writing, subsequent reads see your write
```

**2. Monotonic Reads**:
```
If read sees value V, future reads see V or newer
```

**3. Causal Consistency**:
```
Causally related reads see causally consistent values
```

**4. Session Consistency**:
```
Within session, reads see writes from same session
```

## Conflict Resolution

### The Problem

**Concurrent Updates**: Multiple nodes update same key

**Example**:
```
Node A: Set X = 10
Node B: Set X = 20
Both updates propagate
Which value wins?
```

### Conflict Resolution Strategies

**1. Last Write Wins (LWW)**:
```
Use timestamp
Latest timestamp wins
Simple, but may lose data
```

**2. Version Vectors**:
```
Track versions per node
Detect conflicts
Resolve based on policy
```

**3. Application-Level**:
```
Application resolves conflicts
Merge values
Custom logic
```

**4. CRDTs**:
```
Conflict-free Replicated Data Types
Automatic merging
No conflicts possible
```

### Last Write Wins Example

**Scenario**:
```
Node A: Set X = 10 (timestamp: T1)
Node B: Set X = 20 (timestamp: T2, T2 > T1)
```

**Resolution**:
```
X = 20 (T2 wins, T2 > T1)
```

**Problem**: May lose T1 update

### Application-Level Resolution

**Example**: Counter
```
Node A: Increment counter (+1)
Node B: Increment counter (+1)
Both propagate
Resolution: Counter = sum of increments
Result: Counter = +2 (not lost)
```

## Vector Clocks

### What are Vector Clocks?

**Vector Clock**: Track causality across nodes

**Structure**: Vector of timestamps (one per node)

**Example** (3 nodes):
```
Node A: [A:5, B:3, C:2]
Node B: [A:4, B:4, C:2]
Node C: [A:4, B:3, C:3]
```

### Vector Clock Operations

**1. Increment**:
```
On update at node i:
  clock[i]++
```

**2. Update**:
```
On receive from node j:
  clock[i] = max(clock[i], received_clock[j]) for all j
  clock[i]++ (for local event)
```

**3. Compare**:
```
Clock1 happens before Clock2:
  if Clock1[i] < Clock2[i] for all i
  and Clock1[j] < Clock2[j] for some j

Clock1 concurrent with Clock2:
  if neither happens before the other
```

### Vector Clock Example

**Scenario**:
```
Node A: Update X = 10, clock = [A:1, B:0, C:0]
Node B: Update X = 20, clock = [A:0, B:1, C:0]
```

**Comparison**:
```
[A:1, B:0, C:0] and [A:0, B:1, C:0]
Neither happens before the other
→ Concurrent updates → Conflict!
```

**Resolution**: Application decides (merge, LWW, etc.)

## Read Repair

### What is Read Repair?

**Read Repair**: Fix inconsistencies during read

**Process**:
```
1. Read from multiple replicas
2. Compare values
3. If inconsistent:
   a. Determine correct value
   b. Write correct value to stale replicas
4. Return value to client
```

### Read Repair Example

**Scenario**:
```
Read X from 3 replicas:
  Replica 1: X = 10 (version 5)
  Replica 2: X = 10 (version 5)
  Replica 3: X = 5 (version 3) ← Stale!
```

**Read Repair**:
```
1. Detect: Replica 3 is stale
2. Repair: Write X = 10 (version 5) to Replica 3
3. Return: X = 10 to client
```

**Benefit**: Automatically fixes inconsistencies

## Hinted Handoff

### What is Hinted Handoff?

**Hinted Handoff**: Queue updates for offline nodes

**Process**:
```
1. Node A tries to replicate to Node B
2. Node B is offline
3. Node A stores update in hint log
4. When Node B comes online:
   a. Node A sends hinted updates
   b. Node B applies updates
```

### Hinted Handoff Example

**Scenario**:
```
Node A: Update X = 10
Node B: Offline (network partition)
Node A: Store hint (X = 10, for Node B)
Node B: Comes online
Node A: Send hinted update to Node B
Node B: Apply update
```

**Benefit**: No data loss during partitions

## Tunable Consistency

### What is Tunable Consistency?

**Tunable**: Adjust consistency level

**Levels**:
- **ONE**: Read/write from one replica (fast, weak)
- **QUORUM**: Read/write from majority (balanced)
- **ALL**: Read/write from all replicas (slow, strong)

### Consistency Levels

**Write Consistency**:
```
ONE: Write to one replica
QUORUM: Write to majority
ALL: Write to all replicas
```

**Read Consistency**:
```
ONE: Read from one replica
QUORUM: Read from majority
ALL: Read from all replicas
```

### Example: Cassandra

**Write**:
```cql
INSERT INTO users (id, name) VALUES (1, 'Alice')
USING CONSISTENCY QUORUM;
```

**Read**:
```cql
SELECT * FROM users WHERE id = 1
USING CONSISTENCY QUORUM;
```

**Trade-off**: Higher consistency = slower, more reliable

## Real-World Examples

### Example 1: Amazon DynamoDB

**Consistency**: Eventually consistent by default

**Features**:
- **Replication**: Across multiple AZs
- **Read repair**: Automatic
- **Tunable**: Can use strong consistency

**Use**: High availability, can tolerate temporary inconsistencies

### Example 2: Apache Cassandra

**Consistency**: Eventually consistent

**Features**:
- **Tunable**: Configurable consistency levels
- **Vector clocks**: Track causality
- **Hinted handoff**: Handle offline nodes

**Use**: High write throughput, distributed storage

### Example 3: DNS

**Consistency**: Eventually consistent

**Features**:
- **Replication**: Multiple DNS servers
- **TTL**: Time to live
- **Propagation**: Changes propagate eventually

**Use**: Global name resolution

## Common Pitfalls

### Problem: Assuming Strong Consistency

```c
// BAD: Assume immediate consistency
write(key, value);
read(key); // May not see write immediately!

// GOOD: Understand eventual consistency
write(key, value);
// May need to wait or use read repair
read_with_repair(key);
```

### Problem: Not Handling Conflicts

```c
// BAD: Ignore conflicts
update_counter(); // May lose updates if concurrent

// GOOD: Use CRDTs or conflict resolution
update_counter_crdt(); // Automatically merges
```

## Quiz

1. What is eventual consistency?
   - **A)** Immediate consistency
   - **B)** All replicas eventually converge to same value if no new updates
   - **C)** No consistency
   - **D)** Strong consistency

2. What is the main advantage of eventual consistency?
   - **A)** Strong consistency
   - **B)** Higher availability and better performance
   - **C)** Simpler implementation
   - **D)** No conflicts

3. What are vector clocks used for?
   - **A)** Time synchronization
   - **B)** Tracking causality and detecting concurrent updates
   - **C)** Encryption
   - **D)** Compression

**Answers:**
1. **B** - Eventual consistency guarantees that if no new updates are made, all replicas will eventually converge to the same value, though there may be temporary inconsistencies
2. **B** - Eventual consistency provides higher availability (system remains available during partitions) and better performance (faster writes) compared to strong consistency
3. **B** - Vector clocks track causality across distributed nodes, allowing detection of concurrent updates and ordering of events

## Next Steps

- [Vector Clocks & Lamport Clocks](./04.%20Vector%20Clocks%20%26%20Lamport%20Clocks.md) - Causality tracking
- [Message Queues & Event Streaming](./05.%20Message%20Queues%20%26%20Event%20Streaming.md) - Asynchronous communication

