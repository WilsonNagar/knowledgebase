---
number: 1
title: "CAP Theorem & PACELC"
slug: "cap-theorem-pacelc"
level: "intermediate"
tags: ["distributed-systems", "cap", "consistency", "availability", "partition-tolerance"]
prerequisites: []
estimated_minutes: 95
contributors: []
diagrams: []
examples: []
canonical_id: "cs-dist-01"
---

# CAP Theorem & PACELC

## Overview

The CAP theorem is a fundamental principle in distributed systems that states you can only guarantee two out of three properties: Consistency, Availability, and Partition tolerance. Understanding CAP and its extension PACELC is essential for designing distributed systems and making informed trade-offs.

## Table of Contents

1. [CAP Theorem Overview](#cap-overview)
2. [Consistency](#consistency)
3. [Availability](#availability)
4. [Partition Tolerance](#partition)
5. [CAP Trade-offs](#trade-offs)
6. [PACELC Extension](#pacelc)
7. [Real-World Examples](#examples)
8. [Designing for CAP](#designing)

## CAP Theorem Overview

### What is CAP?

**CAP Theorem**: In a distributed system, you can guarantee at most 2 of 3 properties

**Properties**:
- **C (Consistency)**: All nodes see same data simultaneously
- **A (Availability)**: System responds to requests
- **P (Partition Tolerance)**: System continues despite network partitions

**Theorem**: Cannot have all three simultaneously

### Why CAP Matters

**Distributed Systems Reality**:
- **Network partitions**: Inevitable (network failures)
- **Must choose**: Which property to sacrifice
- **Design decision**: Fundamental trade-off

**Impact**: Affects system architecture, data model, consistency guarantees

## Consistency

### What is Consistency?

**Consistency**: All nodes see same data at same time

**Strong Consistency**:
- **Linearizability**: All operations appear atomic
- **Sequential consistency**: Operations appear in some sequential order
- **All nodes**: See updates immediately

### Consistency Examples

**Strongly Consistent**:
```
Node A: Write X = 10
Node B: Read X → 10 (immediately sees update)
Node C: Read X → 10 (immediately sees update)
```

**Weakly Consistent**:
```
Node A: Write X = 10
Node B: Read X → 5 (old value, not yet updated)
Node C: Read X → 10 (new value)
```

### Consistency Models

**1. Strong Consistency**:
- **Linearizability**: Strongest guarantee
- **Use**: Financial systems, critical data
- **Cost**: Lower availability

**2. Eventual Consistency**:
- **Eventually**: All nodes converge
- **Use**: Social media, content delivery
- **Cost**: Temporary inconsistencies

**3. Weak Consistency**:
- **No guarantees**: About when consistency achieved
- **Use**: Caching, best-effort systems
- **Cost**: May never be consistent

## Availability

### What is Availability?

**Availability**: System responds to requests (even if stale)

**High Availability**:
- **Uptime**: 99.9%+ (less than 1 hour downtime/year)
- **Responsive**: Always responds
- **No errors**: Returns data (may be stale)

### Availability Examples

**Highly Available**:
```
Request → Node A (down) → Try Node B → Response
Always responds, even if some nodes down
```

**Not Available**:
```
Request → All nodes down → Error/Timeout
Cannot respond
```

### Availability Metrics

**Uptime**:
- **99%**: ~3.65 days downtime/year
- **99.9%**: ~8.76 hours downtime/year
- **99.99%**: ~52.56 minutes downtime/year
- **99.999%**: ~5.26 minutes downtime/year

**MTBF/MTTR**:
- **MTBF**: Mean Time Between Failures
- **MTTR**: Mean Time To Repair
- **Availability**: MTBF / (MTBF + MTTR)

## Partition Tolerance

### What is Partition Tolerance?

**Partition**: Network split (nodes cannot communicate)

**Partition Tolerance**: System continues operating despite partitions

**Reality**: Partitions inevitable in distributed systems

### Partition Examples

**Network Partition**:
```
Data Center 1          Data Center 2
    │                      │
    │  Network Failure     │
    │  (Cannot communicate)│
    │                      │
Node A ──X── Node B
```

**System Must**:
- Continue operating
- Handle split
- Merge when partition heals

### Why Partition Tolerance is Required

**Distributed Systems**:
- **Multiple nodes**: Across networks
- **Network failures**: Inevitable
- **Must handle**: Cannot assume perfect network

**Conclusion**: Must choose between C and A when partition occurs

## CAP Trade-offs

### CP System (Consistency + Partition Tolerance)

**Characteristics**:
- **Consistency**: Strong consistency maintained
- **Partition Tolerance**: Handles partitions
- **Availability**: Sacrificed during partitions

**Behavior During Partition**:
```
Partition occurs:
  - System maintains consistency
  - Some nodes unavailable (cannot guarantee consistency)
  - Returns errors instead of stale data
```

**Examples**:
- **Traditional databases**: PostgreSQL, MySQL (with replication)
- **HBase**: Strong consistency
- **MongoDB**: With strong consistency mode

### AP System (Availability + Partition Tolerance)

**Characteristics**:
- **Availability**: Always responds
- **Partition Tolerance**: Handles partitions
- **Consistency**: Sacrificed (eventual consistency)

**Behavior During Partition**:
```
Partition occurs:
  - System remains available
  - Returns data (may be stale)
  - Consistency restored when partition heals
```

**Examples**:
- **DynamoDB**: Eventually consistent
- **Cassandra**: Eventually consistent
- **CouchDB**: Eventually consistent

### CA System (Consistency + Availability)

**Characteristics**:
- **Consistency**: Strong consistency
- **Availability**: High availability
- **Partition Tolerance**: Not handled

**Reality**: **Impossible in distributed systems**

**Why**: Partitions inevitable, must choose C or A

**Single-Node Systems**: Can be CA (no partitions possible)

## PACELC Extension

### What is PACELC?

**PACELC**: Extension of CAP theorem

**When Partition (P)**:
- **A or C**: Choose Availability or Consistency

**Else (E - Else, when no partition)**:
- **L or C**: Choose Latency or Consistency

### PACELC Explained

**P-A-C-E-L-C**:
- **P**: Partition tolerance (required)
- **A**: Availability
- **C**: Consistency
- **E**: Else (no partition)
- **L**: Latency
- **C**: Consistency

**Meaning**: 
- **During partition**: Choose A or C (same as CAP)
- **During normal operation**: Choose L or C

### PACELC Examples

**PACELC/EC** (Partition: A, Else: C):
- **Partition**: Availability over consistency
- **Normal**: Consistency over latency
- **Example**: DynamoDB (eventual consistency, but consistent when possible)

**PACELC/EL** (Partition: A, Else: L):
- **Partition**: Availability over consistency
- **Normal**: Latency over consistency
- **Example**: Cassandra (low latency, eventual consistency)

**PACELC/PC** (Partition: C, Else: C):
- **Partition**: Consistency over availability
- **Normal**: Consistency over latency
- **Example**: Traditional databases (strong consistency always)

## Real-World Examples

### Example 1: DynamoDB (AP/EC)

**CAP**: AP (Availability + Partition Tolerance)

**PACELC**: PACELC/EC

**Characteristics**:
- **Partition**: Available, eventual consistency
- **Normal**: Consistent (when possible)
- **Use**: Web applications, high availability needed

**Trade-off**: Accepts temporary inconsistencies for availability

### Example 2: PostgreSQL (CP)

**CAP**: CP (Consistency + Partition Tolerance)

**PACELC**: PACELC/PC

**Characteristics**:
- **Partition**: Consistent, may be unavailable
- **Normal**: Consistent
- **Use**: Financial systems, critical data

**Trade-off**: May return errors during partitions to maintain consistency

### Example 3: Cassandra (AP/EL)

**CAP**: AP (Availability + Partition Tolerance)

**PACELC**: PACELC/EL

**Characteristics**:
- **Partition**: Available, eventual consistency
- **Normal**: Low latency, eventual consistency
- **Use**: High-throughput, low-latency systems

**Trade-off**: Prioritizes latency and availability over strong consistency

### Example 4: MongoDB (Configurable)

**CAP**: Can be CP or AP (depending on configuration)

**Write Concern**:
- **Majority**: CP (waits for majority)
- **Unacknowledged**: AP (fire and forget)

**Trade-off**: Configurable based on needs

## Designing for CAP

### Step 1: Identify Requirements

**Questions**:
- **Consistency critical?**: Financial data, user accounts
- **Availability critical?**: Web content, social media
- **Latency critical?**: Real-time systems, gaming

### Step 2: Choose CAP Trade-off

**CP System**:
- **When**: Consistency critical
- **Examples**: Financial systems, user accounts
- **Cost**: May be unavailable during partitions

**AP System**:
- **When**: Availability critical
- **Examples**: Content delivery, social media
- **Cost**: Temporary inconsistencies

### Step 3: Implement Consistency Model

**Strong Consistency**:
- **Quorum**: Read/write from majority
- **Two-phase commit**: Coordinate writes
- **Linearizability**: Strong ordering

**Eventual Consistency**:
- **Vector clocks**: Track causality
- **Conflict resolution**: Merge strategies
- **Read repair**: Fix inconsistencies

### Step 4: Handle Partitions

**Detection**:
- **Heartbeats**: Detect node failures
- **Timeouts**: Detect network issues
- **Quorum**: Detect partition

**Recovery**:
- **Merge strategies**: Resolve conflicts
- **Read repair**: Fix inconsistencies
- **Hinted handoff**: Queue updates

## Common Pitfalls

### Problem: Assuming CA System

```c
// BAD: Assume perfect network
distributed_write(data);
distributed_read(); // Assume immediately consistent

// GOOD: Understand CAP trade-offs
if (needs_consistency) {
    quorum_write(data); // CP system
} else {
    eventual_write(data); // AP system
}
```

### Problem: Ignoring Partition Tolerance

```c
// BAD: Assume network always works
send_to_all_nodes(data);
// Partition occurs → System breaks

// GOOD: Handle partitions
if (partition_detected()) {
    if (consistency_critical) {
        return_error(); // CP
    } else {
        return_stale_data(); // AP
    }
}
```

## Quiz

1. According to CAP theorem, how many properties can you guarantee simultaneously?
   - **A)** All three
   - **B)** Two out of three
   - **C)** One
   - **D)** None

2. What does PACELC add to CAP?
   - **A)** More properties
   - **B)** Considers latency trade-offs during normal operation
   - **C)** Eliminates partition tolerance
   - **D)** Makes CA possible

3. Why is CA impossible in distributed systems?
   - **A)** Too expensive
   - **B)** Network partitions are inevitable, forcing choice between C and A
   - **C)** Too complex
   - **D)** Not needed

**Answers:**
1. **B** - CAP theorem states you can guarantee at most 2 out of 3 properties (Consistency, Availability, Partition tolerance)
2. **B** - PACELC extends CAP by considering latency vs consistency trade-offs during normal operation (when there's no partition)
3. **B** - CA is impossible because network partitions are inevitable in distributed systems, forcing you to choose between consistency and availability when a partition occurs

## Next Steps

- [Consensus Algorithms - Paxos & Raft](./02.%20Consensus%20Algorithms%20-%20Paxos%20%26%20Raft.md) - Achieving consensus
- [Eventual Consistency](./03.%20Eventual%20Consistency.md) - Consistency models

