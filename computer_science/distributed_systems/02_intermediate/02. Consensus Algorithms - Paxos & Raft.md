---
number: 2
title: "Consensus Algorithms - Paxos & Raft"
slug: "consensus-algorithms-paxos-raft"
level: "intermediate"
tags: ["distributed-systems", "consensus", "paxos", "raft", "replication"]
prerequisites: ["cap-theorem-pacelc"]
estimated_minutes: 130
contributors: []
diagrams: []
examples: []
canonical_id: "cs-dist-02"
---

# Consensus Algorithms - Paxos & Raft

## Overview

Consensus algorithms enable distributed systems to agree on values despite failures. Understanding Paxos and Raft, the two most important consensus algorithms, is essential for building reliable distributed systems, distributed databases, and replicated state machines.

## Table of Contents

1. [The Consensus Problem](#consensus-problem)
2. [Paxos Overview](#paxos-overview)
3. [Paxos Algorithm](#paxos-algorithm)
4. [Multi-Paxos](#multi-paxos)
5. [Raft Overview](#raft-overview)
6. [Raft Algorithm](#raft-algorithm)
7. [Leader Election](#leader-election)
8. [Log Replication](#log-replication)
9. [Paxos vs Raft](#comparison)
10. [Real-World Usage](#usage)

## The Consensus Problem

### What is Consensus?

**Consensus**: Multiple nodes agree on a value

**Requirements**:
- **Agreement**: All nodes decide same value
- **Validity**: Decided value was proposed by some node
- **Termination**: All nodes eventually decide
- **Fault tolerance**: Works despite failures

### Why Consensus Matters

**Use Cases**:
- **Distributed databases**: Replicate data consistently
- **Configuration management**: Agree on configuration
- **Leader election**: Choose single leader
- **State machine replication**: Replicate state

### The Challenge

**Failures**:
- **Node failures**: Nodes may crash
- **Network partitions**: Messages may be lost
- **Byzantine failures**: Nodes may be malicious (not covered here)

**Requirement**: Must work despite failures

## Paxos Overview

### What is Paxos?

**Paxos**: Consensus algorithm (Leslie Lamport, 1998)

**Goal**: Agree on single value

**Properties**:
- **Safety**: Never agree on wrong value
- **Liveness**: Eventually agree (if majority alive)
- **Fault tolerant**: Works with minority failures

### Paxos Roles

**Roles**:
- **Proposer**: Proposes values
- **Acceptor**: Accepts proposals
- **Learner**: Learns chosen value

**Nodes**: Can play multiple roles

### Paxos Phases

**Two Phases**:
1. **Prepare Phase**: Prepare proposal
2. **Accept Phase**: Accept proposal

**Goal**: Ensure at most one value is chosen

## Paxos Algorithm

### Basic Paxos

**Phase 1: Prepare**:
```
Proposer:
  1. Choose proposal number n (unique, increasing)
  2. Send PREPARE(n) to majority of acceptors
  3. Wait for responses

Acceptor:
  1. If n > highest seen: Promise not to accept < n
  2. If already accepted value: Return (n_old, v_old)
  3. Send PROMISE(n) or PROMISE(n, n_old, v_old)
```

**Phase 2: Accept**:
```
Proposer:
  1. If majority promised:
     a. If responses include value: Use highest n's value
     b. Else: Use own value
  2. Send ACCEPT(n, v) to majority
  3. Wait for responses

Acceptor:
  1. If n >= highest promised: Accept (n, v)
  2. Send ACCEPTED(n, v)
```

**Chosen**: Value accepted by majority

### Paxos Example

**Scenario**: Choose value "X"

**Round 1**:
```
Proposer P1: PREPARE(1)
Acceptors: A1, A2, A3
  A1: PROMISE(1)
  A2: PROMISE(1)
  A3: (no response - failed)

P1: Has majority (2/3), sends ACCEPT(1, "X")
  A1: ACCEPTED(1, "X")
  A2: ACCEPTED(1, "X")
  
Result: "X" chosen (majority accepted)
```

**Round 2** (if P1 failed):
```
Proposer P2: PREPARE(2)
Acceptors:
  A1: PROMISE(2) (but already accepted (1, "X"))
  A2: PROMISE(2, 1, "X") (returns accepted value)
  A3: PROMISE(2)

P2: Receives (1, "X"), must use "X"
P2: Sends ACCEPT(2, "X") (not own value!)
  A1: ACCEPTED(2, "X")
  A2: ACCEPTED(2, "X")
  A3: ACCEPTED(2, "X")
  
Result: "X" still chosen (safety preserved)
```

### Paxos Safety

**Safety Property**: At most one value chosen

**How**: 
- **Prepare**: Ensures no older proposals accepted
- **Accept**: Only if majority promised
- **Value**: Must use highest-numbered accepted value

**Guarantee**: Even with concurrent proposers, at most one value chosen

## Multi-Paxos

### The Problem

**Basic Paxos**: Agrees on single value

**Need**: Agree on sequence of values (log)

**Solution**: Multi-Paxos

### Multi-Paxos

**Idea**: 
- **Leader**: One proposer (elected)
- **Skip Prepare**: After first prepare, skip for subsequent values
- **Efficient**: Only one prepare phase

**Process**:
```
1. Elect leader (using Paxos)
2. Leader proposes values sequentially
3. Skip prepare phase (already leader)
4. Direct to accept phase
```

**Benefits**: More efficient than running Paxos for each value

## Raft Overview

### What is Raft?

**Raft**: Consensus algorithm (Diego Ongaro, 2014)

**Goal**: Understandable alternative to Paxos

**Design**: 
- **Understandable**: Easier to understand than Paxos
- **Equivalent**: Same safety guarantees
- **Practical**: Used in production systems

### Raft Properties

**Safety**:
- **Election safety**: At most one leader per term
- **Log matching**: Logs match on leaders
- **Leader completeness**: Committed entries in all future leaders

**Liveness**:
- **Leader election**: Eventually elect leader
- **Log replication**: Eventually replicate entries

## Raft Algorithm

### Raft States

**States**:
- **Leader**: Handles client requests, replicates log
- **Follower**: Receives log entries from leader
- **Candidate**: Running for election

**Transitions**:
```
Follower → Candidate → Leader
Leader → Follower (if term increases)
Candidate → Follower (if other leader elected)
```

### Raft Terms

**Term**: Logical time (increments)

**Purpose**: Detect stale leaders

**Structure**:
```
Term 1: Leader A
Term 2: Leader B (A becomes follower)
Term 3: Leader C (B becomes follower)
```

## Leader Election

### Election Process

**Trigger**: Follower timeout (no heartbeat from leader)

**Process**:
```
1. Follower becomes Candidate
2. Increment term
3. Vote for self
4. Send RequestVote to all other nodes
5. Wait for responses:
   a. Majority votes: Become Leader
   b. Receive higher term: Become Follower
   c. Timeout: Start new election
```

### RequestVote RPC

**Request**:
```
term: Candidate's term
candidateId: Candidate's ID
lastLogIndex: Index of last log entry
lastLogTerm: Term of last log entry
```

**Response**:
```
term: Current term
voteGranted: true if vote granted
```

**Vote Granting**:
```
Grant vote if:
  1. candidateTerm > currentTerm AND
  2. (votedFor is null OR votedFor == candidateId) AND
  3. candidate's log is at least as up-to-date
```

### Election Example

**Scenario**: 5 nodes, node 3 times out

```
Node 3: Becomes Candidate (term 2)
  Sends RequestVote(term=2, candidateId=3) to all
  Votes: Node 3 (self), Node 1, Node 2
  Result: 3/5 votes → Becomes Leader

Node 3: Sends heartbeats to all
  Nodes 1, 2, 4, 5: Become followers
```

## Log Replication

### Log Structure

**Log Entry**:
```
┌──────┬──────┬──────────┐
│ Term │ Index│ Command   │
└──────┴──────┴──────────┘
```

**Example**:
```
Entry 1: term=1, index=1, command="SET x=10"
Entry 2: term=1, index=2, command="SET y=20"
Entry 3: term=2, index=3, command="SET x=30"
```

### Replication Process

**Leader**:
```
1. Receive client request
2. Append to local log
3. Send AppendEntries to all followers
4. Wait for majority responses
5. If majority: Commit entry, apply to state machine
6. Notify client of success
```

**Follower**:
```
1. Receive AppendEntries
2. Check consistency (prevLogIndex, prevLogTerm)
3. If consistent: Append entries
4. Send success response
5. If inconsistent: Send failure, leader will resend
```

### AppendEntries RPC

**Request**:
```
term: Leader's term
leaderId: Leader's ID
prevLogIndex: Index of entry before new entries
prevLogTerm: Term of entry before new entries
entries: Log entries to append
leaderCommit: Leader's commit index
```

**Response**:
```
term: Current term
success: true if entries appended
```

**Consistency Check**:
```
Follower checks:
  log[prevLogIndex].term == prevLogTerm
  
If match: Append entries
If mismatch: Reject, leader will backtrack
```

### Commit Rules

**Leader Commit**:
```
Entry committed when:
  1. Replicated to majority AND
  2. Leader's term matches entry's term
```

**Follower Commit**:
```
Follower commits when:
  leaderCommit > follower's commitIndex
  Apply entries up to leaderCommit
```

## Paxos vs Raft

### Comparison

| Aspect | Paxos | Raft |
|--------|-------|------|
| **Understandability** | Complex | Simpler |
| **Leader** | Optional | Required |
| **Log** | Unordered | Ordered |
| **Safety** | Same | Same |
| **Performance** | Similar | Similar |

### When to Use What

**Paxos**:
- **Use**: When you need flexibility
- **Complex**: More complex to implement
- **Examples**: Google Chubby, Amazon DynamoDB

**Raft**:
- **Use**: When you want understandability
- **Simpler**: Easier to implement correctly
- **Examples**: etcd, Consul, CockroachDB

## Real-World Usage

### Example 1: etcd (Raft)

**etcd**: Distributed key-value store

**Uses Raft for**:
- **Consensus**: Agree on key-value updates
- **Leader election**: Single leader handles writes
- **Replication**: Replicate log to all nodes

**Benefits**: Consistent, highly available

### Example 2: Google Chubby (Paxos)

**Chubby**: Distributed lock service

**Uses Paxos for**:
- **Consensus**: Agree on lock ownership
- **Configuration**: Agree on configuration
- **Leader election**: Elect master

**Benefits**: Reliable, fault-tolerant

### Example 3: CockroachDB (Raft)

**CockroachDB**: Distributed SQL database

**Uses Raft for**:
- **Range replication**: Each range uses Raft
- **Consensus**: Agree on transactions
- **Consistency**: Strong consistency

**Benefits**: ACID transactions, high availability

## Common Pitfalls

### Problem: Split Vote

```c
// BAD: Even number of nodes
// 4 nodes: 2 vote for A, 2 vote for B
// No majority → No leader → Retry

// GOOD: Odd number of nodes
// 5 nodes: 3 vote for A → Leader elected
```

### Problem: Network Partition

```c
// Scenario: 5 nodes split 3-2
// Partition 1: 3 nodes (majority) → Can elect leader
// Partition 2: 2 nodes (minority) → Cannot elect leader
// Partition 1: Continues operating
// Partition 2: Blocks (cannot make progress)
```

## Quiz

1. What is the main goal of consensus algorithms?
   - **A)** Fast execution
   - **B)** Multiple nodes agree on a value despite failures
   - **C)** Load balancing
   - **D)** Encryption

2. What is the main difference between Paxos and Raft?
   - **A)** Raft is faster
   - **B)** Raft is designed to be more understandable while providing same safety guarantees
   - **C)** Paxos doesn't work
   - **D)** No difference

3. In Raft, when is a log entry committed?
   - **A)** When written to leader
   - **B)** When replicated to majority and leader's term matches
   - **C)** Immediately
   - **D)** After timeout

**Answers:**
1. **B** - Consensus algorithms enable multiple nodes in a distributed system to agree on a value even when some nodes fail or the network partitions
2. **B** - Raft was designed to be more understandable than Paxos while providing equivalent safety guarantees, making it easier to implement correctly
3. **B** - In Raft, a log entry is committed when it's replicated to a majority of nodes AND the leader's current term matches the entry's term (ensuring it won't be overwritten)

## Next Steps

- [Eventual Consistency](../distributed_systems/03.%20Eventual%20Consistency.md) - Consistency models
- [Vector Clocks & Lamport Clocks](../distributed_systems/04.%20Vector%20Clocks%20%26%20Lamport%20Clocks.md) - Causality tracking

