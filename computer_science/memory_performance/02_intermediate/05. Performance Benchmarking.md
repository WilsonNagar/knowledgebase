---
number: 5
title: "Performance Benchmarking"
slug: "performance-benchmarking"
level: "intermediate"
tags: ["performance", "benchmarking", "measurement", "profiling", "optimization"]
prerequisites: ["memory-profiling-analysis"]
estimated_minutes: 135
contributors: []
diagrams: []
examples: []
canonical_id: "cs-mem-05"
---

# Performance Benchmarking

## Overview

Performance benchmarking measures and compares system performance. Understanding benchmarking methodologies, tools, statistical analysis, and best practices is essential for evaluating performance improvements and making informed optimization decisions.

## Table of Contents

1. [What is Benchmarking?](#what-is-benchmarking)
2. [Benchmarking Types](#types)
3. [Benchmarking Methodology](#methodology)
4. [Benchmarking Tools](#tools)
5. [Statistical Analysis](#statistical)
6. [Microbenchmarks](#microbenchmarks)
7. [Macrobenchmarks](#macrobenchmarks)
8. [Best Practices](#best-practices)

## What is Benchmarking?

### Definition

**Benchmarking**: Measure performance

**Purpose**: 
- **Compare**: Compare implementations
- **Optimize**: Guide optimization
- **Validate**: Validate improvements
- **Monitor**: Monitor performance

**Metrics**: 
- **Throughput**: Operations per second
- **Latency**: Time per operation
- **Resource Usage**: CPU, memory usage

### Benchmarking Goals

**1. Accuracy**:
```
Accurate measurements
```

**2. Reproducibility**:
```
Reproducible results
```

**3. Relevance**:
```
Relevant to real usage
```

**4. Comparability**:
```
Comparable results
```

## Benchmarking Types

### Types

**1. Microbenchmarks**:
```
Small, focused benchmarks
```

**2. Macrobenchmarks**:
```
Full application benchmarks
```

**3. Synthetic Benchmarks**:
```
Artificial workloads
```

**4. Real-World Benchmarks**:
```
Real workloads
```

### Microbenchmarks

**Purpose**: Measure specific operations

**Example**: 
```
Measure hash table lookup time
```

**Use**: Optimize specific operations

**Limitation**: May not reflect real performance

### Macrobenchmarks

**Purpose**: Measure full application

**Example**: 
```
Measure web server throughput
```

**Use**: Overall performance

**Benefit**: Reflects real performance

## Benchmarking Methodology

### Methodology Steps

**1. Define Goals**:
```
What to measure
```

**2. Choose Metrics**:
```
Which metrics
```

**3. Prepare Environment**:
```
Stable environment
```

**4. Run Benchmarks**:
```
Run multiple times
```

**5. Analyze Results**:
```
Statistical analysis
```

**6. Report**:
```
Report findings
```

### Environment Preparation

**1. Isolate System**:
```
Minimize interference
```

**2. Warm Up**:
```
Warm up system
```

**3. Stabilize**:
```
Stabilize conditions
```

**4. Control Variables**:
```
Control variables
```

## Benchmarking Tools

### C/C++ Tools

**1. Google Benchmark**:
```
C++ benchmarking framework
```

**2. Catch2**:
```
Testing and benchmarking
```

**3. Celero**:
```
C++ benchmarking
```

**4. perf**:
```
Linux performance profiler
```

### Java Tools

**1. JMH**:
```
Java Microbenchmark Harness
```

**2. Caliper**:
```
Google Caliper
```

**3. JProfiler**:
```
Java profiler
```

### Python Tools

**1. timeit**:
```
Built-in timing
```

**2. pytest-benchmark**:
```
pytest benchmarking
```

**3. py-spy**:
```
Sampling profiler
```

## Statistical Analysis

### Statistical Measures

**1. Mean**:
```
Average value
```

**2. Median**:
```
Middle value
```

**3. Standard Deviation**:
```
Variability measure
```

**4. Percentiles**:
```
P50, P95, P99
```

### Statistical Analysis

**Multiple Runs**:
```
Run multiple times
```

**Outlier Removal**:
```
Remove outliers
```

**Confidence Intervals**:
```
Calculate confidence intervals
```

**Significance Testing**:
```
Test statistical significance
```

## Microbenchmarks

### Microbenchmark Example

**Google Benchmark**:
```cpp
#include <benchmark/benchmark.h>

static void BM_VectorPushBack(benchmark::State& state) {
    for (auto _ : state) {
        std::vector<int> v;
        v.push_back(42);
    }
}
BENCHMARK(BM_VectorPushBack);

BENCHMARK_MAIN();
```

**JMH**:
```java
@BenchmarkMode(Mode.AverageTime)
@OutputTimeUnit(TimeUnit.NANOSECONDS)
public class MyBenchmark {
    @Benchmark
    public void testMethod() {
        // Code to benchmark
    }
}
```

## Macrobenchmarks

### Macrobenchmark Example

**Web Server Benchmark**:
```
Measure requests per second
Measure latency
Measure resource usage
```

**Database Benchmark**:
```
Measure query performance
Measure throughput
Measure latency
```

## Best Practices

### Practice 1: Multiple Runs

**Run Multiple Times**:
```
Run benchmarks multiple times
Calculate statistics
```

**Benefit**: More reliable results

### Practice 2: Warm Up

**Warm Up**:
```
Warm up system before benchmarking
```

**Benefit**: More accurate results

### Practice 3: Control Environment

**Control Environment**:
```
Minimize interference
Stable conditions
```

**Benefit**: Reproducible results

### Practice 4: Measure Right Things

**Measure Right Metrics**:
```
Measure relevant metrics
```

**Benefit**: Meaningful results

## Real-World Examples

### Example 1: Algorithm Comparison

**Benchmark**: Compare sorting algorithms

**Result**: Identify fastest algorithm

**Use**: Choose best algorithm

### Example 2: Optimization Validation

**Benchmark**: Before and after optimization

**Result**: Measure improvement

**Use**: Validate optimization

## Common Pitfalls

### Problem: Insufficient Runs

```c
// BAD: Single run
// Not reliable

// GOOD: Multiple runs
// Statistical analysis
```

### Problem: Wrong Metrics

```c
// BAD: Measure wrong thing
// Misleading results

// GOOD: Measure relevant metrics
// Meaningful results
```

## Quiz

1. What is benchmarking?
   - **A)** Code review
   - **B)** Measuring and comparing performance
   - **C)** Testing
   - **D)** Debugging

2. What is a microbenchmark?
   - **A)** Full application benchmark
   - **B)** Small, focused benchmark for specific operations
   - **C)** Real-world benchmark
   - **D)** Synthetic benchmark

3. Why run benchmarks multiple times?
   - **A)** For fun
   - **B)** For statistical reliability and to account for variability
   - **C)** To waste time
   - **D)** To test

**Answers:**
1. **B** - Benchmarking measures and compares system performance to evaluate implementations and guide optimization
2. **B** - A microbenchmark measures specific operations in isolation, useful for optimizing particular code paths
3. **B** - Multiple runs enable statistical analysis (mean, median, standard deviation) to account for variability and ensure reliable results

## Next Steps

- [Algorithmic Optimization](./06.%20Algorithmic%20Optimization.md) - Algorithm optimization
- [Profiling Tools](./07.%20Profiling%20Tools%20-%20perf%2C%20Valgrind%2C%20Instruments.md) - Profiling tools

