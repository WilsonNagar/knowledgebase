---
number: 1
title: "Linear Algebra Advanced - Eigenvalues, SVD"
slug: "linear-algebra-advanced-eigenvalues-svd"
level: "intermediate"
tags: ["mathematics", "linear-algebra", "eigenvalues", "svd", "matrix-decomposition"]
prerequisites: ["combinatorics"]
estimated_minutes: 145
contributors: []
diagrams: []
examples: []
canonical_id: "cs-math-int-01"
---

# Linear Algebra Advanced - Eigenvalues, SVD

## Overview

Advanced linear algebra covers eigenvalues, eigenvectors, matrix decompositions, and Singular Value Decomposition (SVD). Understanding these concepts is essential for machine learning, data analysis, signal processing, and computer graphics.

## Table of Contents

1. [Eigenvalues and Eigenvectors](#eigenvalues)
2. [Eigenvalue Decomposition](#eigenvalue-decomposition)
3. [Singular Value Decomposition](#svd)
4. [Principal Component Analysis](#pca)
5. [Matrix Decompositions](#decompositions)
6. [Applications](#applications)
7. [Computational Methods](#computational)
8. [Real-World Examples](#examples)

## Eigenvalues and Eigenvectors

### What are Eigenvalues?

**Eigenvalue**: Scalar λ

**Eigenvector**: Non-zero vector v

**Definition**: 
```
Av = λv
```

**Meaning**: 
- **Direction**: Eigenvector direction unchanged
- **Scaling**: Eigenvalue scales eigenvector

**Use**: Matrix analysis, transformations

### Finding Eigenvalues

**Characteristic Equation**: 
```
det(A - λI) = 0
```

**Eigenvalues**: Roots of characteristic polynomial

**Example**: 
```
A = [[2, 1],
     [1, 2]]
det(A - λI) = (2-λ)² - 1 = 0
λ = 1, 3
```

## Eigenvalue Decomposition

### What is Eigenvalue Decomposition?

**Eigenvalue Decomposition**: 
```
A = PDP^(-1)
```

**Where**: 
- **P**: Matrix of eigenvectors
- **D**: Diagonal matrix of eigenvalues
- **P^(-1)**: Inverse of P

**Use**: Matrix powers, exponentials

### Diagonalization

**Diagonalizable**: 
```
A = PDP^(-1)
```

**Condition**: n linearly independent eigenvectors

**Use**: Simplify matrix operations

## Singular Value Decomposition

### What is SVD?

**SVD**: Singular Value Decomposition

**Decomposition**: 
```
A = UΣV^T
```

**Where**: 
- **U**: Left singular vectors
- **Σ**: Singular values (diagonal)
- **V**: Right singular vectors

**Properties**: 
- **Always exists**: For any matrix
- **Unique**: Up to sign
- **Rank**: Number of non-zero singular values

**Use**: Dimensionality reduction, compression

### SVD Properties

**Singular Values**: 
```
σ₁ ≥ σ₂ ≥ ... ≥ σᵣ > 0
```

**Rank**: 
```
rank(A) = number of non-zero σᵢ
```

**Low-Rank Approximation**: 
```
A ≈ UₖΣₖVₖ^T
```

**Use**: Data compression

## Principal Component Analysis

### What is PCA?

**PCA**: Principal Component Analysis

**Method**: 
```
1. Center data
2. Compute covariance matrix
3. Find eigenvectors (principal components)
4. Project to lower dimension
```

**Use**: Dimensionality reduction

### PCA with SVD

**PCA via SVD**: 
```
1. Center data matrix X
2. Compute SVD: X = UΣV^T
3. Principal components = columns of V
4. Project: X' = XVₖ
```

**Benefit**: Efficient computation

## Matrix Decompositions

### Decomposition Types

**1. Eigenvalue Decomposition**:
```
A = PDP^(-1)
```

**2. SVD**:
```
A = UΣV^T
```

**3. QR Decomposition**:
```
A = QR
```

**4. LU Decomposition**:
```
A = LU
```

**5. Cholesky Decomposition**:
```
A = LL^T
```

### When to Use Each

**Eigenvalue**: Square, diagonalizable matrices

**SVD**: Any matrix, always works

**QR**: Solving linear systems

**LU**: Matrix factorization

**Cholesky**: Positive definite matrices

## Applications

### Application 1: Machine Learning

**Use**: Feature extraction

**SVD**: Dimensionality reduction

**PCA**: Principal components

**Result**: Reduced features

### Application 2: Image Compression

**Use**: Compress images

**SVD**: Low-rank approximation

**Method**: Keep top singular values

**Result**: Compressed image

## Computational Methods

### Computing Eigenvalues

**Methods**: 
- **Power Method**: Largest eigenvalue
- **QR Algorithm**: All eigenvalues
- **Jacobi Method**: Symmetric matrices

**Complexity**: O(n³) typically

### Computing SVD

**Methods**: 
- **Golub-Reinsch**: Standard algorithm
- **Randomized SVD**: For large matrices

**Complexity**: O(mn²) for m×n matrix

## Real-World Examples

### Example 1: Face Recognition

**Use**: Face recognition

**Method**: 
- **SVD**: Extract features
- **PCA**: Reduce dimensions
- **Classification**: Classify faces

**Result**: Face recognition system

### Example 2: Recommendation Systems

**Use**: Recommendations

**Method**: 
- **SVD**: Matrix factorization
- **Low-Rank**: Approximate user-item matrix
- **Predictions**: Predict ratings

**Result**: Recommendation engine

## Common Pitfalls

### Problem: Numerical Instability

```c
// BAD: Direct computation
// Numerical errors

// GOOD: Use stable algorithms
// QR, SVD methods
```

## Quiz

1. What is an eigenvalue?
   - **A)** Vector
   - **B)** Scalar λ where Av = λv for eigenvector v
   - **C)** Matrix
   - **D)** Function

2. What is SVD?
   - **A)** Eigenvalue decomposition
   - **B)** Singular Value Decomposition A = UΣV^T for any matrix
   - **C)** QR decomposition
   - **D)** LU decomposition

3. What is PCA?
   - **A)** Matrix multiplication
   - **B)** Principal Component Analysis using eigenvectors for dimensionality reduction
   - **C)** SVD
   - **D)** Eigenvalue computation

**Answers:**
1. **B** - Eigenvalue λ satisfies Av = λv, where v is eigenvector; represents scaling factor in eigenvector direction
2. **B** - SVD decomposes any matrix A = UΣV^T with U (left singular vectors), Σ (singular values), V (right singular vectors)
3. **B** - PCA finds principal components (eigenvectors of covariance) to reduce dimensionality while preserving variance

## Next Steps

- [Statistics Advanced](./02.%20Statistics%20Advanced%20-%20Hypothesis%20Testing%2C%20Bayesian.md) - Hypothesis testing
- [Optimization Theory](./03.%20Optimization%20Theory%20-%20Convex%20Optimization.md) - Convex optimization

