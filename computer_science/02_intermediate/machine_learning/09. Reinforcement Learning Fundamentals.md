---
number: 9
title: "Reinforcement Learning Fundamentals"
slug: "reinforcement-learning-fundamentals"
level: "intermediate"
tags: ["machine-learning", "reinforcement-learning", "rl", "q-learning", "policy"]
prerequisites: ["vector-embeddings-word2vec"]
estimated_minutes: 150
contributors: []
diagrams: []
examples: []
canonical_id: "cs-ml-int-09"
---

# Reinforcement Learning Fundamentals

## Overview

Reinforcement Learning (RL) is a machine learning paradigm where agents learn to make decisions by interacting with an environment. Understanding RL concepts, Q-learning, policy gradients, and value functions is essential for building intelligent agents and autonomous systems.

## Table of Contents

1. [What is Reinforcement Learning?](#what-is-rl)
2. [RL Components](#rl-components)
3. [Markov Decision Process](#mdp)
4. [Value Functions](#value-functions)
5. [Q-Learning](#q-learning)
6. [Policy Gradients](#policy-gradients)
7. [Exploration vs Exploitation](#exploration)
8. [Applications](#applications)

## What is Reinforcement Learning?

### Definition

**Reinforcement Learning**: Learning through interaction

**Components**: 
- **Agent**: Decision maker
- **Environment**: World agent interacts with
- **Actions**: Available actions
- **Rewards**: Feedback signal
- **State**: Current situation

**Goal**: Maximize cumulative reward

### RL vs Supervised Learning

**Supervised Learning**: 
```
Labeled examples
Learn mapping
```

**Reinforcement Learning**: 
```
No labels
Learn from rewards
Trial and error
```

**Difference**: No labeled data, delayed rewards

## RL Components

### Agent-Environment Interaction

**Loop**: 
```
1. Agent observes state
2. Agent selects action
3. Environment transitions
4. Agent receives reward
5. Repeat
```

**Goal**: Learn optimal policy

### Key Concepts

**1. State (s)**: Current situation

**2. Action (a)**: Available actions

**3. Reward (r)**: Immediate feedback

**4. Policy (π)**: Action selection strategy

**5. Value Function**: Expected future reward

## Markov Decision Process

### What is MDP?

**MDP**: Markov Decision Process

**Components**: 
- **States (S)**: State space
- **Actions (A)**: Action space
- **Transition (P)**: P(s'|s,a)
- **Reward (R)**: R(s,a,s')
- **Discount (γ)**: Future discount factor

**Markov Property**: 
```
Future depends only on current state
P(s_{t+1}|s_t, a_t) = P(s_{t+1}|s_t, a_t, history)
```

**Use**: Formal RL framework

### MDP Example

**Example**: Grid World
```
States: Grid positions
Actions: Up, Down, Left, Right
Reward: +10 at goal, -1 per step
```

**Goal**: Find optimal path

## Value Functions

### State Value Function

**V^π(s)**: Expected return from state s

**Definition**: 
```
V^π(s) = E[G_t | S_t = s]
```

**Where**: 
```
G_t = R_{t+1} + γR_{t+2} + γ²R_{t+3} + ...
```

**Use**: Evaluate policy

### Action Value Function

**Q^π(s,a)**: Expected return from state s, action a

**Definition**: 
```
Q^π(s,a) = E[G_t | S_t = s, A_t = a]
```

**Use**: Evaluate actions

### Bellman Equation

**Bellman Equation**: 
```
V^π(s) = Σ_a π(a|s) Σ_{s'} P(s'|s,a)[R(s,a,s') + γV^π(s')]
```

**Meaning**: Value = immediate reward + discounted future value

**Use**: Compute values

## Q-Learning

### What is Q-Learning?

**Q-Learning**: Model-free RL algorithm

**Q-Value Update**: 
```
Q(s,a) ← Q(s,a) + α[r + γ max_{a'} Q(s',a') - Q(s,a)]
```

**Properties**: 
- **Off-Policy**: Learn optimal policy
- **Model-Free**: No transition model needed
- **Convergence**: Converges to optimal Q*

**Use**: Discrete state/action spaces

### Q-Learning Algorithm

**Algorithm**: 
```
1. Initialize Q(s,a) arbitrarily
2. For each episode:
   a. Observe state s
   b. Choose action a (ε-greedy)
   c. Execute a, observe r, s'
   d. Update Q(s,a)
   e. s ← s'
```

**ε-Greedy**: Explore with probability ε

## Policy Gradients

### What are Policy Gradients?

**Policy Gradients**: Direct policy optimization

**Method**: 
```
Gradient ascent on expected reward
```

**Policy**: 
```
π_θ(a|s) = probability of action a in state s
```

**Gradient**: 
```
∇_θ J(θ) = E[∇_θ log π_θ(a|s) Q^π(s,a)]
```

**Use**: Continuous actions, high-dimensional spaces

### REINFORCE Algorithm

**REINFORCE**: Policy gradient algorithm

**Update**: 
```
θ ← θ + α ∇_θ log π_θ(a|s) G_t
```

**Where**: G_t is return

**Use**: On-policy learning

## Exploration vs Exploitation

### Exploration-Exploitation Trade-off

**Exploitation**: 
```
Use current best action
```

**Exploration**: 
```
Try new actions
```

**Trade-off**: 
```
Balance exploration and exploitation
```

**Methods**: 
- **ε-Greedy**: Random exploration
- **UCB**: Upper Confidence Bound
- **Thompson Sampling**: Bayesian approach

### ε-Greedy

**ε-Greedy**: 
```
With probability ε: random action
With probability 1-ε: best action
```

**ε**: Exploration rate

**Schedule**: Decrease ε over time

## Applications

### Application 1: Game Playing

**Use**: Game AI

**Examples**: 
- **Chess**: AlphaZero
- **Go**: AlphaGo
- **Atari**: DQN

**Method**: RL agents

**Result**: Superhuman performance

### Application 2: Robotics

**Use**: Robot control

**Method**: 
- **Policy Learning**: Learn policies
- **Sim-to-Real**: Transfer from simulation

**Result**: Autonomous robots

## Real-World Examples

### Example 1: AlphaGo

**Use**: Play Go

**Method**: 
- **Monte Carlo Tree Search**: MCTS
- **Deep Neural Networks**: Policy and value
- **Self-Play**: Learn from self

**Result**: Defeated world champion

### Example 2: Autonomous Driving

**Use**: Self-driving cars

**Method**: 
- **RL**: Learn driving policy
- **Rewards**: Safety, efficiency
- **Simulation**: Train in simulation

**Result**: Autonomous vehicles

## Common Pitfalls

### Problem: Insufficient Exploration

```c
// BAD: Too much exploitation
// Suboptimal policy

// GOOD: Balance exploration
// Use ε-greedy, UCB
```

### Problem: Delayed Rewards

```c
// BAD: Ignore delayed rewards
// Short-sighted

// GOOD: Use discount factor
// Consider long-term
```

## Quiz

1. What is reinforcement learning?
   - **A)** Supervised learning
   - **B)** Learning through interaction with environment using rewards
   - **C)** Unsupervised learning
   - **D)** Semi-supervised learning

2. What is Q-learning?
   - **A)** Supervised algorithm
   - **B)** Model-free RL algorithm learning action values Q(s,a)
   - **C)** Clustering algorithm
   - **D)** Classification algorithm

3. What is the exploration-exploitation trade-off?
   - **A)** Only exploration
   - **B)** Balance between trying new actions and using best known actions
   - **C)** Only exploitation
   - **D)** No trade-off

**Answers:**
1. **B** - Reinforcement learning agents learn optimal policies by interacting with environment and receiving reward signals
2. **B** - Q-learning updates Q-values using Bellman equation, learning optimal action values without transition model
3. **B** - Must balance exploring new actions to discover better policies vs exploiting current best actions for immediate reward

## Next Steps

- [Advanced NLP Techniques](../machine_learning/10.%20Advanced%20NLP%20Techniques.md) - NLP
- [Computer Vision Fundamentals](../machine_learning/11.%20Computer%20Vision%20Fundamentals.md) - Computer vision

