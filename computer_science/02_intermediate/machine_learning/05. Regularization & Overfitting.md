---
number: 5
title: "Regularization & Overfitting"
slug: "regularization-overfitting"
level: "intermediate"
tags: ["machine-learning", "regularization", "overfitting", "bias-variance", "generalization"]
prerequisites: ["backpropagation-deep-dive"]
estimated_minutes: 110
contributors: []
diagrams: []
examples: []
canonical_id: "cs-ml-05"
---

# Regularization & Overfitting

## Overview

Overfitting is a fundamental problem in machine learning where models memorize training data but fail to generalize. Understanding overfitting, bias-variance trade-off, and regularization techniques like L1/L2, dropout, and early stopping is essential for building models that perform well on new data.

## Table of Contents

1. [What is Overfitting?](#what-is-overfitting)
2. [Bias-Variance Trade-off](#bias-variance)
3. [Underfitting](#underfitting)
4. [L1 Regularization](#l1-regularization)
5. [L2 Regularization](#l2-regularization)
6. [Dropout](#dropout)
7. [Early Stopping](#early-stopping)
8. [Data Augmentation](#data-augmentation)

## What is Overfitting?

### Definition

**Overfitting**: Model learns training data too well

**Symptoms**:
- **Training accuracy**: High
- **Validation accuracy**: Low
- **Gap**: Large gap between train/val

**Problem**: Model memorizes noise, not patterns

### Overfitting Example

**Training Data**:
```
Perfect fit to training points
But doesn't generalize
```

**Test Data**:
```
Poor performance on new data
```

**Visual**: Model curve goes through every training point

## Bias-Variance Trade-off

### Bias

**Bias**: Error from oversimplifying

**High Bias**: Model too simple

**Symptoms**: Underfitting

**Example**: Linear model for non-linear data

### Variance

**Variance**: Error from sensitivity to training set

**High Variance**: Model too complex

**Symptoms**: Overfitting

**Example**: Model memorizes training data

### Trade-off

**Bias-Variance Trade-off**:
```
Total Error = Bias² + Variance + Irreducible Error
```

**Goal**: Balance bias and variance

**Optimal**: Low bias, low variance

## Underfitting

### What is Underfitting?

**Underfitting**: Model too simple

**Symptoms**:
- **Training accuracy**: Low
- **Validation accuracy**: Low
- **Both**: Poor performance

**Cause**: Model cannot capture patterns

### Solutions

**1. Increase Model Complexity**:
```
Add more layers/neurons
```

**2. Reduce Regularization**:
```
Reduce regularization strength
```

**3. Train Longer**:
```
More epochs
```

## L1 Regularization

### What is L1 Regularization?

**L1**: Lasso regularization

**Penalty**: Sum of absolute weights

**Formula**:
```
Loss = Original_Loss + λ × Σ|w|
```

**Effect**: Encourages sparse weights (many weights → 0)

### L1 Properties

**Sparsity**: Many weights become exactly 0

**Feature Selection**: Automatically selects features

**Use**: When want sparse model

### L1 Example

**Before L1**:
```
Weights: [0.5, 0.3, 0.2, 0.1, 0.05]
```

**After L1**:
```
Weights: [0.4, 0.2, 0.0, 0.0, 0.0]
Many weights → 0
```

## L2 Regularization

### What is L2 Regularization?

**L2**: Ridge regularization

**Penalty**: Sum of squared weights

**Formula**:
```
Loss = Original_Loss + λ × Σw²
```

**Effect**: Shrinks weights toward zero

### L2 Properties

**Weight Decay**: Weights shrink but don't become 0

**Smooth**: Smooth weight distribution

**Use**: General regularization

### L2 Example

**Before L2**:
```
Weights: [10, 8, 6, 4, 2]
```

**After L2**:
```
Weights: [5, 4, 3, 2, 1]
All weights shrunk proportionally
```

## Dropout

### What is Dropout?

**Dropout**: Randomly set neurons to 0 during training

**Rate**: Probability of dropping (e.g., 0.5)

**Process**:
```
Training: Randomly drop neurons
Testing: Use all neurons (scale weights)
```

### Dropout Benefits

**1. Prevents Overfitting**:
```
Forces network to not rely on specific neurons
```

**2. Ensemble Effect**:
```
Trains many subnetworks
Averages predictions
```

**3. Regularization**:
```
Reduces co-adaptation
```

### Dropout Implementation

**Training**:
```python
def dropout_forward(x, dropout_rate, training):
    if training:
        mask = np.random.binomial(1, 1-dropout_rate, x.shape)
        return x * mask / (1 - dropout_rate)
    else:
        return x
```

**Testing**: Use all neurons (no dropout)

## Early Stopping

### What is Early Stopping?

**Early Stopping**: Stop training when validation loss increases

**Purpose**: Prevent overfitting

**Process**:
```
1. Monitor validation loss
2. If validation loss increases: Stop
3. Use best model (lowest validation loss)
```

### Early Stopping Example

**Training**:
```
Epoch 1: Train loss 0.5, Val loss 0.6
Epoch 2: Train loss 0.3, Val loss 0.4
Epoch 3: Train loss 0.2, Val loss 0.35
Epoch 4: Train loss 0.1, Val loss 0.4  ← Increased!
Stop training
Use model from epoch 3
```

## Data Augmentation

### What is Data Augmentation?

**Data Augmentation**: Create more training data

**Methods**:
- **Rotation**: Rotate images
- **Translation**: Shift images
- **Flip**: Mirror images
- **Noise**: Add noise

**Benefit**: More diverse training data, less overfitting

### Data Augmentation Example

**Original Image**: 1 image

**Augmented**:
```
Original
Rotated 90°
Rotated 180°
Flipped horizontally
Shifted
→ 5 images from 1
```

## Real-World Examples

### Example 1: Image Classification

**Problem**: Overfitting on small dataset

**Solutions**:
- **Dropout**: 0.5 dropout rate
- **Data Augmentation**: Rotate, flip, shift
- **L2 Regularization**: λ = 0.001

### Example 2: Text Classification

**Problem**: Overfitting on text data

**Solutions**:
- **Dropout**: 0.3 dropout rate
- **L2 Regularization**: Weight decay
- **Early Stopping**: Monitor validation F1

## Common Pitfalls

### Problem: Too Much Regularization

```python
# BAD: Too much regularization
model.add(Dropout(0.9))  # Too high!
# Model underfits

# GOOD: Appropriate regularization
model.add(Dropout(0.5))  # Balanced
```

### Problem: Not Using Validation Set

```python
# BAD: Only check training loss
# May overfit without knowing

# GOOD: Monitor validation loss
# Early stop based on validation
```

## Quiz

1. What is overfitting?
   - **A)** Model too simple
   - **B)** Model memorizes training data but fails to generalize
   - **C)** Model too fast
   - **D)** Model too slow

2. What is the difference between L1 and L2 regularization?
   - **A)** No difference
   - **B)** L1 encourages sparsity (weights → 0), L2 shrinks weights proportionally
   - **C)** L1 is faster
   - **D)** L2 is simpler

3. What is dropout?
   - **A)** Removing layers
   - **B)** Randomly setting neurons to 0 during training to prevent overfitting
   - **C)** Removing weights
   - **D)** Stopping training

**Answers:**
1. **B** - Overfitting occurs when a model learns the training data too well (including noise) but fails to generalize to new data, shown by high training accuracy but low validation accuracy
2. **B** - L1 regularization (sum of absolute weights) encourages sparsity by driving many weights to exactly 0, while L2 regularization (sum of squared weights) shrinks all weights proportionally toward 0
3. **B** - Dropout randomly sets neurons to 0 during training to prevent overfitting by forcing the network to not rely on specific neurons, creating an ensemble effect

## Next Steps

- [Transformers & Attention](../machine_learning/06.%20Transformers%20%26%20Attention.md) - Modern architectures
- [Decision Trees & Random Forest](../machine_learning/07.%20Decision%20Trees.md) - Tree-based models

