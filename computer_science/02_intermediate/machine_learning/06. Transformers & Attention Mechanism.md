---
number: 6
title: "Transformers & Attention Mechanism"
slug: "transformers-attention-mechanism"
level: "intermediate"
tags: ["machine-learning", "transformers", "attention", "nlp", "deep-learning"]
prerequisites: ["regularization-overfitting"]
estimated_minutes: 135
contributors: []
diagrams: []
examples: []
canonical_id: "cs-ml-06"
---

# Transformers & Attention Mechanism

## Overview

Transformers revolutionized natural language processing and deep learning. Understanding the attention mechanism, self-attention, multi-head attention, and transformer architecture is essential for modern NLP, computer vision, and understanding state-of-the-art models like BERT, GPT, and Vision Transformers.

## Table of Contents

1. [The Attention Problem](#attention-problem)
2. [Attention Mechanism](#attention-mechanism)
3. [Self-Attention](#self-attention)
4. [Multi-Head Attention](#multi-head)
5. [Transformer Architecture](#transformer)
6. [Positional Encoding](#positional)
7. [Encoder-Decoder](#encoder-decoder)
8. [Applications](#applications)

## The Attention Problem

### The Challenge

**Sequence Models**: Need to relate distant positions

**RNNs**: Sequential processing, slow

**Problem**: Hard to capture long-range dependencies

**Solution**: Attention mechanism

### Why Attention?

**1. Parallelization**:
```
Process all positions simultaneously
Faster than RNNs
```

**2. Long-Range Dependencies**:
```
Direct connections between positions
No degradation with distance
```

**3. Interpretability**:
```
Can see what model attends to
```

## Attention Mechanism

### What is Attention?

**Attention**: Mechanism to focus on relevant parts

**Query, Key, Value**: Three components

**Process**: 
```
1. Compute attention scores (query × key)
2. Apply softmax (weights)
3. Weighted sum of values
```

### Attention Formula

**Scaled Dot-Product Attention**:
```
Attention(Q, K, V) = softmax(QK^T / √d_k) × V
```

**Where**:
- **Q**: Query matrix
- **K**: Key matrix
- **V**: Value matrix
- **d_k**: Dimension of keys

### Attention Intuition

**Query**: "What am I looking for?"

**Key**: "What do I represent?"

**Value**: "What information do I contain?"

**Attention**: Weighted combination based on relevance

## Self-Attention

### What is Self-Attention?

**Self-Attention**: Attention within same sequence

**Q, K, V**: All from same input

**Purpose**: Relate different positions in sequence

### Self-Attention Process

**Input**: Sequence of embeddings

**Process**:
```
1. Compute Q, K, V from input
2. Compute attention scores
3. Apply attention to get output
```

**Output**: Context-aware representations

### Self-Attention Example

**Input**: "The cat sat on the mat"

**Attention**: 
- "cat" attends to "The", "sat", "on", "the", "mat"
- "sat" attends to "The", "cat", "on", "the", "mat"
- Each word gets context from all words

## Multi-Head Attention

### What is Multi-Head Attention?

**Multi-Head**: Multiple attention mechanisms

**Heads**: Different learned projections

**Purpose**: Attend to different aspects

### Multi-Head Process

**Process**:
```
1. Split Q, K, V into h heads
2. Apply attention to each head
3. Concatenate heads
4. Linear projection
```

**Formula**:
```
MultiHead(Q, K, V) = Concat(head₁, ..., headₕ)W^O
where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
```

### Multi-Head Benefits

**1. Different Representations**:
```
Each head learns different patterns
```

**2. Richer Representations**:
```
Multiple perspectives
```

**3. Better Performance**:
```
Empirically better than single head
```

## Transformer Architecture

### Transformer Components

**1. Encoder**:
```
Self-attention + Feed-forward
```

**2. Decoder**:
```
Masked self-attention + Encoder-decoder attention + Feed-forward
```

**3. Positional Encoding**:
```
Add positional information
```

**4. Layer Normalization**:
```
Normalize activations
```

**5. Residual Connections**:
```
Skip connections
```

### Encoder Block

**Components**:
```
1. Multi-Head Self-Attention
2. Add & Norm (residual + layer norm)
3. Feed-Forward Network
4. Add & Norm
```

**Stack**: Multiple encoder blocks

### Decoder Block

**Components**:
```
1. Masked Multi-Head Self-Attention
2. Add & Norm
3. Multi-Head Encoder-Decoder Attention
4. Add & Norm
5. Feed-Forward Network
6. Add & Norm
```

**Masking**: Prevent attending to future positions

## Positional Encoding

### Why Positional Encoding?

**Problem**: Attention is permutation-invariant

**Solution**: Add positional information

**Methods**:
- **Sinusoidal**: Fixed positional encoding
- **Learned**: Learned positional embeddings

### Sinusoidal Encoding

**Formula**:
```
PE(pos, 2i) = sin(pos / 10000^(2i/d))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d))
```

**Properties**:
- **Fixed**: Not learned
- **Extrapolation**: Can handle longer sequences
- **Relative**: Captures relative positions

## Encoder-Decoder

### Encoder

**Purpose**: Encode input sequence

**Process**:
```
Input → Embeddings → Positional Encoding
→ Encoder Blocks → Encoded Representation
```

**Output**: Context-aware representation

### Decoder

**Purpose**: Generate output sequence

**Process**:
```
Previous Output → Embeddings → Positional Encoding
→ Decoder Blocks → Output Probabilities
```

**Attention**: 
- **Self-Attention**: Over previous outputs
- **Cross-Attention**: Over encoder outputs

## Applications

### Application 1: BERT

**Architecture**: Encoder-only

**Training**: Masked language modeling

**Use**: Language understanding

**Tasks**: Classification, Q&A, NER

### Application 2: GPT

**Architecture**: Decoder-only

**Training**: Language modeling

**Use**: Text generation

**Tasks**: Completion, generation

### Application 3: T5

**Architecture**: Encoder-decoder

**Training**: Text-to-text

**Use**: All NLP tasks as text generation

### Application 4: Vision Transformers

**Architecture**: Encoder

**Input**: Image patches

**Use**: Image classification

**Tasks**: Classification, detection

## Real-World Examples

### Example 1: Machine Translation

**Input**: "Hello world" (English)

**Output**: "Hola mundo" (Spanish)

**Process**: Encoder encodes English, decoder generates Spanish

### Example 2: Text Summarization

**Input**: Long article

**Output**: Summary

**Process**: Encoder encodes article, decoder generates summary

## Common Pitfalls

### Problem: Not Using Positional Encoding

```python
# BAD: No positional encoding
# Model can't distinguish positions

# GOOD: Add positional encoding
x = embeddings + positional_encoding
```

### Problem: Forgetting Masking in Decoder

```python
# BAD: No masking in decoder
# Can attend to future positions (cheating!)

# GOOD: Mask future positions
mask = create_look_ahead_mask(seq_len)
attention_output = attention(q, k, v, mask=mask)
```

## Quiz

1. What is the attention mechanism?
   - **A)** Memory allocation
   - **B)** Mechanism to focus on relevant parts using query, key, value
   - **C)** CPU attention
   - **D)** Network attention

2. What is self-attention?
   - **A)** Attention to self
   - **B)** Attention within same sequence where Q, K, V come from same input
   - **C)** Self-referential attention
   - **D)** Single-head attention

3. What is the main advantage of transformers over RNNs?
   - **A)** Simpler architecture
   - **B)** Parallelization and ability to capture long-range dependencies
   - **C)** Less memory
   - **D)** Faster training

**Answers:**
1. **B** - Attention mechanism uses query, key, and value matrices to compute weighted combinations, allowing the model to focus on relevant parts of the input
2. **B** - Self-attention is attention within the same sequence where query, key, and value all come from the same input, allowing positions to relate to each other
3. **B** - Transformers can process all positions in parallel (unlike sequential RNNs) and capture long-range dependencies directly without degradation over distance

## Next Steps

- [Decision Trees & Random Forest](../machine_learning/07.%20Decision%20Trees.md) - Tree-based models
- [Vector Embeddings](../machine_learning/08.%20Vector%20Embeddings.md) - Word embeddings

