---
number: 6
title: "Socket Programming - epoll, kqueue, select"
slug: "socket-programming-epoll-kqueue-select"
level: "intermediate"
tags: ["networking", "sockets", "epoll", "kqueue", "select", "io-multiplexing"]
prerequisites: ["tcp-ip-internals-deep-dive"]
estimated_minutes: 120
contributors: []
diagrams: []
examples: []
canonical_id: "cs-net-06"
---

# Socket Programming - epoll, kqueue, select

## Overview

Socket programming with I/O multiplexing is essential for building high-performance network servers. Understanding select, poll, epoll (Linux), and kqueue (BSD/macOS) is crucial for handling multiple connections efficiently and building scalable network applications.

## Table of Contents

1. [Socket Programming Basics](#basics)
2. [The C10K Problem](#c10k)
3. [select() System Call](#select)
4. [poll() System Call](#poll)
5. [epoll (Linux)](#epoll)
6. [kqueue (BSD/macOS)](#kqueue)
7. [Performance Comparison](#comparison)
8. [Event-Driven Architecture](#event-driven)

## Socket Programming Basics

### What are Sockets?

**Socket**: Endpoint for communication

**Types**:
- **TCP**: Reliable, connection-oriented
- **UDP**: Unreliable, connectionless

### Basic Socket Operations

**Server**:
```c
1. socket() - Create socket
2. bind() - Bind to address
3. listen() - Listen for connections
4. accept() - Accept connection
5. read/write - Communicate
6. close() - Close socket
```

**Client**:
```c
1. socket() - Create socket
2. connect() - Connect to server
3. read/write - Communicate
4. close() - Close socket
```

### Blocking I/O Problem

**Blocking**:
```c
accept(); // Blocks until connection arrives
read();   // Blocks until data available
```

**Problem**: One thread per connection

**Limitation**: Doesn't scale to many connections

## The C10K Problem

### What is C10K?

**C10K**: Handling 10,000 concurrent connections

**Challenge**: Traditional one-thread-per-connection doesn't scale

**Solutions**:
- **I/O Multiplexing**: Monitor multiple sockets
- **Event-driven**: React to events
- **Non-blocking**: Don't block on I/O

### Approaches

**1. Threading**:
```
One thread per connection
Problem: Thread overhead, context switching
```

**2. I/O Multiplexing**:
```
One thread monitors multiple sockets
Better: Lower overhead
```

**3. Asynchronous I/O**:
```
Non-blocking I/O with callbacks
Best: Most efficient
```

## select() System Call

### What is select?

**select**: Monitor multiple file descriptors

**Purpose**: Wait for I/O to become ready

**Limitation**: O(n) complexity, limited to FD_SETSIZE

### select() API

**Function**:
```c
int select(int nfds, 
           fd_set *readfds, 
           fd_set *writefds, 
           fd_set *exceptfds, 
           struct timeval *timeout);
```

**Sets**:
- **readfds**: Descriptors ready for reading
- **writefds**: Descriptors ready for writing
- **exceptfds**: Descriptors with exceptions

### select() Example

```c
fd_set readfds;
int max_fd = 0;

// Add sockets to set
FD_ZERO(&readfds);
FD_SET(server_fd, &readfds);
max_fd = server_fd;

while (1) {
    fd_set temp_fds = readfds;
    
    // Wait for activity
    int activity = select(max_fd + 1, &temp_fds, NULL, NULL, NULL);
    
    if (FD_ISSET(server_fd, &temp_fds)) {
        // New connection
        int client_fd = accept(server_fd, NULL, NULL);
        FD_SET(client_fd, &readfds);
        max_fd = max(max_fd, client_fd);
    }
    
    // Check client sockets
    for (int fd = 0; fd <= max_fd; fd++) {
        if (fd != server_fd && FD_ISSET(fd, &temp_fds)) {
            // Data available
            char buffer[1024];
            int n = read(fd, buffer, sizeof(buffer));
            if (n == 0) {
                // Connection closed
                close(fd);
                FD_CLR(fd, &readfds);
            } else {
                // Process data
                process_data(fd, buffer, n);
            }
        }
    }
}
```

### select() Limitations

**1. FD_SETSIZE**:
```
Limited to FD_SETSIZE (typically 1024)
Cannot handle more file descriptors
```

**2. O(n) Complexity**:
```
Must check all file descriptors
Inefficient for many descriptors
```

**3. Copy Overhead**:
```
Kernel copies fd_set to user space
Overhead on each call
```

## poll() System Call

### What is poll?

**poll**: Monitor multiple file descriptors

**Improvement**: Over select

**Advantages**: No FD_SETSIZE limit, more efficient

### poll() API

**Function**:
```c
int poll(struct pollfd *fds, nfds_t nfds, int timeout);
```

**Structure**:
```c
struct pollfd {
    int fd;         // File descriptor
    short events;   // Requested events
    short revents;  // Returned events
};
```

### poll() Example

```c
struct pollfd fds[MAX_CONNECTIONS];
int nfds = 1;

// Initialize server socket
fds[0].fd = server_fd;
fds[0].events = POLLIN;

while (1) {
    // Wait for activity
    int activity = poll(fds, nfds, -1);
    
    if (fds[0].revents & POLLIN) {
        // New connection
        int client_fd = accept(server_fd, NULL, NULL);
        fds[nfds].fd = client_fd;
        fds[nfds].events = POLLIN;
        nfds++;
    }
    
    // Check client sockets
    for (int i = 1; i < nfds; i++) {
        if (fds[i].revents & POLLIN) {
            char buffer[1024];
            int n = read(fds[i].fd, buffer, sizeof(buffer));
            if (n == 0) {
                close(fds[i].fd);
                // Remove from array
                fds[i] = fds[nfds - 1];
                nfds--;
            } else {
                process_data(fds[i].fd, buffer, n);
            }
        }
    }
}
```

### poll() Limitations

**Still O(n)**: Must check all file descriptors

**Better than select**: No FD_SETSIZE limit

## epoll (Linux)

### What is epoll?

**epoll**: Event poll (Linux-specific)

**Advantages**: O(1) for ready descriptors, scalable

**API**: epoll_create, epoll_ctl, epoll_wait

### epoll API

**Create**:
```c
int epfd = epoll_create1(0);
```

**Control**:
```c
struct epoll_event ev;
ev.events = EPOLLIN;
ev.data.fd = fd;
epoll_ctl(epfd, EPOLL_CTL_ADD, fd, &ev);
```

**Wait**:
```c
struct epoll_event events[MAX_EVENTS];
int nfds = epoll_wait(epfd, events, MAX_EVENTS, -1);
```

### epoll Example

```c
int epfd = epoll_create1(0);
struct epoll_event ev, events[MAX_EVENTS];

// Add server socket
ev.events = EPOLLIN;
ev.data.fd = server_fd;
epoll_ctl(epfd, EPOLL_CTL_ADD, server_fd, &ev);

while (1) {
    // Wait for events
    int nfds = epoll_wait(epfd, events, MAX_EVENTS, -1);
    
    for (int i = 0; i < nfds; i++) {
        if (events[i].data.fd == server_fd) {
            // New connection
            int client_fd = accept(server_fd, NULL, NULL);
            ev.events = EPOLLIN | EPOLLET; // Edge-triggered
            ev.data.fd = client_fd;
            epoll_ctl(epfd, EPOLL_CTL_ADD, client_fd, &ev);
        } else {
            // Client data
            int fd = events[i].data.fd;
            char buffer[1024];
            int n = read(fd, buffer, sizeof(buffer));
            if (n == 0) {
                epoll_ctl(epfd, EPOLL_CTL_DEL, fd, NULL);
                close(fd);
            } else {
                process_data(fd, buffer, n);
            }
        }
    }
}
```

### epoll Modes

**Level-Triggered (LT)**:
```
Event reported while condition true
Default mode
```

**Edge-Triggered (ET)**:
```
Event reported only on state change
More efficient, requires non-blocking I/O
```

## kqueue (BSD/macOS)

### What is kqueue?

**kqueue**: Kernel event queue (BSD/macOS)

**Similar**: To epoll

**Advantages**: O(1) for ready descriptors, scalable

### kqueue API

**Create**:
```c
int kq = kqueue();
```

**Register**:
```c
struct kevent kev;
EV_SET(&kev, fd, EVFILT_READ, EV_ADD, 0, 0, NULL);
kevent(kq, &kev, 1, NULL, 0, NULL);
```

**Wait**:
```c
struct kevent events[MAX_EVENTS];
int nfds = kevent(kq, NULL, 0, events, MAX_EVENTS, NULL);
```

### kqueue Example

```c
int kq = kqueue();
struct kevent kev, events[MAX_EVENTS];

// Add server socket
EV_SET(&kev, server_fd, EVFILT_READ, EV_ADD, 0, 0, NULL);
kevent(kq, &kev, 1, NULL, 0, NULL);

while (1) {
    // Wait for events
    int nfds = kevent(kq, NULL, 0, events, MAX_EVENTS, NULL);
    
    for (int i = 0; i < nfds; i++) {
        int fd = events[i].ident;
        
        if (fd == server_fd) {
            // New connection
            int client_fd = accept(server_fd, NULL, NULL);
            EV_SET(&kev, client_fd, EVFILT_READ, EV_ADD, 0, 0, NULL);
            kevent(kq, &kev, 1, NULL, 0, NULL);
        } else {
            // Client data
            char buffer[1024];
            int n = read(fd, buffer, sizeof(buffer));
            if (n == 0) {
                EV_SET(&kev, fd, EVFILT_READ, EV_DELETE, 0, 0, NULL);
                kevent(kq, &kev, 1, NULL, 0, NULL);
                close(fd);
            } else {
                process_data(fd, buffer, n);
            }
        }
    }
}
```

## Performance Comparison

### Scalability

**select/poll**: O(n) - Linear with number of descriptors

**epoll/kqueue**: O(1) - Constant for ready descriptors

### Performance at Scale

**10 connections**: All similar

**100 connections**: select/poll slower

**1000+ connections**: epoll/kqueue much faster

### Benchmark Results

```
select:  ~1000 connections/second
poll:    ~2000 connections/second
epoll:   ~10000+ connections/second
kqueue:  ~10000+ connections/second
```

## Event-Driven Architecture

### Event Loop Pattern

**Pattern**:
```
1. Register file descriptors
2. Wait for events
3. Process events
4. Repeat
```

**Benefits**:
- **Scalable**: Handles many connections
- **Efficient**: Low overhead
- **Responsive**: Fast event handling

### Non-Blocking I/O

**Requirement**: For edge-triggered mode

**Set Non-Blocking**:
```c
int flags = fcntl(fd, F_GETFL, 0);
fcntl(fd, F_SETFL, flags | O_NONBLOCK);
```

**Read Until EAGAIN**:
```c
while (1) {
    int n = read(fd, buffer, sizeof(buffer));
    if (n < 0) {
        if (errno == EAGAIN || errno == EWOULDBLOCK) {
            break; // No more data
        }
        // Error
    } else if (n == 0) {
        // EOF
        break;
    } else {
        // Process data
    }
}
```

## Real-World Examples

### Example 1: Web Server

**Nginx**: Uses epoll (Linux) or kqueue (BSD)

**Architecture**: Event-driven, non-blocking

**Performance**: Handles thousands of connections

### Example 2: Node.js

**libuv**: Uses epoll/kqueue/select

**Event Loop**: JavaScript event loop built on top

**Scalability**: Handles many concurrent connections

## Common Pitfalls

### Problem: Blocking I/O with epoll

```c
// BAD: Blocking read with epoll
read(fd, buffer, size); // Blocks!

// GOOD: Non-blocking I/O
fcntl(fd, F_SETFL, O_NONBLOCK);
read(fd, buffer, size); // Returns immediately
```

### Problem: Not Handling EAGAIN

```c
// BAD: Ignore EAGAIN
int n = read(fd, buffer, size);
if (n < 0) {
    // Error - but may be EAGAIN!
}

// GOOD: Handle EAGAIN
int n = read(fd, buffer, size);
if (n < 0 && errno != EAGAIN && errno != EWOULDBLOCK) {
    // Real error
}
```

## Quiz

1. What is the main advantage of epoll over select?
   - **A)** Simpler API
   - **B)** O(1) performance for ready descriptors vs O(n) for select
   - **C)** Works on all platforms
   - **D)** No advantages

2. What is the C10K problem?
   - **A)** Handling 10,000 concurrent connections efficiently
   - **B)** 10KB memory limit
   - **C)** 10K requests per second
   - **D)** 10K file descriptors

3. What is edge-triggered mode in epoll?
   - **A)** Event reported continuously
   - **B)** Event reported only on state change
   - **C)** Event reported slowly
   - **D)** No events reported

**Answers:**
1. **B** - epoll provides O(1) performance for ready descriptors (only returns ready ones), while select has O(n) complexity (must check all descriptors)
2. **A** - The C10K problem refers to efficiently handling 10,000 concurrent connections, which requires I/O multiplexing instead of one thread per connection
3. **B** - Edge-triggered mode reports events only when the state changes (e.g., data arrives), requiring non-blocking I/O and reading until EAGAIN

## Next Steps

- [Load Balancing & High Availability](../networking/07.%20Load%20Balancing.md) - Scalability patterns
- [Routing Protocols - BGP & OSPF](../networking/08.%20Routing%20Protocols.md) - Network routing

