---
number: 3
title: "Optimization Theory - Convex Optimization"
slug: "optimization-theory-convex-optimization"
level: "intermediate"
tags: ["mathematics", "optimization", "convex", "gradient-descent", "lagrange"]
prerequisites: ["statistics-advanced-hypothesis-testing-bayesian"]
estimated_minutes: 150
contributors: []
diagrams: []
examples: []
canonical_id: "cs-math-int-03"
---

# Optimization Theory - Convex Optimization

## Overview

Optimization theory covers finding optimal solutions to mathematical problems. Understanding convex optimization, gradient descent, Lagrange multipliers, and optimization algorithms is essential for machine learning, operations research, and engineering design.

## Table of Contents

1. [What is Optimization?](#what-is-optimization)
2. [Convex Sets and Functions](#convex)
3. [Convex Optimization](#convex-optimization)
4. [Gradient Descent](#gradient-descent)
5. [Lagrange Multipliers](#lagrange)
6. [Duality](#duality)
7. [Optimization Algorithms](#algorithms)
8. [Applications](#applications)

## What is Optimization?

### Definition

**Optimization**: Find best solution

**Problem**: 
```
minimize f(x)
subject to g(x) ≤ 0
           h(x) = 0
```

**Components**: 
- **Objective**: f(x) to minimize/maximize
- **Constraints**: g(x), h(x)
- **Variables**: x

**Use**: Machine learning, engineering, economics

### Optimization Types

**1. Unconstrained**:
```
No constraints
```

**2. Constrained**:
```
With constraints
```

**3. Linear**:
```
Linear objective and constraints
```

**4. Convex**:
```
Convex objective and constraints
```

**5. Non-Convex**:
```
Non-convex problems
```

## Convex Sets and Functions

### Convex Sets

**Convex Set**: 
```
For any x, y in set,
λx + (1-λ)y in set for λ ∈ [0,1]
```

**Meaning**: Line segment in set

**Examples**: 
- **Line**: Convex
- **Circle**: Convex
- **Star**: Not convex

### Convex Functions

**Convex Function**: 
```
f(λx + (1-λ)y) ≤ λf(x) + (1-λ)f(y)
```

**Meaning**: Function below line segment

**Properties**: 
- **Local minimum = Global minimum**
- **Second derivative ≥ 0**

**Examples**: 
- **x²**: Convex
- **e^x**: Convex
- **x³**: Not convex (for x < 0)

## Convex Optimization

### What is Convex Optimization?

**Convex Optimization**: 
```
minimize f(x)
subject to gᵢ(x) ≤ 0 (convex)
           hᵢ(x) = 0 (affine)
```

**Properties**: 
- **Global optimum**: Guaranteed
- **Efficient**: Efficient algorithms
- **Tractable**: Solvable

**Use**: Many practical problems

### Convex Optimization Example

**Example**: 
```
minimize ||Ax - b||²
subject to x ≥ 0
```

**Solution**: Efficient algorithms exist

**Use**: Least squares with constraints

## Gradient Descent

### What is Gradient Descent?

**Gradient Descent**: Iterative optimization

**Method**: 
```
xₖ₊₁ = xₖ - α∇f(xₖ)
```

**Where**: 
- **α**: Learning rate
- **∇f**: Gradient

**Convergence**: 
```
Converges to local minimum
For convex: global minimum
```

**Use**: Machine learning, optimization

### Gradient Descent Variants

**1. Batch Gradient Descent**:
```
Use all data
```

**2. Stochastic Gradient Descent**:
```
Use one sample
```

**3. Mini-Batch Gradient Descent**:
```
Use batch of samples
```

**4. Momentum**:
```
Add momentum term
```

## Lagrange Multipliers

### What are Lagrange Multipliers?

**Lagrange Multipliers**: Handle constraints

**Lagrangian**: 
```
L(x,λ) = f(x) + Σλᵢgᵢ(x)
```

**Optimality**: 
```
∇ₓL = 0
∇λL = 0
```

**Use**: Constrained optimization

### Lagrange Multiplier Example

**Example**: 
```
minimize x² + y²
subject to x + y = 1
```

**Lagrangian**: 
```
L = x² + y² + λ(x + y - 1)
```

**Solution**: 
```
x = y = 1/2, λ = -1
```

## Duality

### What is Duality?

**Duality**: Dual problem

**Primal**: 
```
minimize f(x)
subject to g(x) ≤ 0
```

**Dual**: 
```
maximize g(λ)
subject to λ ≥ 0
```

**Duality Gap**: 
```
Primal optimal - Dual optimal ≥ 0
```

**Strong Duality**: Gap = 0

## Optimization Algorithms

### Algorithm Types

**1. Gradient-Based**:
```
Gradient descent
```

**2. Newton's Method**:
```
Second-order information
```

**3. Interior Point**:
```
For constrained problems
```

**4. Simplex**:
```
For linear programming
```

### Algorithm Selection

**Choose**: 
```
Match algorithm to problem
Convex: Gradient descent
Linear: Simplex
Non-convex: Heuristics
```

## Applications

### Application 1: Machine Learning

**Use**: Training models

**Optimization**: 
```
minimize loss function
```

**Method**: Gradient descent

**Result**: Trained models

### Application 2: Operations Research

**Use**: Resource allocation

**Optimization**: 
```
minimize cost
subject to constraints
```

**Method**: Linear/convex programming

**Result**: Optimal allocation

## Real-World Examples

### Example 1: Neural Network Training

**Use**: Train neural networks

**Optimization**: 
```
minimize loss(θ)
```

**Method**: Gradient descent variants

**Result**: Trained network

### Example 2: Portfolio Optimization

**Use**: Optimize investment

**Optimization**: 
```
maximize return
subject to risk constraints
```

**Method**: Convex optimization

**Result**: Optimal portfolio

## Common Pitfalls

### Problem: Non-Convex Problems

```c
// BAD: Assume convex
// Local optima

// GOOD: Check convexity
// Use appropriate algorithms
```

## Quiz

1. What is optimization?
   - **A)** Data collection
   - **B)** Finding optimal solution to mathematical problem
   - **C)** Data visualization
   - **D)** Data storage

2. What is a convex function?
   - **A)** Linear function
   - **B)** Function where line segment between points lies above function
   - **C)** Constant function
   - **D)** Random function

3. What is gradient descent?
   - **A)** Data analysis
   - **B)** Iterative optimization method following negative gradient
   - **C)** Data collection
   - **D)** Data visualization

**Answers:**
1. **B** - Optimization finds the best solution (minimum/maximum) to a mathematical problem, often subject to constraints
2. **B** - Convex functions have the property that any line segment between two points lies above the function, ensuring global optima
3. **B** - Gradient descent iteratively updates variables by moving in the direction of negative gradient to minimize function

## Next Steps

- [Information Theory](../mathematics/04.%20Information%20Theory.md) - Information theory
- [Game Theory](../mathematics/05.%20Game%20Theory.md) - Game theory

