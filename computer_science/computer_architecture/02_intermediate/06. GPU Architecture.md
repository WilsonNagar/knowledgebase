---
number: 6
title: "GPU Architecture"
slug: "gpu-architecture"
level: "intermediate"
tags: ["computer-architecture", "gpu", "graphics", "parallel-computing", "cuda"]
prerequisites: ["simd-vectorization"]
estimated_minutes: 125
contributors: []
diagrams: []
examples: []
canonical_id: "cs-arch-06"
---

# GPU Architecture

## Overview

GPUs (Graphics Processing Units) are specialized processors designed for parallel computation. Understanding GPU architecture, SIMD execution, memory hierarchy, and programming models like CUDA and OpenCL is essential for parallel computing, machine learning, and graphics programming.

## Table of Contents

1. [What is a GPU?](#what-is-gpu)
2. [GPU vs CPU](#gpu-vs-cpu)
3. [GPU Architecture](#gpu-architecture)
4. [SIMD Execution](#simd-execution)
5. [Memory Hierarchy](#memory-hierarchy)
6. [CUDA Programming](#cuda)
7. [GPU Applications](#applications)
8. [Performance Considerations](#performance)

## What is a GPU?

### Definition

**GPU**: Graphics Processing Unit

**Purpose**: Originally for graphics rendering

**Now**: General-purpose parallel computing

**Use**: Graphics, machine learning, scientific computing

### GPU Evolution

**1. Fixed Function**:
```
Hardcoded graphics operations
```

**2. Programmable Shaders**:
```
Programmable graphics pipeline
```

**3. General Purpose**:
```
General-purpose computing (GPGPU)
CUDA, OpenCL
```

## GPU vs CPU

### CPU Characteristics

**Cores**: Few (2-64)

**Clock Speed**: High (2-5 GHz)

**Cache**: Large, complex

**Use**: Sequential, complex tasks

**Latency**: Optimized for low latency

### GPU Characteristics

**Cores**: Many (hundreds to thousands)

**Clock Speed**: Lower (1-2 GHz)

**Cache**: Smaller, simpler

**Use**: Parallel, simple tasks

**Throughput**: Optimized for high throughput

### When to Use What

**CPU**: 
- **Complex logic**: Branch-heavy code
- **Sequential**: Sequential algorithms
- **Low latency**: Real-time systems

**GPU**:
- **Parallel**: Many independent operations
- **Simple operations**: Element-wise operations
- **High throughput**: Batch processing

## GPU Architecture

### Streaming Multiprocessors (SMs)

**SM**: Processing unit in GPU

**Contains**: 
- **CUDA Cores**: Execution units
- **Shared Memory**: Fast on-chip memory
- **Registers**: Per-thread storage

**Multiple SMs**: GPU has many SMs

### Warp (SIMT)

**Warp**: Group of threads (32 threads)

**SIMT**: Single Instruction, Multiple Threads

**Execution**: All threads in warp execute same instruction

**Divergence**: If threads diverge, serialized

### Thread Hierarchy

**Thread**: Single execution unit

**Warp**: 32 threads

**Block**: Multiple warps (up to 1024 threads)

**Grid**: Multiple blocks

## SIMD Execution

### SIMD Concept

**SIMD**: Single Instruction, Multiple Data

**GPU**: Executes same instruction on many data elements

**Example**: Add two arrays
```
A[i] + B[i] for all i
All additions execute in parallel
```

### Warp Execution

**Process**:
```
1. Warp of 32 threads
2. All execute same instruction
3. Different data per thread
4. Very efficient if no divergence
```

**Divergence Problem**:
```
if (thread_id % 2 == 0) {
    // Some threads take this path
} else {
    // Others take this path
}
// Warp must execute both paths (serialized)
```

## Memory Hierarchy

### GPU Memory Types

**1. Global Memory**:
```
Large, slow (DRAM)
Accessible by all threads
```

**2. Shared Memory**:
```
Small, fast (on-chip)
Shared within block
```

**3. Registers**:
```
Very fast
Per-thread
```

**4. Constant Memory**:
```
Read-only, cached
Constants, lookup tables
```

**5. Texture Memory**:
```
Cached, optimized for 2D access
```

### Memory Access Patterns

**Coalesced Access**:
```
Threads access consecutive memory
Efficient: Single memory transaction
```

**Non-Coalesced Access**:
```
Threads access scattered memory
Inefficient: Multiple transactions
```

## CUDA Programming

### CUDA Basics

**CUDA**: Compute Unified Device Architecture

**Language**: C/C++ extensions

**Kernel**: Function executed on GPU

**Example**:
```c
__global__ void add(int* a, int* b, int* c, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        c[i] = a[i] + b[i];
    }
}

// Host code
int* d_a, *d_b, *d_c;
cudaMalloc(&d_a, n * sizeof(int));
// ... copy data to GPU ...
add<<<numBlocks, threadsPerBlock>>>(d_a, d_b, d_c, n);
// ... copy results back ...
```

### CUDA Thread Organization

**threadIdx**: Thread index within block

**blockIdx**: Block index within grid

**blockDim**: Block dimensions

**gridDim**: Grid dimensions

**Index Calculation**:
```
Global index = blockIdx.x * blockDim.x + threadIdx.x
```

## GPU Applications

### Application 1: Graphics Rendering

**Use**: Render 3D graphics

**Process**: 
- **Vertex Shader**: Transform vertices
- **Fragment Shader**: Compute pixel colors
- **Rasterization**: Convert to pixels

### Application 2: Machine Learning

**Use**: Training neural networks

**Operations**: 
- **Matrix multiplication**: Very parallel
- **Convolutions**: Parallel filters
- **Activation functions**: Element-wise

**Frameworks**: TensorFlow, PyTorch use GPUs

### Application 3: Scientific Computing

**Use**: Simulations, modeling

**Operations**: 
- **Linear algebra**: Matrix operations
- **FFT**: Fast Fourier Transform
- **Monte Carlo**: Parallel simulations

## Performance Considerations

### Optimization Strategies

**1. Coalesced Memory Access**:
```
Access consecutive memory
```

**2. Shared Memory**:
```
Use shared memory for frequently accessed data
```

**3. Avoid Divergence**:
```
Minimize branching within warps
```

**4. Occupancy**:
```
Keep SMs busy
Balance threads per SM
```

### Common Pitfalls

**1. Memory Bandwidth**:
```
Memory bandwidth often bottleneck
Not compute
```

**2. Divergence**:
```
Branching reduces performance
```

**3. Low Occupancy**:
```
Too few threads per SM
Underutilization
```

## Real-World Examples

### Example 1: Matrix Multiplication

**Operation**: C = A Ã— B

**GPU**: Very efficient

**Parallelization**: Each thread computes one element

**Speedup**: 100-1000x over CPU

### Example 2: Image Processing

**Operation**: Apply filter to image

**GPU**: Process many pixels in parallel

**Speedup**: Real-time processing

## Common Pitfalls

### Problem: Not Considering Memory Bandwidth

```c
// BAD: Assume compute-bound
// May be memory-bound

// GOOD: Profile memory access
// Optimize memory patterns
```

### Problem: Warp Divergence

```c
// BAD: Branching in kernel
if (threadIdx.x % 2 == 0) {
    // Causes divergence
}

// GOOD: Minimize branching
// Or restructure algorithm
```

## Quiz

1. What is a GPU optimized for?
   - **A)** Low latency
   - **B)** High throughput parallel computation
   - **C)** Complex branching
   - **D)** Sequential processing

2. What is a warp?
   - **A)** Thread
   - **B)** Group of 32 threads executing same instruction (SIMT)
   - **C)** Block
   - **D)** Grid

3. What is coalesced memory access?
   - **A)** Scattered access
   - **B)** Threads accessing consecutive memory efficiently
   - **C)** Random access
   - **D)** Slow access

**Answers:**
1. **B** - GPUs are optimized for high throughput parallel computation, with many simple cores executing the same instruction on different data elements simultaneously
2. **B** - A warp is a group of 32 threads that execute the same instruction together in SIMT (Single Instruction, Multiple Threads) fashion
3. **B** - Coalesced memory access occurs when threads in a warp access consecutive memory locations, allowing the GPU to combine these accesses into a single efficient memory transaction

## Next Steps

- [RISC vs CISC](./07.%20RISC%20vs%20CISC%20Architectures.md) - CPU architectures
- [Assembly Language Fundamentals](./08.%20Assembly%20Language%20Fundamentals%20-%20x86%2C%20ARM.md) - Low-level programming

