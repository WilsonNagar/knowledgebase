---
number: 4
title: "SIMD & Vectorization"
slug: "simd-vectorization"
level: "intermediate"
tags: ["computer-architecture", "simd", "vectorization", "parallel", "performance"]
prerequisites: ["cpu-pipelines-superscalar-execution"]
estimated_minutes: 100
contributors: []
diagrams: []
examples: []
canonical_id: "cs-arch-04"
---

# SIMD & Vectorization

## Overview

SIMD (Single Instruction, Multiple Data) enables parallel processing of data by executing the same operation on multiple data elements simultaneously. Understanding SIMD instructions, vectorization, and how to leverage them is essential for high-performance computing, multimedia processing, and scientific computing.

## Table of Contents

1. [What is SIMD?](#what-is-simd)
2. [SIMD Concepts](#concepts)
3. [x86 SIMD - SSE/AVX](#x86-simd)
4. [ARM SIMD - NEON](#arm-simd)
5. [Vectorization](#vectorization)
6. [Auto-Vectorization](#auto-vectorization)
7. [Manual Vectorization](#manual-vectorization)
8. [Performance Benefits](#performance)

## What is SIMD?

### Definition

**SIMD**: Single Instruction, Multiple Data

**Concept**: One instruction operates on multiple data elements

**Example**:
```
Scalar:    a[0] + b[0]
           a[1] + b[1]
           a[2] + b[2]
           a[3] + b[3]
           (4 instructions)

SIMD:      [a[0], a[1], a[2], a[3]] + [b[0], b[1], b[2], b[3]]
           (1 instruction, 4 operations)
```

### SIMD Benefits

**1. Parallelism**:
- **Multiple operations**: In single instruction
- **Throughput**: 4-16x improvement (depending on width)

**2. Efficiency**:
- **Less overhead**: One instruction vs many
- **Better utilization**: Uses wide execution units

**3. Power**:
- **Lower power**: Per operation
- **Better performance/power**: More efficient

## SIMD Concepts

### Vector Registers

**SIMD Registers**: Wide registers holding multiple values

**x86-64**:
- **128-bit**: XMM registers (SSE) - 4 floats or 2 doubles
- **256-bit**: YMM registers (AVX) - 8 floats or 4 doubles
- **512-bit**: ZMM registers (AVX-512) - 16 floats or 8 doubles

**ARM**:
- **128-bit**: NEON registers - 4 floats or 2 doubles

### Data Types

**Packed Data**:
```
128-bit register:
  [float0][float1][float2][float3]
  32-bit   32-bit   32-bit   32-bit
```

**Operations**: Operate on all elements simultaneously

## x86 SIMD - SSE/AVX

### SSE (Streaming SIMD Extensions)

**SSE**: x86 SIMD instruction set

**Versions**:
- **SSE**: 128-bit, 4 floats
- **SSE2**: 64-bit integers, doubles
- **SSE3**: Additional instructions
- **SSE4**: More instructions

**Registers**: XMM0-XMM15 (128-bit)

### AVX (Advanced Vector Extensions)

**AVX**: Extended SIMD (256-bit)

**AVX2**: More instructions, integer support

**AVX-512**: 512-bit registers

**Registers**: YMM0-YMM15 (256-bit), ZMM0-ZMM31 (512-bit)

### SSE Example

**C Code**:
```c
float a[4] = {1.0, 2.0, 3.0, 4.0};
float b[4] = {5.0, 6.0, 7.0, 8.0};
float c[4];

for (int i = 0; i < 4; i++) {
    c[i] = a[i] + b[i];
}
```

**SSE Assembly**:
```asm
movaps xmm0, [a]    ; Load a[0-3] into XMM0
movaps xmm1, [b]    ; Load b[0-3] into XMM1
addps xmm0, xmm1    ; Add (4 floats in parallel)
movaps [c], xmm0    ; Store result
```

**Result**: 4 additions in 1 instruction

### AVX Example

**256-bit AVX**:
```asm
vmovaps ymm0, [a]   ; Load 8 floats
vmovaps ymm1, [b]   ; Load 8 floats
vaddps ymm0, ymm0, ymm1  ; Add 8 floats
vmovaps [c], ymm0   ; Store 8 results
```

**Result**: 8 additions in 1 instruction

## ARM SIMD - NEON

### NEON Overview

**NEON**: ARM SIMD instruction set

**Registers**: 128-bit (Q registers)

**Capabilities**:
- **16× 8-bit integers**
- **8× 16-bit integers**
- **4× 32-bit integers or floats**
- **2× 64-bit integers or doubles**

### NEON Example

**C Code**:
```c
float a[4] = {1.0, 2.0, 3.0, 4.0};
float b[4] = {5.0, 6.0, 7.0, 8.0};
float c[4];
```

**NEON Assembly**:
```asm
vld1.32 {q0}, [a]   ; Load 4 floats
vld1.32 {q1}, [b]   ; Load 4 floats
vadd.f32 q0, q0, q1 ; Add 4 floats
vst1.32 {q0}, [c]   ; Store result
```

## Vectorization

### What is Vectorization?

**Vectorization**: Converting scalar code to SIMD

**Process**:
```
Scalar loop:
  for (i = 0; i < n; i++) {
    c[i] = a[i] + b[i];
  }

Vectorized:
  Process 4-8 elements at a time
  Use SIMD instructions
```

### Vectorization Requirements

**1. Data Alignment**:
- **Aligned**: Data aligned to register size
- **Example**: 16-byte aligned for 128-bit registers
- **Benefit**: Faster loads/stores

**2. No Dependencies**:
- **Independent**: Operations independent
- **No loop-carried**: No dependencies between iterations

**3. Same Operation**:
- **Uniform**: Same operation on all elements
- **No conditionals**: (usually)

## Auto-Vectorization

### Compiler Vectorization

**Auto-Vectorization**: Compiler automatically vectorizes

**GCC/Clang Flags**:
```bash
-O3 -march=native -ftree-vectorize
```

**Example**:
```c
void add_arrays(float* a, float* b, float* c, int n) {
    for (int i = 0; i < n; i++) {
        c[i] = a[i] + b[i];
    }
}
```

**Compiler Output**: May use SIMD instructions

### Helping Compiler

**1. Alignment Hints**:
```c
float* a __attribute__((aligned(16)));
```

**2. Loop Hints**:
```c
#pragma GCC ivdep  // No dependencies
for (int i = 0; i < n; i++) {
    c[i] = a[i] + b[i];
}
```

**3. Restrict Pointers**:
```c
void add(float* restrict a, float* restrict b, float* restrict c) {
    // Compiler knows no aliasing
}
```

## Manual Vectorization

### Intrinsics

**Intrinsics**: C functions mapping to SIMD instructions

**SSE Intrinsics**:
```c
#include <xmmintrin.h>

void add_sse(float* a, float* b, float* c) {
    __m128 va = _mm_load_ps(a);  // Load 4 floats
    __m128 vb = _mm_load_ps(b);  // Load 4 floats
    __m128 vc = _mm_add_ps(va, vb);  // Add
    _mm_store_ps(c, vc);  // Store
}
```

**AVX Intrinsics**:
```c
#include <immintrin.h>

void add_avx(float* a, float* b, float* c) {
    __m256 va = _mm256_load_ps(a);  // Load 8 floats
    __m256 vb = _mm256_load_ps(b);  // Load 8 floats
    __m256 vc = _mm256_add_ps(va, vb);  // Add
    _mm256_store_ps(c, vc);  // Store
}
```

### NEON Intrinsics

**ARM NEON**:
```c
#include <arm_neon.h>

void add_neon(float* a, float* b, float* c) {
    float32x4_t va = vld1q_f32(a);  // Load 4 floats
    float32x4_t vb = vld1q_f32(b);  // Load 4 floats
    float32x4_t vc = vaddq_f32(va, vb);  // Add
    vst1q_f32(c, vc);  // Store
}
```

## Performance Benefits

### Speedup Examples

**Vector Addition** (4 elements):
```
Scalar:    4 instructions
SIMD:      1 instruction
Speedup:   4x (theoretical)
Actual:    3-3.5x (overhead)
```

**Matrix Multiplication**:
```
Scalar:    O(n³) operations
SIMD:      O(n³/4) operations (4-wide)
Speedup:   ~3-4x
```

### Real-World Performance

**Image Processing**:
```
Pixel operations: 4-8x speedup
Filters: 3-6x speedup
```

**Scientific Computing**:
```
Linear algebra: 4-8x speedup
FFT: 2-4x speedup
```

## Real-World Examples

### Example 1: Array Sum

**Scalar**:
```c
float sum = 0;
for (int i = 0; i < n; i++) {
    sum += array[i];
}
```

**SIMD**:
```c
__m128 sum_vec = _mm_setzero_ps();
for (int i = 0; i < n; i += 4) {
    __m128 v = _mm_load_ps(&array[i]);
    sum_vec = _mm_add_ps(sum_vec, v);
}
// Horizontal sum
float sum = sum_vec[0] + sum_vec[1] + sum_vec[2] + sum_vec[3];
```

**Speedup**: ~3-4x

### Example 2: Image Processing

**Brightness Adjustment**:
```c
// Scalar
for (int i = 0; i < pixels; i++) {
    image[i] = image[i] + brightness;
}

// SIMD (process 4 pixels)
for (int i = 0; i < pixels; i += 4) {
    __m128 img = _mm_load_ps(&image[i]);
    __m128 bright = _mm_set1_ps(brightness);
    __m128 result = _mm_add_ps(img, bright);
    _mm_store_ps(&image[i], result);
}
```

## Common Pitfalls

### Problem: Misalignment

```c
// BAD: Unaligned data
float* data = malloc(100 * sizeof(float));
// May not be 16-byte aligned
_mm_load_ps(data); // May crash or be slow

// GOOD: Aligned allocation
float* data = _mm_malloc(100 * sizeof(float), 16);
_mm_load_ps(data); // Fast, aligned
```

### Problem: Data Dependencies

```c
// BAD: Loop-carried dependency
for (int i = 1; i < n; i++) {
    a[i] = a[i-1] + b[i]; // Depends on previous iteration
}
// Cannot vectorize

// GOOD: No dependencies
for (int i = 0; i < n; i++) {
    c[i] = a[i] + b[i]; // Independent
}
// Can vectorize
```

## Quiz

1. What does SIMD stand for?
   - **A)** Single Instruction, Multiple Data
   - **B)** Simple Instruction, Multiple Data
   - **C)** Single Instruction, Multiple Devices
   - **D)** Sequential Instruction, Multiple Data

2. What is the main benefit of SIMD?
   - **A)** Simpler code
   - **B)** Parallel processing of multiple data elements with one instruction
   - **C)** Less memory
   - **D)** Better security

3. What is required for effective vectorization?
   - **A)** Complex code
   - **B)** Data alignment, independent operations, uniform operations
   - **C)** Small arrays
   - **D)** Single core

**Answers:**
1. **A** - SIMD stands for Single Instruction, Multiple Data, meaning one instruction operates on multiple data elements simultaneously
2. **B** - SIMD enables parallel processing where a single instruction performs the same operation on multiple data elements, providing significant speedup (typically 4-8x)
3. **B** - Effective vectorization requires data alignment (for efficient loads/stores), independent operations (no loop-carried dependencies), and uniform operations (same operation on all elements)

## Next Steps

- [Virtualization - VT-x & AMD-V](./05.%20Virtualization%20-%20VT-x%20%26%20AMD-V.md) - Hardware virtualization
- [GPU Architecture](./06.%20GPU%20Architecture.md) - Graphics processing

