---
number: 2
title: "Caches & Cache Coherence"
slug: "caches-cache-coherence"
level: "fundamentals"
tags: ["computer-architecture", "caches", "memory", "coherence", "performance"]
prerequisites: ["cpu-pipelines-superscalar-execution"]
estimated_minutes: 120
contributors: []
diagrams: []
examples: []
canonical_id: "cs-arch-02"
---

# Caches & Cache Coherence

## Overview

Caches are small, fast memory stores that hold frequently accessed data to bridge the speed gap between CPU and main memory. Understanding cache hierarchies, cache organization, replacement policies, and cache coherence protocols is essential for understanding modern computer performance and optimizing code.

## Table of Contents

1. [The Memory Hierarchy](#memory-hierarchy)
2. [Cache Basics](#cache-basics)
3. [Cache Organization](#cache-organization)
4. [Cache Replacement Policies](#replacement)
5. [Write Policies](#write-policies)
6. [Cache Coherence](#coherence)
7. [MESI Protocol](#mesi)
8. [Cache Performance](#performance)

## The Memory Hierarchy

### Speed vs Capacity Trade-off

**Memory Hierarchy** (fastest to slowest):
```
CPU Registers:     ~1 cycle,   ~1KB
L1 Cache:          ~3 cycles,  ~32KB
L2 Cache:          ~10 cycles, ~256KB
L3 Cache:          ~40 cycles, ~8MB
Main Memory (RAM): ~100 cycles, ~16GB
Disk:              ~10M cycles, ~1TB
```

**Principle**: Smaller = Faster, Larger = Slower

### Why Caches?

**Problem**: CPU much faster than memory

**Example**:
- **CPU**: 4 GHz (0.25 ns per cycle)
- **Memory**: ~100 ns access time
- **Gap**: 400x slower!

**Solution**: Cache frequently used data

**Benefit**: Most accesses hit cache (fast)

## Cache Basics

### What is a Cache?

**Cache**: Small, fast memory storing frequently accessed data

**Purpose**: Reduce average memory access time

**Principle**: Temporal and spatial locality

### Locality Principles

**1. Temporal Locality**:
- Recently accessed data likely accessed again
- **Example**: Loop variables, function parameters

**2. Spatial Locality**:
- Data near accessed data likely accessed
- **Example**: Array elements, instruction sequences

### Cache Hit vs Miss

**Cache Hit**:
- Data found in cache
- **Fast**: ~1-10 cycles
- **Ideal**: Most accesses should hit

**Cache Miss**:
- Data not in cache
- **Slow**: Must fetch from memory
- **Types**: Cold, capacity, conflict, coherence

**Hit Rate**: Percentage of accesses that hit

**Target**: 90-99% hit rate

## Cache Organization

### Cache Structure

**Basic Components**:
```
Cache Line:    64 bytes (typical)
Cache Set:     Multiple lines
Cache:         Multiple sets
```

**Cache Line**:
- **Unit**: Data transferred to/from memory
- **Size**: Typically 32-128 bytes (64 bytes common)
- **Tag**: Address bits identifying line
- **Data**: Actual data
- **Valid**: Line contains valid data
- **Dirty**: Line modified (write-back)

### Cache Mapping

**1. Direct-Mapped Cache**:
- **Mapping**: One memory location → One cache line
- **Simple**: Easy to implement
- **Conflict**: Many conflicts possible

**Example** (4-line cache):
```
Memory Address → Cache Line
0x0000 → Line 0
0x0040 → Line 1
0x0080 → Line 2
0x00C0 → Line 3
0x0100 → Line 0 (conflict with 0x0000!)
```

**2. Fully Associative Cache**:
- **Mapping**: Any memory location → Any cache line
- **Flexible**: No conflicts
- **Complex**: Expensive to implement

**Example**:
```
Any address → Any available line
No conflicts
But expensive (must check all lines)
```

**3. Set-Associative Cache**:
- **Mapping**: Memory location → Set (multiple lines)
- **Balance**: Good compromise
- **Common**: Most modern caches

**Example** (2-way set-associative, 4 sets):
```
Set 0: [Line 0, Line 1]
Set 1: [Line 2, Line 3]
Set 2: [Line 4, Line 5]
Set 3: [Line 6, Line 7]

Address maps to set, can use any line in set
```

### Cache Addressing

**Address Breakdown** (64-byte lines):
```
63    12 11    6 5     0
┌──────┬──────┬──────┐
│ Tag  │Index │Offset│
└──────┴──────┴──────┘
```

**Components**:
- **Tag**: Identifies which memory block
- **Index**: Which cache set/line
- **Offset**: Byte within line

**Example**:
```
Address: 0x12345678
Tag:     0x12345
Index:   0x67 (which set)
Offset:  0x78 (byte within line)
```

## Cache Replacement Policies

### When Cache is Full

**Problem**: Cache miss, cache full

**Solution**: Replace existing line

**Policies**:

**1. Random**:
- **Simple**: Random line replaced
- **Poor**: No consideration of usage

**2. FIFO (First-In-First-Out)**:
- **Simple**: Oldest line replaced
- **Fair**: But ignores access patterns

**3. LRU (Least Recently Used)**:
- **Good**: Replace least recently used
- **Complex**: Must track access order
- **Common**: Most caches use approximation

**4. LFU (Least Frequently Used)**:
- **Good**: Replace least frequently used
- **Complex**: Must track access frequency

### LRU Implementation

**Ideal LRU**:
- Track exact access order
- Expensive (hardware)

**Approximate LRU**:
- **Bits**: Use bits to approximate
- **Pseudo-LRU**: Tree-based approximation
- **Common**: Good enough, cheaper

## Write Policies

### Write-Through

**Policy**: Write to cache and memory simultaneously

**Characteristics**:
- **Simple**: Always consistent
- **Slow**: Every write goes to memory
- **Use**: Small caches, simple systems

**Example**:
```
Write data → Cache + Memory (both updated)
```

### Write-Back

**Policy**: Write only to cache, write to memory later

**Characteristics**:
- **Fast**: Writes only to cache
- **Complex**: Must track dirty lines
- **Common**: Most modern caches

**Example**:
```
Write data → Cache only (marked dirty)
Later: Dirty line written to memory (on eviction)
```

### Write Allocation

**Policy**: Allocate cache line on write miss

**Write-Allocate**:
- **Allocate**: Load line into cache
- **Write**: Write to cache line
- **Common**: With write-back

**No-Write-Allocate**:
- **No allocate**: Write directly to memory
- **Common**: With write-through

## Cache Coherence

### The Problem

**Multi-Core System**:
```
CPU 0: Cache has X = 10
CPU 1: Cache has X = 10
CPU 0: Writes X = 20
CPU 1: Still sees X = 10 (stale!)
```

**Problem**: Caches have different values

**Solution**: Cache coherence protocol

### Coherence Requirements

**1. Write Propagation**:
- Writes eventually visible to all
- **Not immediate**: But eventually

**2. Write Serialization**:
- All processors see writes in same order
- **Consistency**: Consistent view

**3. Coherence**: 
- All caches see same value
- **After propagation**: Eventually consistent

## MESI Protocol

### What is MESI?

**MESI**: Cache coherence protocol

**States**:
- **M (Modified)**: Exclusive, dirty
- **E (Exclusive)**: Exclusive, clean
- **S (Shared)**: Shared, clean
- **I (Invalid)**: Not in cache

### State Transitions

**M (Modified)**:
- **Exclusive**: Only this cache has it
- **Dirty**: Modified, not written to memory
- **Can write**: Without notifying others
- **On read by other**: Write back, transition to S

**E (Exclusive)**:
- **Exclusive**: Only this cache has it
- **Clean**: Matches memory
- **Can write**: Transition to M
- **On read by other**: Transition to S

**S (Shared)**:
- **Shared**: Multiple caches may have it
- **Clean**: Matches memory
- **Can read**: Freely
- **Cannot write**: Must invalidate others first

**I (Invalid)**:
- **Not in cache**: Or invalidated
- **Must fetch**: On next access

### MESI Example

**Initial State**:
```
CPU 0: X in state E (exclusive)
CPU 1: X not in cache (I)
```

**CPU 1 Reads X**:
```
CPU 0: X → S (shared)
CPU 1: X → S (shared)
Memory: Provides X
```

**CPU 0 Writes X**:
```
CPU 0: X → M (modified)
CPU 1: X → I (invalidated)
CPU 0: Writes X
```

**CPU 1 Reads X**:
```
CPU 0: X → S (write back to memory)
CPU 1: X → S (read from memory)
Memory: Updated with X
```

### MOESI Protocol

**Extension**: Adds O (Owned) state

**O (Owned)**:
- **Shared**: But this cache owns it
- **Dirty**: Modified, others may have stale copy
- **Must write back**: When evicted

**Benefit**: Reduces write-backs

## Cache Performance

### Metrics

**1. Hit Rate**:
- **Definition**: Percentage of cache hits
- **Target**: 90-99%
- **Impact**: High hit rate = good performance

**2. Miss Rate**:
- **Definition**: 1 - Hit Rate
- **Target**: 1-10%
- **Impact**: Low miss rate = good performance

**3. Average Access Time**:
```
T_avg = Hit_Rate × T_hit + Miss_Rate × T_miss
```

**Example**:
```
T_hit = 1 cycle
T_miss = 100 cycles
Hit_Rate = 95%

T_avg = 0.95 × 1 + 0.05 × 100
      = 0.95 + 5
      = 5.95 cycles
```

### Miss Types

**1. Cold Miss (Compulsory)**:
- **Cause**: First access to line
- **Unavoidable**: Must load from memory
- **Solution**: Prefetching

**2. Capacity Miss**:
- **Cause**: Cache too small
- **Solution**: Larger cache
- **Example**: Working set > cache size

**3. Conflict Miss**:
- **Cause**: Mapping conflicts
- **Solution**: Higher associativity
- **Example**: Direct-mapped cache conflicts

**4. Coherence Miss**:
- **Cause**: Invalidation from other cores
- **Solution**: Better coherence protocol
- **Example**: False sharing

### Optimization Techniques

**1. Prefetching**:
- **Idea**: Load data before needed
- **Types**: Hardware, software
- **Benefit**: Hide memory latency

**2. Block Size**:
- **Larger**: Better spatial locality
- **Smaller**: Less waste, fewer conflicts
- **Trade-off**: Balance needed

**3. Associativity**:
- **Higher**: Fewer conflicts
- **Lower**: Faster access, cheaper
- **Common**: 4-8 way associative

## Real-World Considerations

### Modern Cache Hierarchies

**Intel Core i7**:
```
L1 Instruction: 32KB, 8-way, ~4 cycles
L1 Data:        32KB, 8-way, ~4 cycles
L2:             256KB, 8-way, ~12 cycles
L3:             8-16MB, 16-way, ~40 cycles
```

**ARM Cortex-A78**:
```
L1 Instruction: 64KB, 4-way, ~3 cycles
L1 Data:        64KB, 4-way, ~3 cycles
L2:             512KB-1MB, 8-way, ~10 cycles
L3:             4-8MB, 16-way, ~35 cycles
```

### False Sharing

**Problem**: Unrelated data in same cache line

**Example**:
```c
struct {
    int counter1; // CPU 0 modifies
    int counter2; // CPU 1 modifies
} counters; // Same cache line!

// CPU 0 writes counter1 → Invalidates CPU 1's cache
// CPU 1 writes counter2 → Invalidates CPU 0's cache
// Constant invalidation → Poor performance
```

**Solution**: Padding or separate cache lines

```c
struct {
    int counter1;
    char padding[60]; // Pad to cache line size
    int counter2;
} counters; // Different cache lines
```

## Common Pitfalls

### Problem: Ignoring Cache Effects

```c
// BAD: Random access pattern
for (int i = 0; i < n; i++) {
    sum += array[random_index[i]]; // Poor locality
}

// GOOD: Sequential access
for (int i = 0; i < n; i++) {
    sum += array[i]; // Good locality
}
```

### Problem: False Sharing

```c
// BAD: False sharing
int counters[NUM_THREADS]; // Adjacent in memory

// GOOD: Avoid false sharing
struct {
    int counter;
    char padding[64]; // Pad to cache line
} counters[NUM_THREADS];
```

## Quiz

1. What is the main purpose of caches?
   - **A)** Increase memory capacity
   - **B)** Bridge speed gap between CPU and memory
   - **C)** Reduce power consumption
   - **D)** Simplify memory management

2. What does MESI stand for?
   - **A)** Memory, Execute, Store, Invalidate
   - **B)** Modified, Exclusive, Shared, Invalid
   - **C)** Most, Exclusive, Some, Invalid
   - **D)** Memory, Exclusive, Shared, Instruction

3. What is false sharing?
   - **A)** Sharing cache between cores
   - **B)** Unrelated data in same cache line causing unnecessary invalidations
   - **C)** Invalid cache data
   - **D)** Cache miss

**Answers:**
1. **B** - Caches bridge the speed gap between fast CPU and slower main memory by storing frequently accessed data
2. **B** - MESI stands for Modified (exclusive dirty), Exclusive (exclusive clean), Shared (multiple copies), Invalid (not in cache)
3. **B** - False sharing occurs when unrelated data shares the same cache line, causing unnecessary cache invalidations when one core modifies its data

## Next Steps

- [Branch Prediction & Speculation](../02_intermediate/03.%20Branch%20Prediction%20%26%20Speculation.md) - Branch prediction
- [SIMD & Vectorization](../02_intermediate/04.%20SIMD%20%26%20Vectorization.md) - Parallel data processing

