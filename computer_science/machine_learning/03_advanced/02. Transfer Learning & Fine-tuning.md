---
number: 2
title: "Transfer Learning & Fine-tuning"
slug: "transfer-learning-fine-tuning"
level: "advanced"
tags: ["machine-learning", "transfer-learning", "fine-tuning", "pre-training", "domain-adaptation"]
prerequisites: ["deep-learning-advanced-gans-vaes-diffusion-models"]
estimated_minutes: 145
contributors: []
diagrams: []
examples: []
canonical_id: "cs-ml-adv-02"
---

# Transfer Learning & Fine-tuning

## Overview

Transfer learning leverages knowledge from pre-trained models to solve new tasks with limited data. Understanding transfer learning strategies, fine-tuning techniques, domain adaptation, and best practices is essential for efficient model development.

## Table of Contents

1. [What is Transfer Learning?](#what-is-transfer-learning)
2. [Transfer Learning Strategies](#strategies)
3. [Fine-tuning Techniques](#fine-tuning)
4. [Domain Adaptation](#domain-adaptation)
5. [Few-Shot Learning](#few-shot)
6. [Pre-training Methods](#pre-training)
7. [Best Practices](#best-practices)
8. [Applications](#applications)

## What is Transfer Learning?

### Definition

**Transfer Learning**: Use knowledge from one task for another

**Process**: 
```
1. Pre-train on large dataset
2. Transfer knowledge
3. Fine-tune on target task
```

**Benefit**: 
- **Less Data**: Need less data
- **Faster**: Faster training
- **Better**: Often better performance

**Use**: Limited data scenarios

### Transfer Learning Types

**1. Inductive**: 
```
Source and target tasks differ
```

**2. Transductive**: 
```
Same task, different domains
```

**3. Unsupervised**: 
```
Transfer from unlabeled data
```

**4. Self-Supervised**: 
```
Pre-train without labels
```

## Transfer Learning Strategies

### Strategy 1: Feature Extraction

**Feature Extraction**: 
```
1. Use pre-trained model as feature extractor
2. Freeze all layers
3. Train classifier on top
```

**Use**: Very limited data

**Benefit**: Fast, simple

### Strategy 2: Fine-tuning

**Fine-tuning**: 
```
1. Use pre-trained weights
2. Unfreeze some/all layers
3. Train on target task
```

**Use**: More data available

**Benefit**: Better adaptation

### Strategy 3: Progressive Unfreezing

**Progressive Unfreezing**: 
```
1. Freeze all layers
2. Unfreeze last layer, train
3. Unfreeze more layers gradually
```

**Use**: Careful fine-tuning

**Benefit**: Stable training

## Fine-tuning Techniques

### Fine-tuning Approaches

**1. Full Fine-tuning**: 
```
Unfreeze all layers
Train all parameters
```

**2. Partial Fine-tuning**: 
```
Freeze early layers
Fine-tune later layers
```

**3. Layer-wise Fine-tuning**: 
```
Fine-tune layers progressively
```

**4. Learning Rate Scheduling**: 
```
Use different learning rates
Lower LR for pre-trained layers
```

### Learning Rate Strategies

**Differential Learning Rates**: 
```
Early layers: Low LR
Later layers: Higher LR
```

**Learning Rate Finder**: 
```
Find optimal learning rate
```

**Cosine Annealing**: 
```
Schedule learning rate
```

## Domain Adaptation

### What is Domain Adaptation?

**Domain Adaptation**: Adapt to new domain

**Problem**: 
```
Source domain â‰  Target domain
```

**Methods**: 
- **Domain Adversarial**: Adversarial training
- **Domain Randomization**: Randomize domain
- **Fine-tuning**: Fine-tune on target

**Use**: Different distributions

### Domain Adaptation Methods

**1. Domain Adversarial Training**: 
```
Train domain classifier
Fool domain classifier
```

**2. Domain Randomization**: 
```
Train on diverse domains
Generalize better
```

**3. Pseudo-Labeling**: 
```
Use model predictions as labels
```

## Few-Shot Learning

### What is Few-Shot Learning?

**Few-Shot Learning**: Learn from few examples

**Settings**: 
- **Zero-Shot**: No examples
- **One-Shot**: One example
- **Few-Shot**: Few examples

**Methods**: 
- **Meta-Learning**: Learn to learn
- **Metric Learning**: Learn metrics
- **Prompting**: Prompt-based

**Use**: Very limited data

### Few-Shot Methods

**1. Prototypical Networks**: 
```
Learn prototypes
Classify by distance
```

**2. Matching Networks**: 
```
Match support and query
```

**3. Model-Agnostic Meta-Learning**: 
```
MAML: Learn good initialization
```

## Pre-training Methods

### Pre-training Approaches

**1. Supervised Pre-training**: 
```
Pre-train on labeled data
ImageNet, etc.
```

**2. Self-Supervised Pre-training**: 
```
Pre-train without labels
Contrastive learning, etc.
```

**3. Multi-Task Pre-training**: 
```
Pre-train on multiple tasks
```

**4. Large-Scale Pre-training**: 
```
Pre-train on huge datasets
```

### Self-Supervised Learning

**Self-Supervised**: 
```
Create labels from data
Learn representations
```

**Methods**: 
- **Contrastive**: Contrastive learning
- **Predictive**: Predict masked parts
- **Generative**: Generate data

**Use**: Unlabeled data

## Best Practices

### Practice 1: Choose Right Strategy

**Choose**: 
```
Match strategy to data
Feature extraction: Very limited
Fine-tuning: More data
```

**Benefit**: Optimal performance

### Practice 2: Learning Rate

**Learning Rate**: 
```
Use lower LR for pre-trained layers
Higher LR for new layers
```

**Benefit**: Stable training

### Practice 3: Data Augmentation

**Augmentation**: 
```
Use data augmentation
Especially for limited data
```

**Benefit**: Better generalization

## Applications

### Application 1: Computer Vision

**Use**: Image classification

**Method**: 
- **Pre-train**: ImageNet
- **Fine-tune**: Target dataset
- **Transfer**: Transfer knowledge

**Result**: Better performance

### Application 2: NLP

**Use**: Language tasks

**Method**: 
- **Pre-train**: Large text corpus
- **Fine-tune**: Downstream tasks
- **BERT, GPT**: Pre-trained models

**Result**: State-of-the-art NLP

## Real-World Examples

### Example 1: ImageNet Transfer

**Use**: Custom image classification

**Method**: 
- **Pre-train**: ImageNet
- **Fine-tune**: Custom dataset
- **Result**: High accuracy

**Benefit**: Less data needed

### Example 2: BERT Fine-tuning

**Use**: NLP tasks

**Method**: 
- **Pre-train**: Large corpus
- **Fine-tune**: Specific tasks
- **Result**: Excellent performance

**Benefit**: Transfer knowledge

## Common Pitfalls

### Problem: Overfitting

```c
// BAD: Overfit to small dataset
// Poor generalization

// GOOD: Use regularization
// Data augmentation
```

### Problem: Wrong Learning Rate

```c
// BAD: Too high LR
// Destroy pre-trained weights

// GOOD: Use lower LR
// Differential learning rates
```

## Quiz

1. What is transfer learning?
   - **A)** Train from scratch
   - **B)** Use knowledge from pre-trained models for new tasks
   - **C)** No learning
   - **D)** Random initialization

2. What is fine-tuning?
   - **A)** Feature extraction only
   - **B)** Updating pre-trained model weights on target task
   - **C)** No training
   - **D)** Freezing all layers

3. What is domain adaptation?
   - **A)** Same domain
   - **B)** Adapting model to different data distribution
   - **C)** No adaptation
   - **D)** Same distribution

**Answers:**
1. **B** - Transfer learning leverages knowledge learned from source task (often large dataset) to improve performance on target task
2. **B** - Fine-tuning updates pre-trained model weights by training on target task, often with lower learning rates
3. **B** - Domain adaptation addresses distribution shift between source and target domains using techniques like adversarial training

## Next Steps

- [Model Interpretability](./03.%20Model%20Interpretability.md) - Interpretability
- [ML Systems Design](./04.%20ML%20Systems%20Design%20-%20MLOps.md) - MLOps

