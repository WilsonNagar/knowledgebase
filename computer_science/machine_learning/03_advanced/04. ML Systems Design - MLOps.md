---
number: 4
title: "ML Systems Design - MLOps"
slug: "ml-systems-design-mlops"
level: "advanced"
tags: ["machine-learning", "mlops", "ml-systems", "deployment", "monitoring"]
prerequisites: ["model-interpretability"]
estimated_minutes: 160
contributors: []
diagrams: []
examples: []
canonical_id: "cs-ml-adv-04"
---

# ML Systems Design - MLOps

## Overview

MLOps (Machine Learning Operations) encompasses practices for deploying, monitoring, and maintaining ML systems in production. Understanding ML system architecture, CI/CD for ML, model versioning, monitoring, and production best practices is essential for building reliable ML systems.

## Table of Contents

1. [What is MLOps?](#what-is-mlops)
2. [ML System Architecture](#ml-architecture)
3. [Model Versioning](#model-versioning)
4. [CI/CD for ML](#cicd)
5. [Model Deployment](#deployment)
6. [Monitoring & Observability](#monitoring)
7. [Data Management](#data-management)
8. [Best Practices](#best-practices)

## What is MLOps?

### Definition

**MLOps**: ML + DevOps

**Goal**: 
```
Deploy and maintain ML systems
```

**Practices**: 
- **Versioning**: Model and data versioning
- **CI/CD**: Continuous integration/deployment
- **Monitoring**: Monitor models
- **Automation**: Automate workflows

**Use**: Production ML systems

### MLOps vs DevOps

**DevOps**: 
```
Code deployment
Infrastructure
```

**MLOps**: 
```
Model deployment
Data pipelines
Model monitoring
```

**Difference**: Additional ML-specific concerns

## ML System Architecture

### ML System Components

**1. Data Pipeline**: 
```
Data ingestion
Preprocessing
Feature engineering
```

**2. Training Pipeline**: 
```
Model training
Evaluation
Validation
```

**3. Serving Infrastructure**: 
```
Model serving
API endpoints
Load balancing
```

**4. Monitoring**: 
```
Model monitoring
Data monitoring
Performance monitoring
```

### ML System Flow

**Flow**: 
```
Data → Training → Model → Deployment → Monitoring
```

**Feedback Loop**: 
```
Monitoring → Retraining → Redeployment
```

## Model Versioning

### What is Model Versioning?

**Model Versioning**: 
```
Track model versions
```

**Components**: 
- **Model Artifacts**: Model files
- **Code**: Training code
- **Data**: Training data versions
- **Hyperparameters**: Configurations

**Use**: Reproducibility, rollback

### Model Registry

**Model Registry**: 
```
Centralized model storage
```

**Features**: 
- **Versioning**: Version models
- **Metadata**: Store metadata
- **Lineage**: Track lineage
- **Staging**: Staging environments

**Use**: Model management

## CI/CD for ML

### CI/CD Pipeline

**Continuous Integration**: 
```
1. Code changes trigger tests
2. Run tests
3. Validate code
```

**Continuous Deployment**: 
```
1. Train model
2. Evaluate model
3. Deploy if passes
```

**ML-Specific**: 
- **Data Tests**: Test data quality
- **Model Tests**: Test model performance
- **Integration Tests**: End-to-end tests

### ML CI/CD Stages

**1. Data Validation**: 
```
Validate data quality
```

**2. Training**: 
```
Train model
```

**3. Evaluation**: 
```
Evaluate model
```

**4. Deployment**: 
```
Deploy if passes
```

**5. Monitoring**: 
```
Monitor in production
```

## Model Deployment

### Deployment Strategies

**1. Batch**: 
```
Batch predictions
```

**2. Real-Time**: 
```
Online predictions
```

**3. Edge**: 
```
Edge deployment
```

**4. Hybrid**: 
```
Combination
```

### Deployment Patterns

**1. Blue-Green**: 
```
Two environments
Switch traffic
```

**2. Canary**: 
```
Gradual rollout
```

**3. A/B Testing**: 
```
Compare models
```

**4. Shadow Mode**: 
```
Run alongside production
```

## Monitoring & Observability

### What to Monitor?

**1. Model Performance**: 
```
Accuracy, metrics
```

**2. Data Drift**: 
```
Input distribution changes
```

**3. Concept Drift**: 
```
Target distribution changes
```

**4. System Metrics**: 
```
Latency, throughput
```

**5. Business Metrics**: 
```
Business impact
```

### Monitoring Tools

**Tools**: 
- **MLflow**: ML lifecycle
- **Weights & Biases**: Experiment tracking
- **Evidently AI**: Data drift
- **Prometheus**: Metrics
- **Grafana**: Visualization

**Use**: Monitor ML systems

## Data Management

### Data Versioning

**Data Versioning**: 
```
Version datasets
```

**Tools**: 
- **DVC**: Data Version Control
- **Pachyderm**: Data pipelines
- **Delta Lake**: Data versioning

**Use**: Reproducibility

### Feature Stores

**Feature Store**: 
```
Centralized feature storage
```

**Features**: 
- **Versioning**: Feature versions
- **Serving**: Feature serving
- **Monitoring**: Feature monitoring

**Use**: Feature management

## Best Practices

### Practice 1: Version Everything

**Version**: 
```
Models, data, code, configs
```

**Benefit**: Reproducibility

### Practice 2: Automate

**Automate**: 
```
Training, deployment, monitoring
```

**Benefit**: Efficiency

### Practice 3: Monitor Continuously

**Monitor**: 
```
Performance, drift, metrics
```

**Benefit**: Early detection

### Practice 4: Test Thoroughly

**Test**: 
```
Unit, integration, end-to-end
```

**Benefit**: Quality

## Applications

### Application 1: Recommendation Systems

**Use**: Product recommendations

**MLOps**: 
- **Training**: Continuous training
- **Deployment**: Real-time serving
- **Monitoring**: Monitor recommendations

**Result**: Reliable recommendations

### Application 2: Fraud Detection

**Use**: Detect fraud

**MLOps**: 
- **Training**: Regular retraining
- **Deployment**: Real-time detection
- **Monitoring**: Monitor fraud rates

**Result**: Effective fraud detection

## Real-World Examples

### Example 1: Netflix Recommendations

**Use**: Content recommendations

**MLOps**: 
- **A/B Testing**: Test algorithms
- **Monitoring**: Monitor engagement
- **Retraining**: Regular updates

**Result**: Personalized recommendations

### Example 2: Uber ETA

**Use**: Estimate arrival times

**MLOps**: 
- **Real-Time**: Real-time predictions
- **Monitoring**: Monitor accuracy
- **Retraining**: Update models

**Result**: Accurate ETAs

## Common Pitfalls

### Problem: No Monitoring

```c
// BAD: Deploy and forget
// Models degrade

// GOOD: Monitor continuously
// Detect issues early
```

### Problem: No Versioning

```c
// BAD: No versioning
// Cannot reproduce

// GOOD: Version everything
// Reproducibility
```

## Quiz

1. What is MLOps?
   - **A)** Model training
   - **B)** Practices for deploying and maintaining ML systems in production
   - **C)** Data collection
   - **D)** Model evaluation

2. What is model versioning?
   - **A)** Model training
   - **B)** Tracking model versions with code, data, and configurations
   - **C)** Model deletion
   - **D)** Model evaluation

3. What should be monitored in production?
   - **A)** Only accuracy
   - **B)** Model performance, data drift, concept drift, system metrics
   - **C)** Only latency
   - **D)** Only throughput

**Answers:**
1. **B** - MLOps combines ML and DevOps practices for reliable production ML systems
2. **B** - Model versioning tracks models along with training code, data versions, and hyperparameters for reproducibility
3. **B** - Production monitoring should track model performance, data/concept drift, system metrics, and business impact

## Next Steps

- [Advanced ML Topics](../machine_learning/05.%20Advanced%20ML%20Topics.md) - Advanced topics
- [ML Case Studies](../machine_learning/06.%20ML%20Case%20Studies.md) - Case studies

