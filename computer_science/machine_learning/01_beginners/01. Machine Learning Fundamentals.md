---
number: 1
title: "Machine Learning Fundamentals"
slug: "machine-learning-fundamentals"
level: "fundamentals"
tags: ["machine-learning", "ml", "neural-networks", "supervised-learning", "algorithms"]
prerequisites: []
estimated_minutes: 120
contributors: []
diagrams: []
examples: []
canonical_id: "cs-ml-01"
---

# Machine Learning Fundamentals

## Overview

Machine Learning enables computers to learn from data without explicit programming. Understanding supervised learning, neural networks, optimization, and fundamental ML concepts is essential for AI applications, data science, and modern software development.

## Table of Contents

1. [What is Machine Learning?](#what-is-ml)
2. [Types of Learning](#types)
3. [Supervised Learning](#supervised)
4. [Linear Regression](#linear-regression)
5. [Logistic Regression](#logistic-regression)
6. [Neural Networks](#neural-networks)
7. [Training & Optimization](#training)
8. [Overfitting & Regularization](#overfitting)
9. [Evaluation Metrics](#evaluation)

## What is Machine Learning?

### Traditional Programming vs ML

**Traditional Programming**:
```
Input + Rules → Output

Example:
if (temperature > 25) {
    output = "hot";
} else {
    output = "cold";
}
```

**Machine Learning**:
```
Input + Output → Rules (Model)

Example:
Data: [(20°C, "cold"), (30°C, "hot"), ...]
Model learns: temperature → "hot"/"cold"
```

### ML Definition

**Machine Learning**: Algorithm that improves performance on a task through experience

**Key Components**:
- **Data**: Training examples
- **Model**: Mathematical function
- **Learning**: Adjusting model parameters
- **Prediction**: Using model on new data

## Types of Learning

### Supervised Learning

**Definition**: Learn from labeled examples

**Input**: (features, label) pairs

**Goal**: Predict label for new examples

**Examples**:
- **Classification**: Email spam/not spam
- **Regression**: Predict house price

### Unsupervised Learning

**Definition**: Learn from unlabeled data

**Input**: Features only (no labels)

**Goal**: Find patterns, structure

**Examples**:
- **Clustering**: Group similar data
- **Dimensionality reduction**: Reduce features

### Reinforcement Learning

**Definition**: Learn through interaction and rewards

**Input**: Actions, states, rewards

**Goal**: Maximize cumulative reward

**Examples**: Game playing, robotics

## Supervised Learning

### Classification

**Goal**: Predict discrete label

**Examples**:
- **Binary**: Spam/Not spam
- **Multi-class**: Cat/Dog/Bird

**Algorithm**: Logistic regression, neural networks, decision trees

### Regression

**Goal**: Predict continuous value

**Examples**:
- **House price**: Predict price from features
- **Temperature**: Predict temperature from time

**Algorithm**: Linear regression, neural networks

## Linear Regression

### What is Linear Regression?

**Linear Regression**: Predict continuous value using linear function

**Model**: y = w₁x₁ + w₂x₂ + ... + wₙxₙ + b

**Parameters**:
- **Weights** (w): Slope coefficients
- **Bias** (b): Intercept

### Cost Function

**Mean Squared Error (MSE)**:
```
MSE = (1/m) Σ(y_pred - y_true)²

Where:
  m = number of examples
  y_pred = predicted value
  y_true = actual value
```

**Goal**: Minimize MSE

### Gradient Descent

**Algorithm**: Iteratively update parameters

**Update Rule**:
```
w = w - α × (∂MSE/∂w)

Where:
  α = learning rate
  ∂MSE/∂w = gradient (derivative)
```

**Process**:
```
1. Initialize weights randomly
2. For each iteration:
   a. Compute predictions
   b. Compute cost
   c. Compute gradients
   d. Update weights
3. Repeat until convergence
```

### Example

**Data**: House size → Price
```
Size (sq ft)  Price ($)
1000          200,000
1500          300,000
2000          400,000
```

**Model**: price = w × size + b

**Training**:
```
Initialize: w = 0, b = 0
After training: w ≈ 200, b ≈ 0
Model: price = 200 × size
```

## Logistic Regression

### What is Logistic Regression?

**Logistic Regression**: Predict probability for classification

**Model**: p = σ(w₁x₁ + w₂x₂ + ... + wₙxₙ + b)

**Sigmoid Function**: σ(z) = 1 / (1 + e⁻ᶻ)

**Output**: Probability between 0 and 1

### Decision Boundary

**Threshold**: Typically 0.5

**Classification**:
```
if p ≥ 0.5: predict class 1
if p < 0.5: predict class 0
```

### Cost Function

**Log Loss (Binary Cross-Entropy)**:
```
Loss = -[y×log(p) + (1-y)×log(1-p)]

Where:
  y = true label (0 or 1)
  p = predicted probability
```

**Goal**: Minimize log loss

## Neural Networks

### What are Neural Networks?

**Neural Network**: Network of interconnected neurons

**Inspired by**: Biological neurons

**Structure**: Layers of neurons

### Neuron (Perceptron)

**Structure**:
```
Inputs: x₁, x₂, ..., xₙ
Weights: w₁, w₂, ..., wₙ
Bias: b

Output: f(Σ(wᵢxᵢ) + b)

Where f = activation function
```

**Activation Functions**:
- **Sigmoid**: σ(z) = 1 / (1 + e⁻ᶻ)
- **ReLU**: f(z) = max(0, z)
- **Tanh**: tanh(z)

### Neural Network Architecture

**Layers**:
```
Input Layer → Hidden Layer(s) → Output Layer

Example:
Input (3 neurons) → Hidden (4 neurons) → Output (1 neuron)
```

**Feedforward**:
```
1. Input → Hidden layer
2. Hidden → Output layer
3. Output = prediction
```

### Backpropagation

**Purpose**: Compute gradients for training

**Process**:
```
1. Forward pass: Compute predictions
2. Compute loss
3. Backward pass: Compute gradients
   a. Output layer → Hidden layer
   b. Hidden layer → Input layer
4. Update weights using gradients
```

**Chain Rule**: Used to compute gradients through layers

## Training & Optimization

### Training Process

**Steps**:
```
1. Initialize weights (random)
2. For each epoch:
   a. Forward pass: Compute predictions
   b. Compute loss
   c. Backward pass: Compute gradients
   d. Update weights
3. Evaluate on validation set
4. Repeat until convergence
```

### Optimization Algorithms

**1. Gradient Descent**:
```
w = w - α × ∇L

Where:
  α = learning rate
  ∇L = gradient of loss
```

**2. Stochastic Gradient Descent (SGD)**:
```
Update after each example (or small batch)
Faster, noisier
```

**3. Adam**:
```
Adaptive learning rate
Momentum + RMSprop
Faster convergence
```

### Learning Rate

**Importance**: Critical hyperparameter

**Too High**: Overshoots minimum, diverges

**Too Low**: Slow convergence, may get stuck

**Adaptive**: Learning rate schedules, Adam

## Overfitting & Regularization

### Overfitting

**Problem**: Model memorizes training data, doesn't generalize

**Symptoms**:
- **Training accuracy**: High
- **Validation accuracy**: Low
- **Gap**: Large gap between train/val

**Causes**:
- **Complex model**: Too many parameters
- **Small dataset**: Not enough data
- **Noise**: Model learns noise

### Regularization

**Purpose**: Prevent overfitting

**Methods**:

**1. L2 Regularization**:
```
Add penalty: λ × Σw²

Cost = MSE + λ × Σw²

Effect: Shrinks weights toward zero
```

**2. L1 Regularization**:
```
Add penalty: λ × Σ|w|

Effect: Sets some weights to zero (feature selection)
```

**3. Dropout**:
```
Randomly set neurons to zero during training

Effect: Prevents co-adaptation
```

**4. Early Stopping**:
```
Stop training when validation loss increases

Effect: Prevents overfitting
```

## Evaluation Metrics

### Classification Metrics

**Accuracy**:
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)

Where:
  TP = True Positives
  TN = True Negatives
  FP = False Positives
  FN = False Negatives
```

**Precision**:
```
Precision = TP / (TP + FP)

What fraction of positive predictions are correct?
```

**Recall**:
```
Recall = TP / (TP + FN)

What fraction of actual positives are found?
```

**F1 Score**:
```
F1 = 2 × (Precision × Recall) / (Precision + Recall)

Harmonic mean of precision and recall
```

### Regression Metrics

**Mean Squared Error (MSE)**:
```
MSE = (1/m) Σ(y_pred - y_true)²
```

**Mean Absolute Error (MAE)**:
```
MAE = (1/m) Σ|y_pred - y_true|
```

**R² Score**:
```
R² = 1 - (SS_res / SS_tot)

Measures how well model fits data
```

## Real-World Examples

### Example 1: Image Classification

**Task**: Classify images (cat/dog)

**Model**: Convolutional Neural Network (CNN)

**Process**:
```
1. Input: Image pixels
2. Convolutional layers: Extract features
3. Fully connected layers: Classify
4. Output: Probability (cat/dog)
```

### Example 2: Sentiment Analysis

**Task**: Classify text sentiment (positive/negative)

**Model**: Neural network or transformer

**Process**:
```
1. Input: Text → Embeddings
2. Neural network: Process embeddings
3. Output: Probability (positive/negative)
```

### Example 3: House Price Prediction

**Task**: Predict house price

**Model**: Linear regression or neural network

**Features**: Size, bedrooms, location, etc.

**Output**: Price (continuous value)

## Common Pitfalls

### Problem: Data Leakage

```python
# BAD: Use future data to predict past
# Train on 2024 data, test on 2023 data

# GOOD: Proper train/test split
train_data = data[:2023]
test_data = data[2024:]
```

### Problem: Overfitting

```python
# BAD: Complex model, small dataset
model = NeuralNetwork(layers=10, neurons=1000)
train(model, small_dataset)  # Overfits!

# GOOD: Regularization or simpler model
model = NeuralNetwork(layers=2, neurons=64)
train(model, large_dataset, regularization=True)
```

### Problem: Wrong Evaluation

```python
# BAD: Evaluate on training data only
accuracy = evaluate(model, train_data)  # Overly optimistic

# GOOD: Evaluate on held-out test set
accuracy = evaluate(model, test_data)  # True performance
```

## Quiz

1. What is the main difference between supervised and unsupervised learning?
   - **A)** No difference
   - **B)** Supervised uses labeled data, unsupervised uses unlabeled data
   - **C)** Supervised is faster
   - **D)** Unsupervised is more accurate

2. What is overfitting?
   - **A)** Model too simple
   - **B)** Model memorizes training data and doesn't generalize
   - **C)** Model too slow
   - **D)** Model uses too much memory

3. What does gradient descent do?
   - **A)** Increases cost
   - **B)** Iteratively updates parameters to minimize cost function
   - **C)** Classifies data
   - **D)** Visualizes data

**Answers:**
1. **B** - Supervised learning uses labeled training data (input-output pairs), while unsupervised learning finds patterns in unlabeled data
2. **B** - Overfitting occurs when a model memorizes the training data (high training accuracy) but fails to generalize to new data (low validation/test accuracy)
3. **B** - Gradient descent is an optimization algorithm that iteratively updates model parameters (weights) by moving in the direction that minimizes the cost function

## Next Steps

- [Neural Network Architecture Basics](../02_intermediate/02.%20Neural%20Network%20Architecture%20Basics.md) - Deep dive into networks
- [Optimization Algorithms - SGD, Adam, Momentum](../02_intermediate/03.%20Optimization%20Algorithms%20-%20SGD%2C%20Adam%2C%20Momentum.md) - Training optimization

