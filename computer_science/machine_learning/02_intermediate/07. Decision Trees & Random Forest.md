---
number: 7
title: "Decision Trees & Random Forest"
slug: "decision-trees-random-forest"
level: "intermediate"
tags: ["machine-learning", "decision-trees", "random-forest", "ensemble", "classification"]
prerequisites: ["regularization-overfitting"]
estimated_minutes: 115
contributors: []
diagrams: []
examples: []
canonical_id: "cs-ml-07"
---

# Decision Trees & Random Forest

## Overview

Decision trees are interpretable machine learning models that make decisions by splitting data based on features. Random forests combine multiple decision trees for improved performance. Understanding decision tree construction, splitting criteria, and ensemble methods is essential for both interpretable models and high-performance classifiers.

## Table of Contents

1. [What are Decision Trees?](#what-are-decision-trees)
2. [Tree Construction](#tree-construction)
3. [Splitting Criteria](#splitting-criteria)
4. [Pruning](#pruning)
5. [Decision Tree Algorithms](#algorithms)
6. [Random Forest](#random-forest)
7. [Gradient Boosting](#gradient-boosting)
8. [Applications](#applications)

## What are Decision Trees?

### Definition

**Decision Tree**: Tree-like model for decisions

**Structure**: 
- **Nodes**: Decision points (features)
- **Branches**: Feature values
- **Leaves**: Predictions (classes or values)

**Use**: Classification and regression

### Decision Tree Example

**Classification**:
```
Is it sunny?
├── Yes → Is humidity high?
│   ├── Yes → Don't play
│   └── No → Play
└── No → Play
```

**Interpretable**: Easy to understand

## Tree Construction

### Construction Process

**Algorithm** (Recursive):
```
1. Start with all data at root
2. Find best split (feature + threshold)
3. Split data based on split
4. Recursively build subtrees
5. Stop when stopping criterion met
```

**Stopping Criteria**:
- **Pure node**: All same class
- **No features**: No features left
- **Max depth**: Reached maximum depth
- **Min samples**: Too few samples

### Split Selection

**Goal**: Find best split

**Method**: Try all features and thresholds

**Criterion**: Maximize information gain (or minimize impurity)

## Splitting Criteria

### Information Gain

**Information Gain**: Reduction in entropy

**Entropy**: Measure of impurity

**Formula**:
```
Entropy(S) = -Σ p_i × log₂(p_i)
Information Gain = Entropy(S) - Σ (|S_v|/|S|) × Entropy(S_v)
```

**Higher Gain**: Better split

### Gini Impurity

**Gini Impurity**: Measure of impurity

**Formula**:
```
Gini(S) = 1 - Σ p_i²
```

**Lower Gini**: Purer node

**Use**: CART algorithm

### Variance Reduction

**Variance**: For regression

**Formula**:
```
Variance(S) = (1/n) × Σ (x_i - mean)²
```

**Lower Variance**: Better split

**Use**: Regression trees

## Pruning

### What is Pruning?

**Pruning**: Remove unnecessary branches

**Purpose**: Prevent overfitting

**Types**: Pre-pruning, post-pruning

### Pre-Pruning

**Method**: Stop early

**Criteria**:
- **Max depth**: Limit tree depth
- **Min samples**: Minimum samples per leaf
- **Min gain**: Minimum information gain

**Benefits**: Simpler, faster

**Limitation**: May underfit

### Post-Pruning

**Method**: Build full tree, then prune

**Process**:
```
1. Build complete tree
2. Evaluate subtrees
3. Remove subtrees that don't improve validation
```

**Benefits**: Better generalization

**Trade-off**: More complex

## Decision Tree Algorithms

### ID3

**Algorithm**: Iterative Dichotomiser 3

**Criterion**: Information gain

**Use**: Classification only

**Limitation**: No handling of continuous features

### C4.5

**Algorithm**: Improvement over ID3

**Features**:
- **Continuous features**: Handles continuous values
- **Missing values**: Handles missing data
- **Pruning**: Post-pruning

**Use**: Classification

### CART

**Algorithm**: Classification and Regression Trees

**Features**:
- **Binary splits**: Always binary
- **Gini/ Variance**: Uses Gini (classification) or variance (regression)
- **Pruning**: Cost-complexity pruning

**Use**: Both classification and regression

## Random Forest

### What is Random Forest?

**Random Forest**: Ensemble of decision trees

**Method**: 
- **Bootstrap**: Sample data with replacement
- **Feature sampling**: Random subset of features
- **Voting**: Majority vote (classification) or average (regression)

**Benefits**: 
- **Reduces overfitting**: Less prone to overfitting
- **Better performance**: Often better than single tree
- **Handles missing values**: Robust to missing data

### Random Forest Algorithm

**Process**:
```
For each tree:
  1. Bootstrap sample data
  2. Build tree with random feature subset
  3. No pruning (grow full tree)
  
Prediction:
  Classification: Majority vote
  Regression: Average
```

**Hyperparameters**:
- **n_estimators**: Number of trees
- **max_features**: Features per split
- **max_depth**: Maximum depth

### Random Forest Benefits

**1. Reduces Overfitting**:
```
Averaging reduces variance
```

**2. Handles Non-linearity**:
```
Can capture complex patterns
```

**3. Feature Importance**:
```
Can identify important features
```

**4. Robust**:
```
Handles outliers, missing values
```

## Gradient Boosting

### What is Gradient Boosting?

**Gradient Boosting**: Sequential ensemble

**Method**: 
- **Sequential**: Trees built sequentially
- **Residuals**: Each tree fits residuals
- **Weighted**: Trees weighted by learning rate

**Process**:
```
1. Start with initial prediction
2. For each tree:
   a. Compute residuals
   b. Fit tree to residuals
   c. Add weighted tree to ensemble
3. Final prediction: Sum of all trees
```

### Gradient Boosting Benefits

**1. High Performance**:
```
Often best performance
```

**2. Flexibility**:
```
Works for classification and regression
```

**3. Feature Importance**:
```
Can identify important features
```

**Limitation**: More prone to overfitting

## Applications

### Application 1: Classification

**Use**: Predict class labels

**Examples**: 
- **Spam detection**: Email classification
- **Medical diagnosis**: Disease classification
- **Credit scoring**: Loan approval

### Application 2: Regression

**Use**: Predict continuous values

**Examples**:
- **Price prediction**: House prices
- **Demand forecasting**: Sales prediction
- **Risk assessment**: Insurance premiums

### Application 3: Feature Selection

**Use**: Identify important features

**Method**: Feature importance scores

**Benefit**: Understand what matters

## Real-World Examples

### Example 1: Credit Scoring

**Features**: Income, age, credit history

**Tree**: Splits on features

**Output**: Approve/Reject

**Interpretable**: Can explain decision

### Example 2: Medical Diagnosis

**Features**: Symptoms, test results

**Tree**: Decision path

**Output**: Diagnosis

**Interpretable**: Doctors can understand

## Common Pitfalls

### Problem: Overfitting

```python
# BAD: No depth limit
tree = DecisionTreeClassifier()  # May overfit!

# GOOD: Limit depth
tree = DecisionTreeClassifier(max_depth=10)
```

### Problem: Not Using Ensemble

```python
# BAD: Single tree
# May overfit, unstable

# GOOD: Random forest
forest = RandomForestClassifier(n_estimators=100)
```

## Quiz

1. What is a decision tree?
   - **A)** Neural network
   - **B)** Tree-like model making decisions by splitting on features
   - **C)** Linear model
   - **D)** Clustering algorithm

2. What is information gain?
   - **A)** Information added
   - **B)** Reduction in entropy after split
   - **C)** Information lost
   - **D)** Information stored

3. What is random forest?
   - **A)** Single tree
   - **B)** Ensemble of decision trees using bootstrap sampling and feature randomization
   - **C)** Neural network
   - **D)** Linear model

**Answers:**
1. **B** - A decision tree is a tree-like model where internal nodes represent decisions (feature splits) and leaves represent predictions, making it interpretable
2. **B** - Information gain measures the reduction in entropy (impurity) achieved by splitting data, with higher gain indicating better splits
3. **B** - Random forest is an ensemble method that combines multiple decision trees trained on bootstrap samples with random feature subsets, then votes/averages predictions

## Next Steps

- [Vector Embeddings](./08.%20Vector%20Embeddings%20%26%20Word2Vec.md) - Word embeddings
- [Reinforcement Learning Fundamentals](./09.%20Reinforcement%20Learning%20Fundamentals.md) - RL basics

