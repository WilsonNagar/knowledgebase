---
number: 2
title: "Neural Network Architecture Basics"
slug: "neural-network-architecture-basics"
level: "intermediate"
tags: ["machine-learning", "neural-networks", "deep-learning", "architecture", "layers"]
prerequisites: ["machine-learning-fundamentals"]
estimated_minutes: 125
contributors: []
diagrams: []
examples: []
canonical_id: "cs-ml-02"
---

# Neural Network Architecture Basics

## Overview

Neural network architecture is fundamental to deep learning. Understanding different layer types, activation functions, network topologies, and architectural patterns is essential for designing effective neural networks and understanding modern deep learning models.

## Table of Contents

1. [Neural Network Basics](#basics)
2. [Layer Types](#layer-types)
3. [Activation Functions](#activations)
4. [Network Topologies](#topologies)
5. [Feedforward Networks](#feedforward)
6. [Convolutional Neural Networks (CNNs)](#cnns)
7. [Recurrent Neural Networks (RNNs)](#rnns)
8. [Architecture Design Principles](#design)

## Neural Network Basics

### What is a Neural Network?

**Neural Network**: Network of interconnected neurons

**Inspired by**: Biological neurons

**Structure**: Layers of neurons

**Function**: Learn complex patterns from data

### Basic Components

**1. Neurons (Nodes)**:
- **Input**: Receives inputs
- **Weights**: Multiply inputs
- **Bias**: Add offset
- **Activation**: Apply activation function
- **Output**: Produce output

**2. Layers**:
- **Input Layer**: Receives data
- **Hidden Layers**: Process data
- **Output Layer**: Produces predictions

**3. Connections**:
- **Weights**: Strength of connections
- **Fully Connected**: Each neuron connected to all in next layer

## Layer Types

### Dense (Fully Connected) Layer

**Structure**: Every neuron connected to all neurons in next layer

**Operation**: y = f(Wx + b)

**Where**:
- **W**: Weight matrix
- **x**: Input vector
- **b**: Bias vector
- **f**: Activation function

**Use**: General-purpose, final layers

### Convolutional Layer

**Structure**: Convolution operation

**Operation**: Apply filters (kernels) to input

**Properties**:
- **Sparse connections**: Not fully connected
- **Parameter sharing**: Same weights across positions
- **Translation invariant**: Detects patterns regardless of position

**Use**: Image processing, spatial data

### Pooling Layer

**Types**:
- **Max Pooling**: Take maximum value
- **Average Pooling**: Take average value

**Purpose**: Reduce spatial dimensions

**Benefits**: 
- **Reduces parameters**: Fewer computations
- **Translation invariant**: More robust

### Recurrent Layer

**Structure**: Connections form cycles

**Operation**: Maintains hidden state

**Properties**:
- **Memory**: Remembers previous inputs
- **Sequential**: Processes sequences

**Use**: Time series, text, sequences

## Activation Functions

### Purpose

**Activation Function**: Non-linear transformation

**Why**: Enables learning non-linear patterns

**Without**: Network is just linear transformation

### Common Activation Functions

**1. Sigmoid**:
```
σ(x) = 1 / (1 + e^(-x))
Range: (0, 1)
Use: Output layer for binary classification
```

**2. Tanh**:
```
tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))
Range: (-1, 1)
Use: Hidden layers
```

**3. ReLU** (Rectified Linear Unit):
```
ReLU(x) = max(0, x)
Range: [0, ∞)
Use: Most common for hidden layers
```

**4. Leaky ReLU**:
```
LeakyReLU(x) = max(0.01x, x)
Range: (-∞, ∞)
Use: Avoids dying ReLU problem
```

**5. Softmax**:
```
softmax(x_i) = e^(x_i) / Σ e^(x_j)
Range: (0, 1), sums to 1
Use: Output layer for multi-class classification
```

### Why ReLU is Popular

**Advantages**:
- **Sparse**: Many activations are 0
- **Gradient**: Simple gradient (0 or 1)
- **Fast**: Fast computation
- **Avoids**: Vanishing gradient (for positive values)

**Disadvantages**:
- **Dying ReLU**: Neurons can "die" (always output 0)
- **Solution**: Leaky ReLU, ELU

## Network Topologies

### Feedforward Networks

**Structure**: Information flows forward only

**No cycles**: No feedback connections

**Use**: Classification, regression

### Recurrent Networks

**Structure**: Cycles allowed

**Feedback**: Output feeds back as input

**Use**: Sequences, time series

### Residual Networks

**Structure**: Skip connections

**Operation**: Add input to output

**Benefits**: Easier training of deep networks

## Feedforward Networks

### Structure

**Layers**:
```
Input → Hidden 1 → Hidden 2 → ... → Output
```

**Forward Pass**:
```
x → h₁ = f₁(W₁x + b₁)
h₁ → h₂ = f₂(W₂h₁ + b₂)
h₂ → y = f₃(W₃h₂ + b₃)
```

### Example: Multi-Layer Perceptron (MLP)

**Architecture**:
```
Input (784) → Hidden (128) → Hidden (64) → Output (10)
```

**For**: MNIST digit classification

**Layers**:
- **Input**: 784 neurons (28×28 image)
- **Hidden 1**: 128 neurons, ReLU
- **Hidden 2**: 64 neurons, ReLU
- **Output**: 10 neurons, Softmax (10 classes)

## Convolutional Neural Networks (CNNs)

### CNN Architecture

**Typical Structure**:
```
Input → Conv → Pool → Conv → Pool → Dense → Output
```

**Components**:
- **Convolutional Layers**: Detect features
- **Pooling Layers**: Reduce dimensions
- **Dense Layers**: Classify

### Convolution Operation

**Filter (Kernel)**: Small matrix

**Operation**: Slide filter over input

**Output**: Feature map

**Example**: 3×3 filter on image
```
Input:     Filter:    Output:
[1 2 3]    [-1 0 1]   [feature map]
[4 5 6]    [-2 0 2]
[7 8 9]    [-1 0 1]
```

### CNN Example: Image Classification

**Architecture**:
```
Input (224×224×3)
  ↓
Conv1 (32 filters, 3×3) → ReLU
  ↓
MaxPool (2×2)
  ↓
Conv2 (64 filters, 3×3) → ReLU
  ↓
MaxPool (2×2)
  ↓
Flatten
  ↓
Dense (128) → ReLU
  ↓
Dense (10) → Softmax
```

## Recurrent Neural Networks (RNNs)

### RNN Structure

**Recurrent Connection**: Hidden state feeds back

**Operation**:
```
h_t = f(W_hh × h_{t-1} + W_xh × x_t + b)
y_t = f(W_hy × h_t + b_y)
```

**Properties**:
- **Memory**: Maintains hidden state
- **Sequential**: Processes one element at a time

### RNN Variants

**1. LSTM** (Long Short-Term Memory):
- **Gates**: Forget, input, output gates
- **Memory**: Long-term memory
- **Use**: Long sequences

**2. GRU** (Gated Recurrent Unit):
- **Simpler**: Fewer parameters than LSTM
- **Gates**: Reset and update gates
- **Use**: Similar to LSTM, faster

### RNN Example: Text Generation

**Architecture**:
```
Input (char/word) → LSTM → LSTM → Dense → Output (next char/word)
```

**Process**: Predict next character/word given previous

## Architecture Design Principles

### Depth vs Width

**Depth**: Number of layers

**Width**: Number of neurons per layer

**Trade-off**:
- **Deeper**: More complex patterns, harder to train
- **Wider**: More capacity, more parameters

### Regularization

**1. Dropout**:
```
Randomly set neurons to 0 during training
Prevents overfitting
```

**2. Batch Normalization**:
```
Normalize activations
Faster training, more stable
```

**3. Weight Decay (L2)**:
```
Penalize large weights
Prevents overfitting
```

### Initialization

**Weights**: Initialize randomly

**Methods**:
- **Xavier/Glorot**: For tanh/sigmoid
- **He**: For ReLU
- **Small random**: Avoid symmetry

## Real-World Examples

### Example 1: Image Classification (ResNet)

**Architecture**: Residual Network

**Key**: Skip connections

**Depth**: 50-152 layers

**Use**: ImageNet classification

### Example 2: Natural Language Processing (Transformer)

**Architecture**: Attention mechanism

**Key**: Self-attention, no recurrence

**Use**: BERT, GPT, translation

### Example 3: Object Detection (YOLO)

**Architecture**: CNN + detection head

**Key**: Single pass detection

**Use**: Real-time object detection

## Common Pitfalls

### Problem: Vanishing Gradients

```python
# BAD: Too many layers with sigmoid/tanh
# Gradients become very small

# GOOD: Use ReLU, residual connections, batch norm
model = Sequential([
    Dense(128, activation='relu'),
    BatchNormalization(),
    Dense(64, activation='relu'),
    BatchNormalization(),
    Dense(10, activation='softmax')
])
```

### Problem: Overfitting

```python
# BAD: Large network, no regularization
model = Sequential([
    Dense(1000, activation='relu'),
    Dense(1000, activation='relu'),
    Dense(10, activation='softmax')
])
# Overfits on small dataset

# GOOD: Add regularization
model = Sequential([
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(10, activation='softmax')
])
```

## Quiz

1. What is the main purpose of activation functions?
   - **A)** Increase speed
   - **B)** Introduce non-linearity to enable learning complex patterns
   - **C)** Reduce memory
   - **D)** Simplify code

2. What is the main advantage of convolutional layers?
   - **A)** Faster computation
   - **B)** Parameter sharing and translation invariance for spatial data
   - **C)** Simpler architecture
   - **D)** Less memory

3. What is the main difference between RNNs and feedforward networks?
   - **A)** No difference
   - **B)** RNNs have cycles and maintain hidden state for sequential data
   - **C)** RNNs are faster
   - **D)** Feedforward networks are more complex

**Answers:**
1. **B** - Activation functions introduce non-linearity, which is essential because without them, a neural network would just be a linear transformation and couldn't learn complex patterns
2. **B** - Convolutional layers use parameter sharing (same weights across positions) and provide translation invariance, making them ideal for spatial data like images
3. **B** - RNNs have recurrent connections (cycles) that allow them to maintain hidden state and process sequential data, while feedforward networks process data in one forward pass without memory

## Next Steps

- [Optimization Algorithms - SGD, Adam, Momentum](../machine_learning/03.%20Optimization%20Algorithms.md) - Training optimization
- [Backpropagation Deep Dive](../machine_learning/04.%20Backpropagation%20Deep%20Dive.md) - Gradient computation

