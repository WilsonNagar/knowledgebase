---
number: 12
title: "Unsupervised Learning - Clustering, Dimensionality Reduction"
slug: "unsupervised-learning-clustering-dimensionality-reduction"
level: "intermediate"
tags: ["machine-learning", "unsupervised", "clustering", "dimensionality-reduction", "pca"]
prerequisites: ["computer-vision-fundamentals"]
estimated_minutes: 145
contributors: []
diagrams: []
examples: []
canonical_id: "cs-ml-int-12"
---

# Unsupervised Learning - Clustering, Dimensionality Reduction

## Overview

Unsupervised learning discovers patterns in data without labels. Understanding clustering algorithms, dimensionality reduction techniques, and their applications is essential for data exploration, feature learning, and pattern discovery.

## Table of Contents

1. [What is Unsupervised Learning?](#what-is-unsupervised)
2. [Clustering](#clustering)
3. [K-Means](#k-means)
4. [Hierarchical Clustering](#hierarchical)
5. [DBSCAN](#dbscan)
6. [Dimensionality Reduction](#dimensionality)
7. [PCA](#pca)
8. [t-SNE and UMAP](#tsne-umap)

## What is Unsupervised Learning?

### Definition

**Unsupervised Learning**: Learn from unlabeled data

**Goal**: 
- **Discover**: Discover patterns
- **Structure**: Find structure
- **Representation**: Learn representations

**Tasks**: 
- **Clustering**: Group similar data
- **Dimensionality Reduction**: Reduce dimensions
- **Anomaly Detection**: Detect anomalies
- **Density Estimation**: Estimate density

**Use**: Data exploration, preprocessing

### Unsupervised vs Supervised

**Supervised**: 
```
Labeled data
Learn mapping
```

**Unsupervised**: 
```
No labels
Discover structure
```

**Difference**: No ground truth

## Clustering

### What is Clustering?

**Clustering**: Group similar data

**Goal**: 
```
Find groups (clusters)
Minimize within-cluster distance
Maximize between-cluster distance
```

**Use**: 
- **Segmentation**: Customer segmentation
- **Exploration**: Data exploration
- **Preprocessing**: Preprocessing step

### Clustering Types

**1. Partitioning**: 
```
K-means, K-medoids
```

**2. Hierarchical**: 
```
Agglomerative, Divisive
```

**3. Density-Based**: 
```
DBSCAN, OPTICS
```

**4. Model-Based**: 
```
Gaussian Mixture Models
```

## K-Means

### What is K-Means?

**K-Means**: Partitioning clustering

**Algorithm**: 
```
1. Initialize k centroids
2. Assign points to nearest centroid
3. Update centroids
4. Repeat until convergence
```

**Objective**: 
```
Minimize within-cluster sum of squares
```

**Use**: Spherical clusters

### K-Means Limitations

**Limitations**: 
- **K Required**: Must specify k
- **Spherical**: Assumes spherical clusters
- **Sensitive**: Sensitive to initialization

**Solutions**: 
- **Elbow Method**: Choose k
- **K-Means++**: Better initialization
- **Other Algorithms**: Use alternatives

## Hierarchical Clustering

### What is Hierarchical Clustering?

**Hierarchical Clustering**: Build hierarchy

**Types**: 
- **Agglomerative**: Bottom-up
- **Divisive**: Top-down

**Output**: 
```
Dendrogram
```

**Use**: Understand structure

### Agglomerative Clustering

**Algorithm**: 
```
1. Start with each point as cluster
2. Merge closest clusters
3. Repeat until one cluster
```

**Linkage**: 
- **Single**: Minimum distance
- **Complete**: Maximum distance
- **Average**: Average distance

**Use**: Flexible clustering

## DBSCAN

### What is DBSCAN?

**DBSCAN**: Density-Based Clustering

**Parameters**: 
- **eps**: Neighborhood radius
- **minPts**: Minimum points

**Points**: 
- **Core**: Dense neighborhood
- **Border**: Near core
- **Noise**: Outliers

**Use**: Arbitrary shapes, noise

### DBSCAN Advantages

**Advantages**: 
- **No K**: No need to specify k
- **Arbitrary Shapes**: Handles non-spherical
- **Noise**: Identifies outliers

**Use**: Real-world data

## Dimensionality Reduction

### What is Dimensionality Reduction?

**Dimensionality Reduction**: Reduce dimensions

**Goals**: 
- **Visualization**: Visualize high-D data
- **Compression**: Compress data
- **Noise Reduction**: Remove noise
- **Feature Learning**: Learn features

**Methods**: 
- **Linear**: PCA, ICA
- **Nonlinear**: t-SNE, UMAP

**Use**: High-dimensional data

## PCA

### What is PCA?

**PCA**: Principal Component Analysis

**Method**: 
```
1. Center data
2. Compute covariance matrix
3. Find eigenvectors (principal components)
4. Project to lower dimension
```

**Goal**: 
```
Maximize variance
```

**Use**: Linear dimensionality reduction

### PCA Properties

**Properties**: 
- **Linear**: Linear transformation
- **Orthogonal**: Orthogonal components
- **Variance**: Preserves variance

**Use**: 
- **Visualization**: 2D/3D visualization
- **Compression**: Data compression
- **Noise**: Noise reduction

## t-SNE and UMAP

### t-SNE

**t-SNE**: t-Distributed Stochastic Neighbor Embedding

**Method**: 
```
Preserve local neighborhoods
Nonlinear mapping
```

**Use**: 
- **Visualization**: 2D visualization
- **Exploration**: Data exploration

**Limitations**: 
- **Slow**: Computationally expensive
- **Hyperparameters**: Sensitive to parameters

### UMAP

**UMAP**: Uniform Manifold Approximation

**Method**: 
```
Preserve both local and global structure
Faster than t-SNE
```

**Use**: 
- **Visualization**: High-quality visualization
- **Dimensionality Reduction**: General reduction

**Advantages**: 
- **Faster**: Faster than t-SNE
- **Global**: Preserves global structure

## Applications

### Application 1: Customer Segmentation

**Use**: Segment customers

**Method**: 
- **Clustering**: K-means, hierarchical
- **Features**: Purchase history, demographics
- **Segments**: Customer segments

**Result**: Targeted marketing

### Application 2: Image Compression

**Use**: Compress images

**Method**: 
- **PCA**: Principal components
- **Reconstruction**: Reconstruct from components
- **Compression**: Reduce storage

**Result**: Compressed images

## Real-World Examples

### Example 1: Image Clustering

**Use**: Group similar images

**Method**: 
- **Features**: Extract features (CNN)
- **Clustering**: Cluster features
- **Groups**: Image groups

**Result**: Image organization

### Example 2: Gene Expression

**Use**: Analyze gene expression

**Method**: 
- **Dimensionality Reduction**: PCA, t-SNE
- **Visualization**: Visualize patterns
- **Clustering**: Cluster genes

**Result**: Biological insights

## Common Pitfalls

### Problem: Choosing Wrong K

```c
// BAD: Arbitrary k
// Poor clusters

// GOOD: Use elbow method
// Or use DBSCAN
```

### Problem: Scaling

```c
// BAD: Don't scale features
// Features dominate

// GOOD: Scale features
// Normalize before clustering
```

## Quiz

1. What is unsupervised learning?
   - **A)** Learning with labels
   - **B)** Learning patterns from unlabeled data
   - **C)** Reinforcement learning
   - **D)** Supervised learning

2. What is K-means?
   - **A)** Classification algorithm
   - **B)** Partitioning clustering algorithm grouping data into k clusters
   - **C)** Regression algorithm
   - **D)** Neural network

3. What is PCA?
   - **A)** Classification method
   - **B)** Principal Component Analysis for linear dimensionality reduction
   - **C)** Clustering method
   - **D)** Regression method

**Answers:**
1. **B** - Unsupervised learning discovers patterns, structure, and representations from data without labels
2. **B** - K-means partitions data into k clusters by minimizing within-cluster sum of squares
3. **B** - PCA finds principal components (eigenvectors) that maximize variance for dimensionality reduction

## Next Steps

- [Deep Learning Advanced](../machine_learning/13.%20Deep%20Learning%20Advanced.md) - GANs, VAEs, Diffusion Models
- [Transfer Learning & Fine-tuning](../machine_learning/14.%20Transfer%20Learning%20%26%20Fine-tuning.md) - Transfer learning

