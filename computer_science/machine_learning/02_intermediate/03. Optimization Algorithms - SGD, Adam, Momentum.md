---
number: 3
title: "Optimization Algorithms - SGD, Adam, Momentum"
slug: "optimization-algorithms-sgd-adam-momentum"
level: "intermediate"
tags: ["machine-learning", "optimization", "sgd", "adam", "momentum", "gradient-descent"]
prerequisites: ["neural-network-architecture-basics"]
estimated_minutes: 120
contributors: []
diagrams: []
examples: []
canonical_id: "cs-ml-03"
---

# Optimization Algorithms - SGD, Adam, Momentum

## Overview

Optimization algorithms are crucial for training neural networks. Understanding gradient descent variants like SGD, Momentum, Adam, and their properties is essential for effective machine learning model training and achieving good performance.

## Table of Contents

1. [Optimization Problem](#optimization-problem)
2. [Gradient Descent Basics](#gradient-descent)
3. [Stochastic Gradient Descent (SGD)](#sgd)
4. [Momentum](#momentum)
5. [AdaGrad](#adagrad)
6. [RMSprop](#rmsprop)
7. [Adam](#adam)
8. [Learning Rate Schedules](#learning-rate)
9. [Algorithm Comparison](#comparison)

## Optimization Problem

### The Goal

**Objective**: Minimize loss function L(θ)

**Parameters**: θ (weights and biases)

**Gradient**: ∇L(θ) (derivative with respect to parameters)

**Goal**: Find θ* such that L(θ*) is minimum

### Loss Function

**Example**: Mean Squared Error
```
L(θ) = (1/m) Σ(y_pred - y_true)²
```

**Gradient**: 
```
∇L(θ) = (2/m) Σ(y_pred - y_true) × ∇y_pred
```

## Gradient Descent Basics

### Gradient Descent Algorithm

**Update Rule**:
```
θ = θ - α × ∇L(θ)
```

**Where**:
- **α**: Learning rate
- **∇L(θ)**: Gradient

**Process**:
```
1. Initialize parameters θ
2. Compute gradient ∇L(θ)
3. Update: θ = θ - α × ∇L(θ)
4. Repeat until convergence
```

### Learning Rate

**Importance**: Critical hyperparameter

**Too High**: Overshoots minimum, diverges

**Too Low**: Slow convergence, may get stuck

**Optimal**: Balance between speed and stability

## Stochastic Gradient Descent (SGD)

### What is SGD?

**SGD**: Stochastic Gradient Descent

**Difference**: Uses one example (or mini-batch) instead of all

**Update**: 
```
θ = θ - α × ∇L(θ, x_i, y_i)
```

**Where**: (x_i, y_i) is one training example

### SGD Advantages

**1. Faster**: Update after each example

**2. Online**: Can update as data arrives

**3. Escapes**: Local minima (noise helps)

**4. Memory**: Less memory needed

### SGD Disadvantages

**1. Noisy**: Gradient estimates noisy

**2. Slow Convergence**: May take many iterations

**3. Oscillation**: May oscillate around minimum

### Mini-Batch SGD

**Compromise**: Between batch and stochastic

**Process**:
```
Use small batch (e.g., 32, 64, 128 examples)
Compute gradient on batch
Update parameters
```

**Benefits**: 
- **Less noisy**: Than SGD
- **Faster**: Than full batch
- **Efficient**: Good GPU utilization

## Momentum

### What is Momentum?

**Momentum**: Accumulates gradient history

**Idea**: Like ball rolling downhill

**Update**:
```
v = β × v + (1 - β) × ∇L(θ)
θ = θ - α × v
```

**Where**:
- **v**: Velocity (momentum term)
- **β**: Momentum coefficient (typically 0.9)

### Momentum Benefits

**1. Faster**: Accelerates in consistent direction

**2. Smoother**: Reduces oscillation

**3. Escapes**: Shallow local minima

### Momentum Example

**Without Momentum**:
```
Gradient oscillates: +5, -4, +5, -4, ...
Slow convergence
```

**With Momentum**:
```
Velocity accumulates: v = 0.9×v + 0.1×gradient
Smoother path, faster convergence
```

## AdaGrad

### What is AdaGrad?

**AdaGrad**: Adaptive Gradient

**Idea**: Adapt learning rate per parameter

**Update**:
```
G = G + (∇L(θ))²
θ = θ - (α / √(G + ε)) × ∇L(θ)
```

**Where**:
- **G**: Accumulated squared gradients
- **ε**: Small constant (avoid division by zero)

### AdaGrad Properties

**1. Adaptive**: Learning rate decreases for frequent parameters

**2. Sparse**: Good for sparse gradients

**3. Problem**: Learning rate becomes too small

## RMSprop

### What is RMSprop?

**RMSprop**: Root Mean Square Propagation

**Idea**: Fix AdaGrad's decaying learning rate

**Update**:
```
G = β × G + (1 - β) × (∇L(θ))²
θ = θ - (α / √(G + ε)) × ∇L(θ)
```

**Where**: **β**: Decay rate (typically 0.9)

### RMSprop Benefits

**1. Adaptive**: Per-parameter learning rates

**2. Stable**: Learning rate doesn't decay too much

**3. Fast**: Good convergence speed

## Adam

### What is Adam?

**Adam**: Adaptive Moment Estimation

**Combines**: Momentum + RMSprop

**Update**:
```
m = β₁ × m + (1 - β₁) × ∇L(θ)  (first moment)
v = β₂ × v + (1 - β₂) × (∇L(θ))²  (second moment)

m̂ = m / (1 - β₁^t)  (bias correction)
v̂ = v / (1 - β₂^t)  (bias correction)

θ = θ - (α / √(v̂ + ε)) × m̂
```

**Where**:
- **β₁**: First moment decay (typically 0.9)
- **β₂**: Second moment decay (typically 0.999)
- **t**: Time step

### Adam Advantages

**1. Adaptive**: Per-parameter learning rates

**2. Momentum**: Includes momentum

**3. Robust**: Works well for many problems

**4. Fast**: Good convergence

**5. Default**: Often good default choice

### Adam Hyperparameters

**Learning Rate (α)**: Typically 0.001

**β₁**: 0.9 (first moment)

**β₂**: 0.999 (second moment)

**ε**: 10⁻⁸ (small constant)

## Learning Rate Schedules

### Fixed Learning Rate

**Simple**: Constant learning rate

**Problem**: May be too high or too low

### Learning Rate Decay

**1. Step Decay**:
```
α = α₀ × γ^(floor(epoch / step))
```

**2. Exponential Decay**:
```
α = α₀ × e^(-k×epoch)
```

**3. Polynomial Decay**:
```
α = α₀ × (1 - epoch/max_epochs)^power
```

### Adaptive Learning Rate

**ReduceLROnPlateau**:
```
Reduce learning rate when loss plateaus
```

**Cyclic Learning Rate**:
```
Cycle learning rate between bounds
May escape local minima
```

## Algorithm Comparison

### Comparison Table

| Algorithm | Convergence | Memory | Hyperparameters |
|-----------|-------------|--------|-----------------|
| **SGD** | Slow | Low | Learning rate |
| **Momentum** | Faster | Low | Learning rate, β |
| **AdaGrad** | Good (sparse) | Medium | Learning rate |
| **RMSprop** | Good | Medium | Learning rate, β |
| **Adam** | Fast | Medium | Learning rate, β₁, β₂ |

### When to Use What

**SGD**:
- **Simple**: When simplicity needed
- **Large datasets**: When full batch expensive

**Momentum**:
- **Oscillation**: When SGD oscillates
- **Shallow minima**: Need to escape

**Adam**:
- **Default**: Good default choice
- **Many problems**: Works well generally
- **Deep learning**: Common in deep learning

**AdaGrad/RMSprop**:
- **Sparse gradients**: When gradients sparse
- **Specific problems**: When known to work well

## Real-World Examples

### Example 1: Image Classification

**Dataset**: ImageNet

**Optimizer**: Adam or SGD with momentum

**Learning Rate**: 0.001 (Adam) or 0.01 (SGD)

**Schedule**: Reduce on plateau

### Example 2: Language Models

**Dataset**: Large text corpus

**Optimizer**: Adam

**Learning Rate**: 0.0001 (smaller for large models)

**Schedule**: Warmup then decay

## Common Pitfalls

### Problem: Learning Rate Too High

```python
# BAD: Learning rate too high
optimizer = Adam(lr=1.0)  # Diverges!

# GOOD: Appropriate learning rate
optimizer = Adam(lr=0.001)  # Typical default
```

### Problem: Not Using Learning Rate Schedule

```python
# BAD: Fixed learning rate
# May converge slowly or not at all

# GOOD: Use learning rate schedule
scheduler = ReduceLROnPlateau(optimizer, 'min')
# Reduces learning rate when loss plateaus
```

### Problem: Wrong Optimizer for Problem

```python
# BAD: Using Adam for all problems
# May not always be best

# GOOD: Try different optimizers
# SGD with momentum often works well
# Adam is good default
```

## Quiz

1. What is the main advantage of SGD over batch gradient descent?
   - **A)** More accurate
   - **B)** Faster updates and can escape local minima
   - **C)** Less memory
   - **D)** Simpler math

2. What does Adam combine?
   - **A)** SGD and Momentum
   - **B)** Momentum and RMSprop (adaptive learning rates)
   - **C)** AdaGrad and SGD
   - **D)** Only Momentum

3. What is momentum used for?
   - **A)** Increase learning rate
   - **B)** Accumulate gradient history to accelerate convergence and reduce oscillation
   - **C)** Decrease learning rate
   - **D)** Store weights

**Answers:**
1. **B** - SGD updates parameters after each example (or mini-batch), making it faster than batch gradient descent and the noise can help escape local minima
2. **B** - Adam combines momentum (first moment) with RMSprop-style adaptive learning rates (second moment), providing both acceleration and per-parameter adaptation
3. **B** - Momentum accumulates gradient history (like a ball rolling downhill), accelerating convergence in consistent directions and reducing oscillation

## Next Steps

- [Backpropagation Deep Dive](./04.%20Backpropagation%20Deep%20Dive.md) - Gradient computation
- [Regularization & Overfitting](./05.%20Regularization%20%26%20Overfitting.md) - Preventing overfitting

