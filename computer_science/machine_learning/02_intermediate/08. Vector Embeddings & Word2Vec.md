---
number: 8
title: "Vector Embeddings & Word2Vec"
slug: "vector-embeddings-word2vec"
level: "intermediate"
tags: ["machine-learning", "embeddings", "word2vec", "nlp", "representation"]
prerequisites: ["transformers-attention-mechanism"]
estimated_minutes: 120
contributors: []
diagrams: []
examples: []
canonical_id: "cs-ml-08"
---

# Vector Embeddings & Word2Vec

## Overview

Vector embeddings represent words, entities, or concepts as dense vectors in a continuous space, capturing semantic relationships. Understanding Word2Vec, embedding properties, and how embeddings enable semantic understanding is essential for natural language processing and modern AI systems.

## Table of Contents

1. [What are Embeddings?](#what-are-embeddings)
2. [Embedding Properties](#embedding-properties)
3. [Word2Vec Overview](#word2vec-overview)
4. [Skip-gram Model](#skip-gram)
5. [CBOW Model](#cbow)
6. [Training Word2Vec](#training)
7. [Embedding Applications](#applications)
8. [Advanced Embeddings](#advanced)

## What are Embeddings?

### Definition

**Embedding**: Dense vector representation

**Purpose**: Represent discrete objects in continuous space

**Examples**: Words, images, users, items

**Properties**: Similar objects have similar vectors

### Why Embeddings?

**1. Semantic Similarity**:
```
Similar words have similar vectors
```

**2. Mathematical Operations**:
```
Can do arithmetic on vectors
king - man + woman ≈ queen
```

**3. Dimensionality**:
```
Compact representation
Typically 100-300 dimensions
```

**4. Transfer Learning**:
```
Pre-trained embeddings reusable
```

## Embedding Properties

### Semantic Relationships

**Similarity**: Similar words close in space

**Example**:
```
"dog" and "cat" are close
"dog" and "airplane" are far
```

### Analogies

**Word Analogies**: Can solve analogies

**Example**:
```
king - man + woman ≈ queen
Paris - France + Italy ≈ Rome
```

**Method**: Vector arithmetic

### Clustering

**Semantic Clusters**: Related words cluster

**Example**:
```
Animals: dog, cat, horse, bird
Vehicles: car, truck, bus, train
```

## Word2Vec Overview

### What is Word2Vec?

**Word2Vec**: Algorithm for learning word embeddings

**Method**: Predict words from context

**Architecture**: Neural network

**Output**: Word embeddings

### Word2Vec Approaches

**1. Skip-gram**:
```
Predict context from word
```

**2. CBOW** (Continuous Bag of Words):
```
Predict word from context
```

**Both**: Learn embeddings as byproduct

## Skip-gram Model

### Architecture

**Input**: Center word (one-hot)

**Hidden Layer**: Embedding layer (no activation)

**Output**: Context words (softmax)

**Goal**: Predict surrounding words

### Skip-gram Process

**Training**:
```
1. Take center word
2. Predict surrounding words (window)
3. Update embeddings
```

**Example**:
```
Sentence: "the quick brown fox"
Center: "quick"
Context: "the", "brown"
Predict: "the" and "brown" from "quick"
```

### Skip-gram Objective

**Objective**: Maximize probability of context words

**Formula**:
```
maximize: Π P(context_word | center_word)
```

**Loss**: Negative log likelihood

## CBOW Model

### Architecture

**Input**: Context words (sum/average)

**Hidden Layer**: Embedding layer

**Output**: Center word (softmax)

**Goal**: Predict center word from context

### CBOW Process

**Training**:
```
1. Take context words
2. Predict center word
3. Update embeddings
```

**Example**:
```
Sentence: "the quick brown fox"
Context: "the", "brown"
Center: "quick"
Predict: "quick" from context
```

### CBOW vs Skip-gram

**CBOW**:
- **Faster**: Fewer predictions
- **Better**: For frequent words

**Skip-gram**:
- **Better**: For rare words
- **More training**: More predictions

## Training Word2Vec

### Training Process

**1. Initialize Embeddings**:
```
Random initialization
```

**2. Sample Training Pairs**:
```
(center_word, context_word)
```

**3. Forward Pass**:
```
Compute predictions
```

**4. Backward Pass**:
```
Update embeddings
```

**5. Repeat**: Until convergence

### Negative Sampling

**Problem**: Softmax over entire vocabulary expensive

**Solution**: Negative sampling

**Method**:
```
1. Sample positive (actual context)
2. Sample negative (random words)
3. Train binary classifier
```

**Benefit**: Much faster training

### Hierarchical Softmax

**Alternative**: To negative sampling

**Method**: Use binary tree

**Benefit**: O(log V) instead of O(V)

**Trade-off**: More complex

## Embedding Applications

### Application 1: Semantic Search

**Use**: Find semantically similar documents

**Method**: Compare embeddings

**Benefit**: Understand meaning, not just keywords

### Application 2: Recommendation Systems

**Use**: Recommend similar items

**Method**: Embed users and items

**Benefit**: Content-based recommendations

### Application 3: Text Classification

**Use**: Classify text

**Method**: Use embeddings as features

**Benefit**: Better than bag-of-words

### Application 4: Machine Translation

**Use**: Translate between languages

**Method**: Align embeddings across languages

**Benefit**: Cross-lingual understanding

## Advanced Embeddings

### GloVe

**GloVe**: Global Vectors for Word Representation

**Method**: Global co-occurrence statistics

**Benefit**: Captures global statistics

**Use**: Alternative to Word2Vec

### FastText

**FastText**: Subword embeddings

**Method**: Character n-grams

**Benefit**: Handles out-of-vocabulary words

**Use**: Morphologically rich languages

### Contextual Embeddings

**Contextual**: Embeddings depend on context

**Examples**: ELMo, BERT, GPT

**Benefit**: Same word different embeddings in different contexts

**Use**: State-of-the-art NLP

## Real-World Examples

### Example 1: Word Similarity

**Task**: Find similar words

**Method**: Cosine similarity of embeddings

**Result**: Semantically similar words

### Example 2: Document Clustering

**Task**: Cluster documents

**Method**: Average word embeddings

**Result**: Thematically similar documents

## Common Pitfalls

### Problem: Out-of-Vocabulary Words

```python
# BAD: No handling of OOV words
embedding = model["unknown_word"]  # KeyError!

# GOOD: Use FastText or handle OOV
embedding = model.get("unknown_word", default_embedding)
```

### Problem: Domain Mismatch

```python
# BAD: Use general embeddings for domain-specific task
# May not capture domain semantics

# GOOD: Fine-tune on domain data
# Or use domain-specific embeddings
```

## Quiz

1. What are vector embeddings?
   - **A)** Sparse vectors
   - **B)** Dense vector representations capturing semantic relationships
   - **C)** One-hot vectors
   - **D)** Binary vectors

2. What is Word2Vec?
   - **A)** Word classification
   - **B)** Algorithm learning word embeddings by predicting context
   - **C)** Word translation
   - **D)** Word generation

3. What is the difference between Skip-gram and CBOW?
   - **A)** No difference
   - **B)** Skip-gram predicts context from word, CBOW predicts word from context
   - **C)** Skip-gram is faster
   - **D)** CBOW is better for rare words

**Answers:**
1. **B** - Vector embeddings are dense (low-dimensional) vector representations that capture semantic relationships, where similar objects have similar vectors
2. **B** - Word2Vec learns word embeddings by training a neural network to predict words from their context (or vice versa), learning useful representations as a byproduct
3. **B** - Skip-gram predicts surrounding context words from a center word, while CBOW (Continuous Bag of Words) predicts the center word from its surrounding context words

## Next Steps

- [Reinforcement Learning Fundamentals](../machine_learning/09.%20Reinforcement%20Learning.md) - RL basics
- [Advanced NLP Techniques](../machine_learning/10.%20Advanced%20NLP.md) - NLP advanced topics

