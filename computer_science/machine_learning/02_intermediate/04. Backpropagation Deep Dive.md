---
number: 4
title: "Backpropagation Deep Dive"
slug: "backpropagation-deep-dive"
level: "intermediate"
tags: ["machine-learning", "backpropagation", "neural-networks", "gradients", "training"]
prerequisites: ["optimization-algorithms-sgd-adam-momentum"]
estimated_minutes: 130
contributors: []
diagrams: []
examples: []
canonical_id: "cs-ml-04"
---

# Backpropagation Deep Dive

## Overview

Backpropagation is the fundamental algorithm for training neural networks. Understanding how gradients flow backward through the network, the chain rule, and implementation details is essential for deep learning and understanding how neural networks learn.

## Table of Contents

1. [The Learning Problem](#learning-problem)
2. [Forward Pass](#forward-pass)
3. [Loss Function](#loss-function)
4. [Backward Pass](#backward-pass)
5. [Chain Rule](#chain-rule)
6. [Gradient Computation](#gradient-computation)
7. [Weight Updates](#weight-updates)
8. [Implementation](#implementation)

## The Learning Problem

### Goal

**Objective**: Minimize loss function

**Parameters**: Weights and biases

**Method**: Gradient descent

**Requirement**: Compute gradients efficiently

### The Challenge

**Network**: Many layers, many parameters

**Gradient**: Need gradient for each parameter

**Naive**: Compute each gradient separately (slow)

**Solution**: Backpropagation (efficient)

## Forward Pass

### Forward Propagation

**Process**: Compute output from input

**Example** (2-layer network):
```
Input: x
Layer 1: h₁ = σ(W₁x + b₁)
Layer 2: y = σ(W₂h₁ + b₂)
Output: y
```

**Activation Function**: σ (e.g., sigmoid, ReLU)

### Forward Pass Code

```python
def forward(x, W1, b1, W2, b2):
    # Layer 1
    z1 = np.dot(W1, x) + b1
    h1 = sigmoid(z1)
    
    # Layer 2
    z2 = np.dot(W2, h1) + b2
    y = sigmoid(z2)
    
    return y, h1, z1, z2
```

## Loss Function

### Common Loss Functions

**1. Mean Squared Error** (Regression):
```
L = (1/m) Σ(y_pred - y_true)²
```

**2. Cross-Entropy** (Classification):
```
L = -Σ[y_true × log(y_pred) + (1-y_true) × log(1-y_pred)]
```

**3. Categorical Cross-Entropy** (Multi-class):
```
L = -Σ y_true × log(y_pred)
```

### Loss Gradient

**Output Layer Gradient**:
```
∂L/∂y_pred = (y_pred - y_true)  (for MSE)
```

**For Cross-Entropy + Softmax**:
```
∂L/∂z = y_pred - y_true  (simplified!)
```

## Backward Pass

### Backpropagation Overview

**Goal**: Compute ∂L/∂W for all weights

**Method**: Chain rule

**Process**: Propagate gradients backward

### Chain Rule

**Chain Rule**: 
```
If z = f(y) and y = g(x), then:
dz/dx = (dz/dy) × (dy/dx)
```

**Application**:
```
L depends on y
y depends on z
z depends on W

∂L/∂W = (∂L/∂y) × (∂y/∂z) × (∂z/∂W)
```

## Gradient Computation

### Output Layer Gradients

**For Sigmoid Output**:
```
∂L/∂z = ∂L/∂y × ∂y/∂z
      = (y_pred - y_true) × y_pred × (1 - y_pred)
```

**For ReLU**:
```
∂L/∂z = ∂L/∂y × ∂y/∂z
      = ∂L/∂y × (1 if z > 0 else 0)
```

### Hidden Layer Gradients

**Process**:
```
1. Compute output layer gradient
2. Propagate backward through layers
3. Use chain rule at each layer
```

**Formula**:
```
∂L/∂h = Σ (∂L/∂z_next × ∂z_next/∂h)
       = Σ (∂L/∂z_next × W_next)
```

### Weight Gradients

**Output Layer**:
```
∂L/∂W₂ = ∂L/∂z₂ × ∂z₂/∂W₂
       = δ₂ × h₁
```

**Hidden Layer**:
```
∂L/∂W₁ = ∂L/∂z₁ × ∂z₁/∂W₁
       = δ₁ × x
```

**Where δ**: Error signal (gradient w.r.t. activation input)

## Weight Updates

### Gradient Descent Update

**Update Rule**:
```
W = W - α × ∂L/∂W
```

**Where**:
- **α**: Learning rate
- **∂L/∂W**: Gradient

### Batch Gradient Descent

**Process**:
```
1. Forward pass for all examples
2. Compute loss
3. Backward pass (average gradients)
4. Update weights
```

**Code**:
```python
def train_batch(X, y, W1, b1, W2, b2, learning_rate):
    m = X.shape[0]
    
    # Forward pass
    y_pred, h1, z1, z2 = forward_batch(X, W1, b1, W2, b2)
    
    # Loss
    loss = compute_loss(y_pred, y)
    
    # Backward pass
    dW2, db2, dW1, db1 = backward(X, y, y_pred, h1, z1, z2, W2)
    
    # Update weights
    W2 -= learning_rate * dW2
    b2 -= learning_rate * db2
    W1 -= learning_rate * dW1
    b1 -= learning_rate * db1
    
    return loss
```

## Implementation

### Complete Backpropagation

```python
def backward(X, y, y_pred, h1, z1, z2, W2):
    m = X.shape[0]
    
    # Output layer gradient
    dz2 = y_pred - y  # For cross-entropy + sigmoid
    dW2 = (1/m) * np.dot(h1.T, dz2)
    db2 = (1/m) * np.sum(dz2, axis=0)
    
    # Hidden layer gradient
    dh1 = np.dot(dz2, W2.T)
    dz1 = dh1 * sigmoid_derivative(z1)
    dW1 = (1/m) * np.dot(X.T, dz1)
    db1 = (1/m) * np.sum(dz1, axis=0)
    
    return dW2, db2, dW1, db1
```

### Activation Derivatives

**Sigmoid**:
```python
def sigmoid_derivative(z):
    s = sigmoid(z)
    return s * (1 - s)
```

**ReLU**:
```python
def relu_derivative(z):
    return (z > 0).astype(float)
```

**Tanh**:
```python
def tanh_derivative(z):
    return 1 - np.tanh(z)**2
```

## Real-World Examples

### Example 1: Training Neural Network

**Process**:
```
1. Initialize weights
2. For each epoch:
   a. Forward pass
   b. Compute loss
   c. Backward pass
   d. Update weights
3. Repeat until convergence
```

### Example 2: Deep Network

**Challenge**: Many layers

**Solution**: Backpropagation handles all layers

**Gradient Flow**: From output to input

## Common Pitfalls

### Problem: Vanishing Gradients

```python
# BAD: Deep network with sigmoid
# Gradients become very small

# GOOD: Use ReLU, residual connections
# Or use gradient clipping
```

### Problem: Exploding Gradients

```python
# BAD: Large gradients
# Weights explode

# GOOD: Gradient clipping
gradient = np.clip(gradient, -1, 1)
```

## Quiz

1. What is backpropagation used for?
   - **A)** Forward pass
   - **B)** Efficiently computing gradients for all weights using chain rule
   - **C)** Loss computation
   - **D)** Data preprocessing

2. What is the chain rule?
   - **A)** Rule for adding gradients
   - **B)** Rule for computing derivatives of composite functions
   - **C)** Rule for multiplying gradients
   - **D)** Rule for dividing gradients

3. What causes vanishing gradients?
   - **A)** Large weights
   - **B)** Deep networks with activation functions that shrink gradients
   - **C)** Small learning rate
   - **D)** Too many layers

**Answers:**
1. **B** - Backpropagation efficiently computes gradients for all weights by propagating errors backward through the network using the chain rule
2. **B** - The chain rule allows computing derivatives of composite functions by multiplying derivatives of each function in the composition
3. **B** - Vanishing gradients occur in deep networks when activation functions (like sigmoid) repeatedly shrink gradients as they propagate backward

## Next Steps

- [Regularization & Overfitting](./05.%20Regularization%20%26%20Overfitting.md) - Preventing overfitting
- [Transformers & Attention](./06.%20Transformers%20%26%20Attention%20Mechanism.md) - Modern architectures

