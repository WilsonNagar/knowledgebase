---
number: 1
title: "GPU Parallel Programming"
slug: "gpu-parallel-programming"
level: "advanced"
tags: ["concurrency", "gpu", "cuda", "opencl", "parallel-programming"]
prerequisites: ["parallel-algorithms-sort-search"]
estimated_minutes: 140
contributors: []
diagrams: []
examples: []
canonical_id: "cs-conc-adv-01"
---

# GPU Parallel Programming

## Overview

GPU parallel programming leverages thousands of cores for massively parallel computation. Understanding CUDA, OpenCL, GPU memory hierarchy, kernel design, and optimization techniques is essential for high-performance computing, machine learning, and scientific computing.

## Table of Contents

1. [GPU Architecture Review](#gpu-review)
2. [CUDA Programming Model](#cuda-model)
3. [CUDA Kernels](#cuda-kernels)
4. [Memory Management](#memory-management)
5. [Thread Organization](#thread-organization)
6. [Optimization Techniques](#optimization)
7. [OpenCL](#opencl)
8. [Applications](#applications)

## GPU Architecture Review

### GPU Characteristics

**Cores**: Hundreds to thousands

**SIMT**: Single Instruction, Multiple Threads

**Memory**: 
- **Global**: Large, slow
- **Shared**: Small, fast
- **Registers**: Very fast

**Use**: Parallel, simple operations

## CUDA Programming Model

### CUDA Basics

**CUDA**: Compute Unified Device Architecture

**Language**: C/C++ extensions

**Host**: CPU code

**Device**: GPU code

**Kernel**: Function executed on GPU

### CUDA Execution Model

**Grid**: Collection of blocks

**Block**: Collection of threads

**Thread**: Single execution unit

**Warp**: 32 threads (execution unit)

## CUDA Kernels

### Kernel Definition

**Kernel**: GPU function

**Syntax**: `__global__` keyword

**Example**:
```c
__global__ void add(int* a, int* b, int* c, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        c[i] = a[i] + b[i];
    }
}
```

### Kernel Launch

**Syntax**: `kernel<<<grid, block>>>(args)`

**Example**:
```c
int threadsPerBlock = 256;
int blocksPerGrid = (n + threadsPerBlock - 1) / threadsPerBlock;
add<<<blocksPerGrid, threadsPerBlock>>>(d_a, d_b, d_c, n);
```

## Memory Management

### Memory Types

**1. Global Memory**:
```
Large, slow (DRAM)
Accessible by all threads
```

**2. Shared Memory**:
```
Small, fast (on-chip)
Shared within block
```

**3. Registers**:
```
Very fast
Per-thread
```

**4. Constant Memory**:
```
Read-only, cached
```

### Memory Allocation

**Allocate**:
```c
int* d_a;
cudaMalloc(&d_a, n * sizeof(int));
```

**Copy**:
```c
cudaMemcpy(d_a, h_a, n * sizeof(int), cudaMemcpyHostToDevice);
```

**Free**:
```c
cudaFree(d_a);
```

## Thread Organization

### Thread Indexing

**threadIdx**: Thread index within block

**blockIdx**: Block index within grid

**blockDim**: Block dimensions

**gridDim**: Grid dimensions

**Global Index**:
```c
int i = blockIdx.x * blockDim.x + threadIdx.x;
```

### 2D Thread Organization

**2D Grid**:
```c
dim3 grid(10, 10);  // 10x10 blocks
dim3 block(16, 16); // 16x16 threads per block

kernel<<<grid, block>>>(args);
```

**Indexing**:
```c
int x = blockIdx.x * blockDim.x + threadIdx.x;
int y = blockIdx.y * blockDim.y + threadIdx.y;
```

## Optimization Techniques

### Coalesced Memory Access

**Coalesced**: Consecutive memory access

**Benefit**: Single memory transaction

**Example**:
```c
// GOOD: Coalesced
int i = blockIdx.x * blockDim.x + threadIdx.x;
value = array[i];  // Consecutive access

// BAD: Non-coalesced
int i = array[threadIdx.x * stride];  // Strided access
```

### Shared Memory Usage

**Use**: Frequently accessed data

**Example**:
```c
__shared__ int shared_data[256];

// Load into shared memory
shared_data[threadIdx.x] = global_data[blockIdx.x * blockDim.x + threadIdx.x];
__syncthreads();  // Synchronize

// Use shared memory
result = shared_data[threadIdx.x];
```

### Occupancy

**Occupancy**: Active warps per SM

**Goal**: High occupancy

**Factors**: 
- **Registers**: Per thread
- **Shared Memory**: Per block
- **Threads**: Per block

## OpenCL

### What is OpenCL?

**OpenCL**: Open Computing Language

**Purpose**: Heterogeneous computing

**Platforms**: CPU, GPU, FPGA

**Vendor**: Khronos Group

### OpenCL vs CUDA

**CUDA**: NVIDIA-specific

**OpenCL**: Cross-platform

**Trade-off**: Portability vs optimization

## Applications

### Application 1: Matrix Multiplication

**Operation**: C = A Ã— B

**Parallelization**: Each thread computes one element

**Speedup**: 100-1000x over CPU

### Application 2: Image Processing

**Operation**: Apply filters

**Parallelization**: Each thread processes one pixel

**Speedup**: Real-time processing

### Application 3: Machine Learning

**Operation**: Neural network training

**Parallelization**: Parallel gradient computation

**Speedup**: Faster training

## Real-World Examples

### Example 1: Deep Learning Frameworks

**Use**: TensorFlow, PyTorch use CUDA

**Benefit**: Fast neural network training

### Example 2: Scientific Computing

**Use**: Simulations, modeling

**Benefit**: High-performance computation

## Common Pitfalls

### Problem: Not Using Shared Memory

```c
// BAD: Access global memory repeatedly
for (int i = 0; i < 100; i++) {
    sum += array[i];  // Slow global memory
}

// GOOD: Use shared memory
__shared__ int shared[100];
// Load once, use many times
```

### Problem: Warp Divergence

```c
// BAD: Branching in warp
if (threadIdx.x % 2 == 0) {
    // Causes divergence
}

// GOOD: Minimize branching
// Or restructure algorithm
```

## Quiz

1. What is CUDA?
   - **A)** CPU architecture
   - **B)** GPU programming platform for NVIDIA GPUs
   - **C)** Memory type
   - **D)** Algorithm

2. What is a CUDA kernel?
   - **A)** OS kernel
   - **B)** Function executed on GPU with __global__ keyword
   - **C)** Memory kernel
   - **D)** Thread kernel

3. What is coalesced memory access?
   - **A)** Random access
   - **B)** Consecutive memory access by threads in warp for efficiency
   - **C)** Slow access
   - **D)** Scattered access

**Answers:**
1. **B** - CUDA (Compute Unified Device Architecture) is NVIDIA's parallel computing platform and programming model for GPU programming
2. **B** - A CUDA kernel is a function marked with __global__ that executes on the GPU, launched with special syntax specifying grid and block dimensions
3. **B** - Coalesced memory access occurs when threads in a warp access consecutive memory locations, allowing the GPU to combine these into a single efficient memory transaction

## Next Steps

- [Distributed Concurrency](../concurrency_parallelism/02.%20Distributed%20Concurrency.md) - Distributed systems
- [Transactional Memory](../concurrency_parallelism/03.%20Transactional%20Memory.md) - Hardware transactions

