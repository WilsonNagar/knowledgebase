---
number: 1
title: "CPU Scheduling Fundamentals"
slug: "cpu-scheduling-fundamentals"
level: "fundamentals"
tags: ["operating-systems", "cpu", "scheduling", "processes", "threads"]
prerequisites: []
estimated_minutes: 90
contributors: []
diagrams: []
examples: []
canonical_id: "cs-os-01"
---

# CPU Scheduling Fundamentals

## Overview

CPU scheduling is a fundamental concept in operating systems that determines which process or thread gets to execute on the CPU at any given time. Understanding scheduling algorithms, preemption, context switching, and how modern schedulers like Linux's CFS (Completely Fair Scheduler) work is essential for system programming and performance optimization.

## Table of Contents

1. [What is CPU Scheduling?](#what-is-scheduling)
2. [Scheduling Goals](#scheduling-goals)
3. [Scheduling Algorithms](#scheduling-algorithms)
4. [Preemption](#preemption)
5. [Context Switching](#context-switching)
6. [Linux CFS (Completely Fair Scheduler)](#cfs)
7. [Multi-Level Feedback Queue (MLFQ)](#mlfq)
8. [Real-World Considerations](#real-world)

## What is CPU Scheduling?

### The Problem

**Multiple Processes**:
- Many processes want to use CPU
- CPU can only run one process at a time (single-core)
- Need to decide which process runs when

**Solution**: CPU Scheduler
- **Decides**: Which process runs next
- **Manages**: Process execution order
- **Optimizes**: System performance and fairness

### Scheduler Components

**Process States**:
```
NEW → READY → RUNNING → WAITING → TERMINATED
       ↑         ↓
       └─────────┘
```

**Scheduler's Job**:
1. **Select**: Choose next process from ready queue
2. **Dispatch**: Switch CPU to that process
3. **Monitor**: Track process execution
4. **Preempt**: Interrupt if needed

## Scheduling Goals

### Key Objectives

**1. Maximize CPU Utilization**:
- Keep CPU busy
- Minimize idle time
- High throughput

**2. Minimize Response Time**:
- Fast response to user input
- Low latency
- Interactive feel

**3. Fairness**:
- All processes get CPU time
- No process starves
- Proportional sharing

**4. Throughput**:
- Maximize processes completed per unit time
- Efficient execution
- High productivity

### Trade-offs

**Response Time vs Throughput**:
- **Short time slices**: Better response time, more overhead
- **Long time slices**: Better throughput, worse response time

**Fairness vs Priority**:
- **Equal time**: Fair but may hurt important processes
- **Priority-based**: Important processes get more time

## Scheduling Algorithms

### First-Come-First-Served (FCFS)

**How It Works**:
- Processes executed in arrival order
- Non-preemptive
- Simple FIFO queue

**Example**:
```
Process  Arrival  Burst
P1       0        24
P2       1        3
P3       2        3

Timeline:
0──────24──────27──────30
P1      P2      P3

Average Waiting Time: (0 + 23 + 25) / 3 = 16
```

**Pros**:
- Simple to implement
- Fair (FIFO)

**Cons**:
- **Convoy effect**: Short process waits behind long process
- Poor average waiting time
- No priority consideration

### Shortest Job First (SJF)

**How It Works**:
- Execute shortest job first
- Minimizes average waiting time
- Optimal for minimizing waiting time

**Example**:
```
Process  Arrival  Burst
P1       0        6
P2       1        8
P3       2        7
P4       3        3

Timeline:
0──6──9──────16──────24
P1 P4  P3     P2

Average Waiting Time: (0 + 0 + 7 + 2) / 4 = 2.25
```

**Pros**:
- Optimal average waiting time
- Good for batch systems

**Cons**:
- **Starvation**: Long jobs may never run
- Requires knowing burst time (unrealistic)
- Not suitable for interactive systems

### Round Robin (RR)

**How It Works**:
- Each process gets time quantum
- Preemptive
- Circular queue

**Example** (Time Quantum = 4):
```
Process  Burst
P1       24
P2       3
P3       3

Timeline:
0─4─7─10─14─18─22─26─30
P1 P2 P3 P1  P1  P1  P1

Average Waiting Time: (6 + 4 + 7) / 3 = 5.67
```

**Pros**:
- Fair (everyone gets time)
- Good response time
- No starvation

**Cons**:
- Context switch overhead
- Performance depends on time quantum size

**Time Quantum Selection**:
- **Too small**: High overhead, low efficiency
- **Too large**: Poor response time, like FCFS
- **Rule of thumb**: 80% of CPU bursts should finish within one quantum

### Priority Scheduling

**How It Works**:
- Each process has priority
- Higher priority runs first
- Can be preemptive or non-preemptive

**Example**:
```
Process  Priority  Burst
P1       3 (low)   10
P2       1 (high)  1
P3       2 (med)   2

Timeline:
0─1─3─13
P2 P3 P1
```

**Pros**:
- Important processes get CPU
- Flexible priority assignment

**Cons**:
- **Starvation**: Low priority may never run
- Indefinite blocking

**Solutions to Starvation**:
- **Aging**: Increase priority over time
- **Priority inheritance**: Inherit priority from waiting process

## Preemption

### What is Preemption?

**Preemption**: Interrupting running process to run another

**When It Happens**:
- Time quantum expires (Round Robin)
- Higher priority process arrives
- I/O operation completes
- System call completes

### Preemptive vs Non-Preemptive

**Preemptive Scheduling**:
- Process can be interrupted
- Better for interactive systems
- More overhead (context switches)

**Non-Preemptive Scheduling**:
- Process runs until completion or I/O
- Simpler implementation
- Less overhead
- Poor for interactive systems

### Preemption Overhead

**Costs**:
- **Context switch**: Save/restore registers
- **Cache misses**: New process uses different memory
- **TLB flush**: Translation Lookaside Buffer cleared

**Minimizing Overhead**:
- Efficient context switching
- Cache-aware scheduling
- Process affinity (keep on same CPU)

## Context Switching

### What is Context Switching?

**Context Switch**: Saving state of one process and loading another

**Process State Includes**:
- **Registers**: CPU registers
- **Program counter**: Current instruction
- **Stack pointer**: Stack location
- **Memory mappings**: Page tables
- **Open files**: File descriptors

### Context Switch Steps

**1. Save Current Process**:
```
Save CPU registers
Save program counter
Save stack pointer
Save memory mappings
```

**2. Load New Process**:
```
Load CPU registers
Load program counter
Load stack pointer
Load memory mappings
Flush TLB (if needed)
```

**3. Switch to New Process**:
```
Jump to new process's program counter
Resume execution
```

### Context Switch Cost

**Typical Cost**: 1-10 microseconds

**Factors**:
- **Hardware**: CPU speed, cache size
- **OS implementation**: Efficiency of switch code
- **Memory**: Amount of state to save

**Optimization**:
- **Lazy TLB flush**: Don't flush if not needed
- **Register windows**: Hardware support
- **Fast path**: Optimize common case

## Linux CFS (Completely Fair Scheduler)

### What is CFS?

**CFS**: Linux's default scheduler (since 2.6.23)

**Goal**: Fair CPU time distribution

**Key Concept**: Virtual runtime (vruntime)

### How CFS Works

**Virtual Runtime**:
- Tracks how much CPU time process has used
- Normalized by process weight (priority)
- Lower vruntime = more CPU time needed

**Scheduling Decision**:
```
1. Calculate vruntime for each process
2. Select process with lowest vruntime
3. Run until vruntime increases
4. Re-select process with lowest vruntime
```

### CFS Features

**1. Fairness**:
- All processes get proportional CPU time
- Based on nice value (priority)
- No starvation

**2. Efficiency**:
- O(log n) selection using red-black tree
- Fast process selection
- Low overhead

**3. Interactive**:
- Good response time
- Interactive processes prioritized
- Low latency

### CFS Implementation

**Red-Black Tree**:
```
Processes stored in tree
Key: vruntime
Leftmost node: Next to run
O(log n) insertion/deletion
```

**Time Slicing**:
```
Time slice = (target_latency / number_of_processes)
Minimum: 1ms
Maximum: target_latency
```

**Weight Calculation**:
```
weight = 1024 / (1.25 ^ nice_value)
Higher nice = lower weight = less CPU time
```

## Multi-Level Feedback Queue (MLFQ)

### What is MLFQ?

**MLFQ**: Adaptive scheduling algorithm

**Goal**: Optimize for both interactive and batch jobs

**Key Idea**: Multiple priority queues with feedback

### How MLFQ Works

**Queue Structure**:
```
Queue 0 (Highest Priority): Interactive jobs
Queue 1: Medium priority
Queue 2: Lower priority
Queue N (Lowest Priority): Batch jobs
```

**Rules**:
1. **Priority**: Higher queue runs first
2. **Time quantum**: Higher queue gets smaller quantum
3. **Promotion**: If job uses full quantum, move to lower queue
4. **Demotion**: If job gives up CPU (I/O), stay in same queue
5. **Aging**: Periodically boost all jobs to top queue

### MLFQ Example

```
Job A: Interactive (I/O bound)
Job B: Batch (CPU bound)

Initial:
Queue 0: [A, B]

After A does I/O:
Queue 0: [A] (stays)
Queue 1: [B] (demoted)

After A returns:
Queue 0: [A] (runs first)
Queue 1: [B]
```

**Benefits**:
- **Interactive jobs**: Get CPU quickly
- **Batch jobs**: Don't starve
- **Adaptive**: Adjusts to job behavior

## Real-World Considerations

### Multi-Core Scheduling

**Challenge**: Multiple CPUs, shared resources

**Solutions**:
- **Load balancing**: Distribute processes across CPUs
- **CPU affinity**: Keep process on same CPU (cache locality)
- **NUMA awareness**: Consider memory locality

### Real-Time Scheduling

**Real-Time Requirements**:
- **Hard real-time**: Must meet deadline (safety-critical)
- **Soft real-time**: Should meet deadline (best effort)

**Algorithms**:
- **Rate Monotonic**: Static priority by period
- **Earliest Deadline First (EDF)**: Dynamic priority by deadline

### I/O-Bound vs CPU-Bound

**I/O-Bound Processes**:
- Spend time waiting for I/O
- Should get CPU quickly when I/O completes
- Benefit from higher priority

**CPU-Bound Processes**:
- Use CPU continuously
- Can wait longer
- Lower priority acceptable

**Modern Schedulers**:
- Detect I/O-bound processes
- Boost priority when I/O completes
- Reduce priority for CPU-bound processes

## Common Pitfalls

### Problem: Priority Inversion

**Scenario**:
```
High priority process waits for low priority process
Medium priority process runs instead
High priority blocked indefinitely
```

**Solution**: Priority inheritance
- Low priority process inherits high priority
- Prevents medium priority from running
- High priority can proceed

### Problem: Convoy Effect

**Scenario**:
```
Long CPU-bound process runs
Short I/O-bound processes wait
CPU underutilized
```

**Solution**: Preemptive scheduling
- Interrupt long-running process
- Give CPU to I/O-bound processes
- Better overall utilization

## Quiz

1. What is the main advantage of Round Robin scheduling?
   - **A)** Lowest average waiting time
   - **B)** Fairness and good response time
   - **C)** Highest throughput
   - **D)** Simplest implementation

2. What does CFS use to determine which process runs next?
   - **A)** Priority only
   - **B)** Virtual runtime (vruntime)
   - **C)** Burst time
   - **D)** Arrival time

3. What is the main problem with priority scheduling?
   - **A)** Too complex
   - **B)** Starvation of low-priority processes
   - **C)** Poor throughput
   - **D)** High overhead

**Answers:**
1. **B** - Round Robin provides fairness (every process gets time) and good response time through preemption
2. **B** - CFS uses virtual runtime (vruntime) to track CPU time usage and ensure fairness
3. **B** - Priority scheduling can cause starvation where low-priority processes may never get CPU time

## Next Steps

- [Memory Management - Paging & Segmentation](../operating_systems/02.%20Memory%20Management%20-%20Paging%20%26%20Segmentation.md) - Memory scheduling
- [Process vs Threads Internals](../operating_systems/03.%20Process%20vs%20Threads%20Internals.md) - Thread scheduling

