---
number: 3
title: "Cache Optimization Techniques"
slug: "cache-optimization-techniques"
level: "fundamentals"
tags: ["memory", "cache", "optimization", "performance", "cpu-cache"]
prerequisites: ["memory-allocation-strategies"]
estimated_minutes: 130
contributors: []
diagrams: []
examples: []
canonical_id: "cs-mem-03"
---

# Cache Optimization Techniques

## Overview

CPU cache optimization is crucial for high-performance applications. Understanding cache hierarchy, cache locality, cache-friendly data structures, and optimization techniques is essential for writing efficient code that maximizes cache utilization.

## Table of Contents

1. [CPU Cache Fundamentals](#cache-fundamentals)
2. [Cache Hierarchy](#cache-hierarchy)
3. [Cache Locality](#cache-locality)
4. [Cache-Friendly Data Structures](#cache-friendly)
5. [Optimization Techniques](#optimization)
6. [Cache Miss Types](#cache-misses)
7. [Performance Impact](#performance)
8. [Best Practices](#best-practices)

## CPU Cache Fundamentals

### What is CPU Cache?

**Cache**: Fast memory close to CPU

**Purpose**: Reduce memory access latency

**Speed**: Much faster than main memory

**Size**: Smaller than main memory

**Types**: L1, L2, L3 caches

### Cache Characteristics

**1. Speed**:
```
Much faster than main memory
```

**2. Size**:
```
Smaller than main memory
```

**3. Cost**:
```
More expensive per byte
```

**4. Proximity**:
```
Close to CPU
```

## Cache Hierarchy

### Cache Levels

**L1 Cache**:
- **Size**: 32-64 KB per core
- **Speed**: Fastest
- **Location**: On CPU die

**L2 Cache**:
- **Size**: 256 KB - 1 MB per core
- **Speed**: Fast
- **Location**: On CPU die

**L3 Cache**:
- **Size**: 8-32 MB shared
- **Speed**: Slower
- **Location**: On CPU die

**Main Memory**:
- **Size**: GBs
- **Speed**: Slowest
- **Location**: Off CPU die

### Cache Hierarchy Example

```
CPU
├── L1 Cache (32 KB) - Fastest
├── L2 Cache (256 KB) - Fast
├── L3 Cache (8 MB) - Slower
└── Main Memory (16 GB) - Slowest
```

## Cache Locality

### Temporal Locality

**Temporal Locality**: Recently accessed data likely to be accessed again

**Principle**: Keep recently used data in cache

**Example**:
```c
// Good temporal locality
for (int i = 0; i < 1000; i++) {
    sum += arr[i];  // arr[i] accessed multiple times
}
```

### Spatial Locality

**Spatial Locality**: Nearby data likely to be accessed

**Principle**: Cache loads nearby data

**Example**:
```c
// Good spatial locality
for (int i = 0; i < 1000; i++) {
    sum += arr[i];  // Sequential access
}

// Poor spatial locality
for (int i = 0; i < 1000; i += 100) {
    sum += arr[i];  // Strided access
}
```

## Cache-Friendly Data Structures

### Array vs Linked List

**Array**:
- **Locality**: Good spatial locality
- **Cache**: Cache-friendly
- **Access**: Sequential access

**Linked List**:
- **Locality**: Poor spatial locality
- **Cache**: Cache-unfriendly
- **Access**: Random access

**Example**:
```c
// Array - cache-friendly
int arr[1000];
for (int i = 0; i < 1000; i++) {
    sum += arr[i];  // Sequential, cache-friendly
}

// Linked list - cache-unfriendly
struct Node* head = ...;
for (struct Node* node = head; node; node = node->next) {
    sum += node->data;  // Random memory access
}
```

### Structure Layout

**Structure Padding**:
```c
// BAD: Padding wastes cache
struct Bad {
    char a;      // 1 byte
    // 7 bytes padding
    double b;    // 8 bytes
};

// GOOD: Packed structure
struct Good {
    double b;    // 8 bytes
    char a;      // 1 byte
    // 7 bytes padding (but better)
};
```

**Cache Line Alignment**:
```c
// Align to cache line (typically 64 bytes)
struct alignas(64) CacheAligned {
    // Structure aligned to cache line
};
```

## Optimization Techniques

### Technique 1: Loop Optimization

**Loop Blocking**:
```c
// BAD: Poor cache usage
for (int i = 0; i < N; i++) {
    for (int j = 0; j < N; j++) {
        C[i][j] = A[i][j] + B[i][j];
    }
}

// GOOD: Cache-friendly blocking
const int BLOCK = 64;
for (int ii = 0; ii < N; ii += BLOCK) {
    for (int jj = 0; jj < N; jj += BLOCK) {
        for (int i = ii; i < min(ii + BLOCK, N); i++) {
            for (int j = jj; j < min(jj + BLOCK, N); j++) {
                C[i][j] = A[i][j] + B[i][j];
            }
        }
    }
}
```

### Technique 2: Data Layout

**Structure of Arrays (SoA)**:
```c
// Structure of Arrays - cache-friendly
struct SoA {
    float* x;
    float* y;
    float* z;
};

// Array of Structures - less cache-friendly
struct AoS {
    float x, y, z;
};
struct AoS* arr;
```

### Technique 3: Prefetching

**Prefetching**: Load data before needed

**Example**:
```c
for (int i = 0; i < N; i++) {
    __builtin_prefetch(&arr[i + 8], 0, 1);  // Prefetch ahead
    process(arr[i]);
}
```

## Cache Miss Types

### Compulsory Misses

**Compulsory**: First access to data

**Cause**: Data not in cache

**Solution**: Cannot avoid (first access)

### Capacity Misses

**Capacity**: Cache too small

**Cause**: Working set > cache size

**Solution**: Reduce working set

### Conflict Misses

**Conflict**: Cache line conflicts

**Cause**: Multiple addresses map to same cache line

**Solution**: Avoid conflicts, use associativity

## Performance Impact

### Cache Hit vs Miss

**Cache Hit**: 
- **Latency**: ~1-3 cycles
- **Throughput**: High

**Cache Miss**: 
- **Latency**: ~100-300 cycles
- **Throughput**: Low

**Impact**: Cache misses significantly slower

### Performance Example

**Cache-Friendly**:
```c
// Sequential access - good cache usage
for (int i = 0; i < N; i++) {
    sum += arr[i];
}
// Fast: Good cache hit rate
```

**Cache-Unfriendly**:
```c
// Random access - poor cache usage
for (int i = 0; i < N; i++) {
    sum += arr[random_index[i]];
}
// Slow: Many cache misses
```

## Best Practices

### Practice 1: Sequential Access

**Sequential Access**:
```c
// GOOD: Sequential access
for (int i = 0; i < N; i++) {
    process(arr[i]);
}

// BAD: Random access
for (int i = 0; i < N; i++) {
    process(arr[indices[i]]);
}
```

### Practice 2: Cache-Friendly Data Structures

**Use Arrays**:
```c
// GOOD: Array
int arr[1000];

// BAD: Linked list (if possible)
struct Node* list;
```

### Practice 3: Minimize Working Set

**Reduce Working Set**:
```c
// Process data in chunks
const int CHUNK = 1000;
for (int i = 0; i < N; i += CHUNK) {
    process_chunk(&arr[i], CHUNK);
}
```

## Real-World Examples

### Example 1: Matrix Multiplication

**Optimization**: Blocking

**Benefit**: Better cache usage

**Result**: Significant speedup

### Example 2: Database Query

**Optimization**: Sequential scan

**Benefit**: Better cache usage

**Result**: Faster queries

## Common Pitfalls

### Problem: Poor Data Layout

```c
// BAD: Poor layout
struct Point {
    char id;
    double x, y, z;  // Padding wastes cache
};

// GOOD: Better layout
struct Point {
    double x, y, z;
    char id;
};
```

### Problem: Random Access

```c
// BAD: Random access
for (int i = 0; i < N; i++) {
    process(arr[random[i]]);
}

// GOOD: Sequential access
for (int i = 0; i < N; i++) {
    process(arr[i]);
}
```

## Quiz

1. What is temporal locality?
   - **A)** Spatial locality
   - **B)** Recently accessed data likely to be accessed again
   - **C)** Random access
   - **D)** Sequential access

2. What is spatial locality?
   - **A)** Temporal locality
   - **B)** Nearby data likely to be accessed
   - **C)** Random access
   - **D)** Cache misses

3. Why are arrays more cache-friendly than linked lists?
   - **A)** Arrays are faster
   - **B)** Arrays have better spatial locality (sequential memory layout)
   - **C)** Arrays are smaller
   - **D)** Arrays are simpler

**Answers:**
1. **B** - Temporal locality means recently accessed data is likely to be accessed again soon, keeping it in cache improves performance
2. **B** - Spatial locality means data near recently accessed data is likely to be accessed, so caches load nearby data (cache lines)
3. **B** - Arrays have sequential memory layout providing good spatial locality, while linked lists have nodes scattered in memory causing poor cache performance

## Next Steps

- [Memory Profiling & Analysis](../memory_performance/04.%20Memory%20Profiling%20%26%20Analysis.md) - Memory profiling
- [Performance Benchmarking](../memory_performance/05.%20Performance%20Benchmarking.md) - Benchmarking

