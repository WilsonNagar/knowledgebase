---
number: 3
title: "Performance Optimization"
slug: "performance-optimization"
level: "advanced"
tags: ["performance", "optimization", "scalability", "caching", "load-testing", "profiling"]
prerequisites: [1, 4]
estimated_minutes: 120
contributors: []
canonical_id: "devops-advanced-03"
---

# Performance Optimization

## Overview

Performance optimization is critical for building scalable, efficient systems that can handle increasing loads while maintaining responsiveness. This guide covers performance optimization strategies including caching, database optimization, load balancing, profiling, load testing, and scaling patterns for cloud-native applications.

## Deep Explanation

### Performance Fundamentals

#### Key Performance Metrics

1. **Latency**: Time to process a request
2. **Throughput**: Requests processed per second
3. **Resource Utilization**: CPU, memory, disk, network
4. **Error Rate**: Percentage of failed requests
5. **Concurrency**: Number of simultaneous requests

#### Performance vs Scalability

**Performance**: How fast a system responds
**Scalability**: How well a system handles increased load

```mermaid
graph LR
    PERF[Performance<br/>Speed]
    SCALE[Scalability<br/>Capacity]
    RELIABILITY[Reliability<br/>Uptime]
    
    PERF --> SYSTEM[System]
    SCALE --> SYSTEM
    RELIABILITY --> SYSTEM
    
    style PERF fill:#4caf50
    style SCALE fill:#2196f3
    style RELIABILITY fill:#ff9800
```

### Caching Strategies

#### Cache Types

**Application Cache**:
```python
# Redis cache example
import redis
import json

redis_client = redis.Redis(host='localhost', port=6379)

def get_user(user_id):
    # Check cache first
    cached = redis_client.get(f"user:{user_id}")
    if cached:
        return json.loads(cached)
    
    # Fetch from database
    user = db.get_user(user_id)
    
    # Cache for 1 hour
    redis_client.setex(
        f"user:{user_id}",
        3600,
        json.dumps(user)
    )
    return user
```

**CDN Caching**:
```yaml
# CloudFront CDN configuration
CacheBehavior:
  PathPattern: "/*"
  TargetOriginId: "S3-Origin"
  ViewerProtocolPolicy: "redirect-to-https"
  CachePolicyId: "4135ea2d-6df8-44a3-9df3-4b5a84be39ad"
  TTL: 86400  # 24 hours
```

**Database Query Cache**:
```sql
-- MySQL query cache
SELECT SQL_CACHE * FROM users WHERE id = 1;

-- PostgreSQL (using pg_stat_statements)
SELECT query, calls, mean_exec_time
FROM pg_stat_statements
ORDER BY mean_exec_time DESC
LIMIT 10;
```

#### Cache Patterns

**Cache-Aside**:
```python
def get_data(key):
    # Check cache
    value = cache.get(key)
    if value:
        return value
    
    # Load from database
    value = database.get(key)
    
    # Store in cache
    cache.set(key, value, ttl=3600)
    return value
```

**Write-Through**:
```python
def set_data(key, value):
    # Write to database
    database.set(key, value)
    
    # Write to cache
    cache.set(key, value, ttl=3600)
```

**Write-Behind**:
```python
def set_data(key, value):
    # Write to cache immediately
    cache.set(key, value, ttl=3600)
    
    # Queue for database write
    queue.enqueue(database.set, key, value)
```

### Database Optimization

#### Query Optimization

**Indexes**:
```sql
-- Create index
CREATE INDEX idx_user_email ON users(email);

-- Composite index
CREATE INDEX idx_user_name_email ON users(name, email);

-- Analyze query plan
EXPLAIN ANALYZE SELECT * FROM users WHERE email = 'user@example.com';
```

**Query Optimization**:
```sql
-- BAD: Full table scan
SELECT * FROM users WHERE name LIKE '%john%';

-- GOOD: Use index
SELECT * FROM users WHERE name LIKE 'john%';

-- BAD: N+1 queries
SELECT * FROM users;
-- Then for each user: SELECT * FROM orders WHERE user_id = ?

-- GOOD: Join or batch
SELECT u.*, o.*
FROM users u
LEFT JOIN orders o ON u.id = o.user_id;
```

**Connection Pooling**:
```python
# Connection pool example
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool

engine = create_engine(
    'postgresql://user:pass@localhost/db',
    poolclass=QueuePool,
    pool_size=20,
    max_overflow=10,
    pool_pre_ping=True
)
```

#### Database Sharding

```python
# Sharding strategy
def get_shard(user_id):
    """Determine shard based on user ID"""
    shard_count = 4
    return user_id % shard_count

def get_user(user_id):
    shard = get_shard(user_id)
    db = get_database_connection(f"shard_{shard}")
    return db.query(User).filter_by(id=user_id).first()
```

### Load Balancing

#### Load Balancing Algorithms

**Round Robin**:
```nginx
upstream backend {
    server backend1.example.com;
    server backend2.example.com;
    server backend3.example.com;
}
```

**Least Connections**:
```nginx
upstream backend {
    least_conn;
    server backend1.example.com;
    server backend2.example.com;
}
```

**IP Hash**:
```nginx
upstream backend {
    ip_hash;
    server backend1.example.com;
    server backend2.example.com;
}
```

**Weighted**:
```nginx
upstream backend {
    server backend1.example.com weight=3;
    server backend2.example.com weight=1;
}
```

#### Health Checks

```yaml
# Kubernetes health checks
livenessProbe:
  httpGet:
    path: /health
    port: 8080
  initialDelaySeconds: 30
  periodSeconds: 10

readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5
```

### Profiling and Monitoring

#### Application Profiling

**Python Profiling**:
```python
import cProfile
import pstats

def profile_function():
    profiler = cProfile.Profile()
    profiler.enable()
    
    # Your code here
    process_data()
    
    profiler.disable()
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')
    stats.print_stats(20)  # Top 20 functions
```

**Go Profiling**:
```go
import _ "net/http/pprof"
import "net/http"

func main() {
    go func() {
        log.Println(http.ListenAndServe("localhost:6060", nil))
    }()
    // Your application code
}
```

#### Performance Monitoring

```python
# Performance metrics
from prometheus_client import Histogram, Counter

request_duration = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint']
)

request_count = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

@app.route('/api/users')
def get_users():
    with request_duration.labels('GET', '/api/users').time():
        request_count.labels('GET', '/api/users', '200').inc()
        return get_users_from_db()
```

### Load Testing

#### Load Testing Tools

**Apache Bench**:
```bash
# Basic load test
ab -n 1000 -c 10 http://localhost:8080/api/users

# With headers
ab -n 1000 -c 10 -H "Authorization: Bearer token" \
  http://localhost:8080/api/users
```

**JMeter**:
```xml
<!-- JMeter test plan -->
<ThreadGroup>
  <stringProp name="ThreadGroup.num_threads">100</stringProp>
  <stringProp name="ThreadGroup.ramp_time">10</stringProp>
  <HTTPSamplerProxy>
    <stringProp name="HTTPSampler.domain">localhost</stringProp>
    <stringProp name="HTTPSampler.path">/api/users</stringProp>
  </HTTPSamplerProxy>
</ThreadGroup>
```

**Locust**:
```python
# locustfile.py
from locust import HttpUser, task, between

class UserBehavior(HttpUser):
    wait_time = between(1, 3)
    
    @task
    def get_users(self):
        self.client.get("/api/users")
    
    @task(3)
    def create_user(self):
        self.client.post("/api/users", json={
            "name": "John",
            "email": "john@example.com"
        })
```

#### Load Testing Strategy

1. **Baseline**: Measure current performance
2. **Stress Test**: Find breaking point
3. **Spike Test**: Sudden load increases
4. **Endurance Test**: Sustained load
5. **Volume Test**: Large data sets

### Scaling Strategies

#### Horizontal Scaling

```yaml
# Kubernetes HPA
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

#### Vertical Scaling

```yaml
# Increase resources
resources:
  requests:
    cpu: "2"
    memory: "4Gi"
  limits:
    cpu: "4"
    memory: "8Gi"
```

#### Auto-Scaling Groups

```hcl
# AWS Auto Scaling Group
resource "aws_autoscaling_group" "app" {
  min_size         = 2
  max_size         = 10
  desired_capacity = 4
  
  launch_template {
    id = aws_launch_template.app.id
  }
  
  target_group_arns = [aws_lb_target_group.app.arn]
}
```

### Code Optimization

#### Algorithm Optimization

```python
# BAD: O(nÂ²) complexity
def find_duplicates(arr):
    duplicates = []
    for i in range(len(arr)):
        for j in range(i+1, len(arr)):
            if arr[i] == arr[j]:
                duplicates.append(arr[i])
    return duplicates

# GOOD: O(n) complexity
def find_duplicates(arr):
    seen = set()
    duplicates = []
    for item in arr:
        if item in seen:
            duplicates.append(item)
        seen.add(item)
    return duplicates
```

#### Database Query Optimization

```python
# BAD: N+1 queries
users = User.query.all()
for user in users:
    orders = Order.query.filter_by(user_id=user.id).all()

# GOOD: Single query with join
users = User.query.options(
    joinedload(User.orders)
).all()
```

#### Async Processing

```python
# Async I/O
import asyncio
import aiohttp

async def fetch_data(url):
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            return await response.json()

async def fetch_multiple(urls):
    tasks = [fetch_data(url) for url in urls]
    return await asyncio.gather(*tasks)
```

### Resource Optimization

#### Memory Optimization

```python
# Use generators for large datasets
def process_large_file(filename):
    with open(filename) as f:
        for line in f:
            yield process_line(line)

# Use __slots__ for memory efficiency
class User:
    __slots__ = ['id', 'name', 'email']
    
    def __init__(self, id, name, email):
        self.id = id
        self.name = name
        self.email = email
```

#### CPU Optimization

```python
# Use multiprocessing for CPU-bound tasks
from multiprocessing import Pool

def process_item(item):
    return expensive_computation(item)

if __name__ == '__main__':
    with Pool(processes=4) as pool:
        results = pool.map(process_item, items)
```

## Diagrams

### Caching Architecture

```mermaid
graph TB
    CLIENT[Client]
    CDN[CDN Cache]
    LB[Load Balancer]
    APP[Application Cache]
    DB[Database]
    
    CLIENT --> CDN
    CDN --> LB
    LB --> APP
    APP --> DB
    
    style CDN fill:#4caf50
    style APP fill:#2196f3
    style DB fill:#ff9800
```

### Scaling Strategy

```mermaid
graph TB
    TRAFFIC[Traffic]
    LB[Load Balancer]
    APP1[App Instance 1]
    APP2[App Instance 2]
    APP3[App Instance 3]
    AUTO[Auto Scaling]
    
    TRAFFIC --> LB
    LB --> APP1
    LB --> APP2
    LB --> APP3
    AUTO --> APP1
    AUTO --> APP2
    AUTO --> APP3
    
    style LB fill:#4caf50
    style AUTO fill:#2196f3
```

## Real Code Examples

### Complete Performance-Optimized Application

```python
# Optimized Flask application
from flask import Flask, jsonify
from flask_caching import Cache
from sqlalchemy import create_engine
from sqlalchemy.pool import QueuePool
import redis
from functools import wraps
import time

app = Flask(__name__)

# Configure cache
cache = Cache(app, config={
    'CACHE_TYPE': 'redis',
    'CACHE_REDIS_URL': 'redis://localhost:6379/0'
})

# Configure database with connection pooling
engine = create_engine(
    'postgresql://user:pass@localhost/db',
    poolclass=QueuePool,
    pool_size=20,
    max_overflow=10,
    pool_pre_ping=True
)

# Redis client for custom caching
redis_client = redis.Redis(host='localhost', port=6379, db=0)

def cache_response(timeout=300):
    """Cache decorator"""
    def decorator(f):
        @wraps(f)
        def decorated_function(*args, **kwargs):
            cache_key = f"{f.__name__}:{str(args)}:{str(kwargs)}"
            cached = redis_client.get(cache_key)
            if cached:
                return jsonify(eval(cached))
            
            result = f(*args, **kwargs)
            redis_client.setex(cache_key, timeout, str(result))
            return result
        return decorated_function
    return decorator

@app.route('/api/users/<int:user_id>')
@cache_response(timeout=3600)
def get_user(user_id):
    """Get user with caching"""
    # Check cache first
    cache_key = f"user:{user_id}"
    cached = redis_client.get(cache_key)
    if cached:
        return jsonify(eval(cached))
    
    # Query database
    with engine.connect() as conn:
        result = conn.execute(
            "SELECT * FROM users WHERE id = %s",
            (user_id,)
        ).fetchone()
    
    user = {
        'id': result[0],
        'name': result[1],
        'email': result[2]
    }
    
    # Cache result
    redis_client.setex(cache_key, 3600, str(user))
    return jsonify(user)

@app.route('/api/users')
def get_users():
    """Get users with pagination"""
    page = request.args.get('page', 1, type=int)
    per_page = request.args.get('per_page', 20, type=int)
    
    offset = (page - 1) * per_page
    
    with engine.connect() as conn:
        users = conn.execute(
            "SELECT * FROM users LIMIT %s OFFSET %s",
            (per_page, offset)
        ).fetchall()
    
    return jsonify([dict(u) for u in users])

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080)
```

### Load Testing Script

```python
# load_test.py
import asyncio
import aiohttp
import time
from statistics import mean, median

async def make_request(session, url):
    """Make a single request"""
    start = time.time()
    try:
        async with session.get(url) as response:
            await response.json()
            duration = time.time() - start
            return {'status': response.status, 'duration': duration}
    except Exception as e:
        duration = time.time() - start
        return {'status': 'error', 'duration': duration, 'error': str(e)}

async def load_test(url, concurrent_requests, total_requests):
    """Run load test"""
    async with aiohttp.ClientSession() as session:
        tasks = []
        for _ in range(total_requests):
            if len(tasks) >= concurrent_requests:
                # Wait for some to complete
                done, pending = await asyncio.wait(
                    tasks, return_when=asyncio.FIRST_COMPLETED
                )
                tasks = list(pending)
            
            tasks.append(make_request(session, url))
        
        # Wait for remaining tasks
        results = await asyncio.gather(*tasks)
    
    # Analyze results
    durations = [r['duration'] for r in results]
    successful = [r for r in results if r['status'] == 200]
    
    print(f"Total requests: {len(results)}")
    print(f"Successful: {len(successful)}")
    print(f"Failed: {len(results) - len(successful)}")
    print(f"Mean duration: {mean(durations):.3f}s")
    print(f"Median duration: {median(durations):.3f}s")
    print(f"Min duration: {min(durations):.3f}s")
    print(f"Max duration: {max(durations):.3f}s")

if __name__ == '__main__':
    asyncio.run(load_test(
        'http://localhost:8080/api/users',
        concurrent_requests=10,
        total_requests=1000
    ))
```

## Hard Use-Case: Optimizing Slow API

### Problem

API endpoint taking 2+ seconds, needs to handle 1000 req/s.

### Solution: Multi-Layer Optimization

#### Step 1: Add Caching

```python
@cache_response(timeout=300)
def get_users():
    return db.query(User).all()
```

#### Step 2: Optimize Database Query

```python
# Add index
CREATE INDEX idx_users_active ON users(active) WHERE active = true;

# Optimize query
def get_users():
    return db.query(User).filter_by(active=True).options(
        joinedload(User.profile)
    ).all()
```

#### Step 3: Add Connection Pooling

```python
engine = create_engine(
    'postgresql://...',
    pool_size=20,
    max_overflow=10
)
```

#### Step 4: Horizontal Scaling

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
spec:
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        averageUtilization: 70
```

#### Results

- **Before**: 2s latency, 100 req/s
- **After**: 200ms latency, 1000 req/s

## Edge Cases and Pitfalls

### 1. Cache Invalidation

**Problem**: Stale cache data

**Solution**: Proper invalidation strategy

```python
def update_user(user_id, data):
    # Update database
    db.update_user(user_id, data)
    
    # Invalidate cache
    redis_client.delete(f"user:{user_id}")
```

### 2. Over-Caching

**Problem**: Caching everything, memory issues

**Solution**: Cache selectively

```python
# Only cache expensive operations
@cache_response(timeout=3600)
def expensive_computation():
    # Expensive operation
    pass
```

### 3. Premature Optimization

**Problem**: Optimizing before measuring

**Solution**: Profile first, optimize bottlenecks

```python
# Profile to find bottlenecks
import cProfile
cProfile.run('your_function()')
```

## References and Further Reading

- [High Performance Browser Networking](https://hpbn.co/) - Web performance
- [Designing Data-Intensive Applications](https://dataintensive.net/) - Scalability patterns
- [System Design Primer](https://github.com/donnemartin/system-design-primer) - System design

## Quiz

### Question 1
What is the difference between performance and scalability?

**A)** They're the same  
**B)** Performance is speed, scalability is capacity  
**C)** Performance is capacity, scalability is speed  
**D)** No difference

**Answer: B** - Performance measures how fast a system responds, while scalability measures how well it handles increased load.

### Question 2
What is cache-aside pattern?

**A)** Cache writes through to database  
**B)** Application checks cache, then database  
**C)** Cache only  
**D)** Database only

**Answer: B** - Cache-aside pattern has the application check cache first, then query the database if not found.

### Question 3
What is horizontal scaling?

**A)** Adding more resources to existing servers  
**B)** Adding more servers  
**C)** Removing servers  
**D)** Optimizing code

**Answer: B** - Horizontal scaling adds more servers/nodes to handle increased load.

### Question 4
What should you do before optimizing?

**A)** Optimize everything  
**B)** Profile to find bottlenecks  
**C)** Guess what's slow  
**D)** Nothing

**Answer: B** - Always profile first to identify actual bottlenecks before optimizing.

### Question 5
What is connection pooling?

**A)** Storing connections  
**B)** Reusing database connections  
**C)** Creating new connections  
**D)** Closing connections

**Answer: B** - Connection pooling reuses database connections to avoid the overhead of creating new ones.

## Related Topics

- [Monitoring & Observability](../02_intermediate/04.%20Monitoring%20%26%20Observability.md) - Performance monitoring
- [Kubernetes Fundamentals](../02_intermediate/01.%20Kubernetes%20Fundamentals.md) - Scaling in K8s
- [Cloud Platforms Overview](../02_intermediate/03.%20Cloud%20Platforms%20Overview.md) - Cloud scaling

